{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d4f213-c3b6-4537-b3a6-2f9965c28625",
   "metadata": {},
   "source": [
    "##　セグメンテーション\n",
    "9から12章では分類機のモデル訓練に使用するデータは人手でアノテーションしている。\n",
    "モデルへの入力の結節を自動で生成したい。\n",
    "\n",
    "現実世界のアプローチでは複数の問題を個々のステップで解決するが、ディープラーニングの研究では複数の問題から構成されている問題を単一のモデルによって解決させる傾向がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadac47f-b2ef-45db-8644-4cae61b29e98",
   "metadata": {},
   "source": [
    "１３章ではCTスキャンの元データから結節である可能性がある領域を全て見つける。\n",
    "結節の一部であるボクセルに対してラベルをつけるセグメンテーション処理を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea29db6-3bf8-4cc2-b7bc-b1803266c18e",
   "metadata": {},
   "source": [
    "セマンティックセグメンテーション\n",
    "\n",
    "画像内の個々のピクセルに対してラベルをつける。今回は結節にTrue、正常組織にFalseのラベルをつける。\n",
    "\n",
    "\n",
    "物体検出\n",
    "\n",
    "画像内の対象にバウンディングボックスを設定する\n",
    "こちらの方が計算リソースが必要である"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49a4ac-1b58-450d-ace6-f353089a13dd",
   "metadata": {},
   "source": [
    "## U-NET\n",
    "\n",
    "分類タスクでは画像を畳み込みとダウンサンプリングを繰り返し、各クラスの確率のベクトルにする。\n",
    "\n",
    "セグメンテーションでは入力と出力のサイズは同じにしたい。畳み込みによりテクスチャや色を検出し、ダウンサプリングによって畳み込みの受容野を広げ局所だけでなく広い特徴を掴む。このようにしていくと画像サイズが小さくなっていくので、1つのピクセルをn＊nのブロックに置き換えるアップサンプリングを行う。\n",
    "\n",
    "さらに今回はパディングをおこなことで、画像周辺のピクセルが失われないようにして、かつUのダウンサンプリング時とアップサンプリング時のサイズが同じになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a955a86-28fc-4251-ae38-49773a23a40a",
   "metadata": {},
   "source": [
    "スキップ接続がないと、ダウンサンプリング時に画像が小さくなり物体境界の正確な位置情報が失われやすくなる。\n",
    "\n",
    "U-Netのスキップ接続はResNetのスキップ接続とは異なり、ダウンサンプリング側の入力を対応する出力側のアップサンプリング側へつなぐ。\n",
    "このことにより、Uの底で広い受容野の情報とネットワーク初期の入力に近い高精細な情報の両方を出力へ繋げていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8a6dd-af39-4308-bc11-99b08fda2c6f",
   "metadata": {},
   "source": [
    "既存コードの再利用は良いアイデアだが、どのようなモデルで、どのような実装、訓練か取り組んでいるプロジェクトに適用できる部分があるか把握しておくことが必要。\n",
    "\n",
    "既存のモデルを変更していく場合は、1つずつ変更し比較していくと良い。\n",
    "\n",
    "今回は既存のU-NETから\n",
    "1：入力をバッチ正規化する\n",
    "2：出力の前にnn.sigmoidを使い[0,1]の範囲にする\n",
    "3：モデルの深さとフィルタを減らす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6203db-15e5-443d-ba30-273252f941ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUNetWrapper\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# kwargs はコンストラクタに渡される全てのキーワード引数を含む辞書\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class UNetWrapper(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        # kwargs はコンストラクタに渡される全てのキーワード引数を含む辞書\n",
    "        super().__init__()\n",
    "\n",
    "        # BatchNorm2d は入力のチャンネル数を必要とする\n",
    "        # その情報をキーワード引数から取り出す\n",
    "        self.input_batchnorm = nn.BatchNorm2d(kwargs[\"in_channels\"])\n",
    "        # U-Netの取り込み部分はこれだけだが、ほとんどの処理はここで行われる\n",
    "        self.unet = UNet(**kwargs)\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "        # 第11章と同じように独自の重み初期化を行う\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        init_set = {\n",
    "            nn.Conv2d,\n",
    "            nn.Conv3d,\n",
    "            nn.ConvTranspose2d,\n",
    "            nn.ConvTranspose3d,\n",
    "            nn.Linear,\n",
    "        }\n",
    "        for m in self.modules():\n",
    "            if type(m) in init_set:\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight.data, mode=\"fan_out\", nonlinearity=\"relu\", a=0\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(\n",
    "                        m.weight.data\n",
    "                    )\n",
    "                    bound = 1 / math.sqrt(fan_out)\n",
    "                    nn.init.normal_(m.bias, -bound, bound)\n",
    "\n",
    "        # nn.init.constant_(self.unet.last.bias, -4)\n",
    "        # nn.init.constant_(self.unet.last.bias, 4)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        bn_output = self.input_batchnorm(input_batch)\n",
    "        un_output = self.unet(bn_output)\n",
    "        fn_output = self.final(un_output)\n",
    "        return fn_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf54cd5-7890-4fec-8f77-bc3b54057485",
   "metadata": {},
   "source": [
    "今回の画像は三次元だが、nn.Batchnormは2d\n",
    "メモリ使用量を減らすためだが、前後の画像の情報は検出には必要。\n",
    "二次元画像として処理してセグメンテーション処理時には三次元画像として渡す。\n",
    "\n",
    "モデル学習時に自ら隣接していることを学ぶ必要があるが、z軸の画像量が少なく容易と考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e7809-fd9e-426d-8c91-546e0a3e3e9c",
   "metadata": {},
   "source": [
    "画像のチャネルをスライス（＋2、＋1、0、-1、-2）に絞り、スライス、x軸、y軸の入力とする。\n",
    "スライス方向の情報を限定的にするが、結節の大きさは小さく今回の問題では十分だと判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0940c46-a95e-4fea-829e-e45dfffdedc4",
   "metadata": {},
   "source": [
    "## モデルの設計\n",
    "\n",
    "どのようなトレードオフを考えなけらばいけないかはフローチャートや経験則はないが、体系的に仮説を検証することが大切であり、\n",
    "思いつきの変更等は堪え、複数の変更を同時にテストすることはだめ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9821d5-9bf3-46a2-bc1d-fb962a12cfea",
   "metadata": {},
   "source": [
    "## 正解データの作成\n",
    "\n",
    "バウンディングボックスを作成し、その後マスクとする\n",
    "\n",
    "結節の中心の位置はわかっているから、閾値以下になるまで左右、上下に探索し閾値以下になったらそこまでの範囲とする。\n",
    "他の組織と隣接している可能性もあるので、片方が低密度に触れたら探索終了となる。片方ずつの独立の探索はできず、結節は中心の情報が必要。\n",
    "\n",
    "下記のループ処理後にバウンディングボックス内の閾値より高い領域を調理和として取り出し、マスクとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56e528-7492-45ab-af6a-2745e4b7ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAnnotationMask(self, positiveInfo_list, threshold_hu=-700):\n",
    "        # hu_aと同じ次元のゼロarrayを作成\n",
    "        boundingBox_a = np.zeros_like(self.hu_a, dtype=np.bool)\n",
    "\n",
    "        for candidateInfo_tup in positiveInfo_list:\n",
    "            center_irc = xyz2irc(\n",
    "                candidateInfo_tup.center_xyz,\n",
    "                self.origin_xyz,\n",
    "                self.vxSize_xyz,\n",
    "                self.direction_a,\n",
    "            )\n",
    "            \n",
    "            # 結節中心の位置情報\n",
    "            ci = int(center_irc.index)\n",
    "            cr = int(center_irc.row)\n",
    "            cc = int(center_irc.col)\n",
    "\n",
    "            index_radius = 2\n",
    "            try:\n",
    "                while (\n",
    "                    self.hu_a[ci + index_radius, cr, cc] > threshold_hu\n",
    "                    and self.hu_a[ci - index_radius, cr, cc] > threshold_hu\n",
    "                ):\n",
    "                    index_radius += 1\n",
    "            except IndexError:\n",
    "                index_radius -= 1\n",
    "                \n",
    "            row_radius = 2\n",
    "            try:\n",
    "                while (\n",
    "                    self.hu_a[ci, cr + row_radius, cc] > threshold_hu\n",
    "                    and self.hu_a[ci, cr - row_radius, cc] > threshold_hu\n",
    "                ):\n",
    "                    row_radius += 1\n",
    "            except IndexError:\n",
    "                row_radius -= 1\n",
    "\n",
    "            col_radius = 2\n",
    "            try:\n",
    "                while (\n",
    "                    self.hu_a[ci, cr, cc + col_radius] > threshold_hu\n",
    "                    and self.hu_a[ci, cr, cc - col_radius] > threshold_hu\n",
    "                ):\n",
    "                    col_radius += 1\n",
    "            except IndexError:\n",
    "                col_radius -= 1\n",
    "                \n",
    "                \n",
    "            \n",
    "            boundingBox_a[\n",
    "                ci - index_radius : ci + index_radius + 1,\n",
    "                cr - row_radius : cr + row_radius + 1,\n",
    "                cc - col_radius : cc + col_radius + 1,\n",
    "            ] = True\n",
    "        \n",
    "        # バウンディングボックスからマスクをくり抜く\n",
    "        mask_a = boundingBox_a & (self.hu_a > threshold_hu)\n",
    "\n",
    "        return mask_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9dbc324-b1f6-4538-90b8-5dcc18df3c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_221/1484239419.py:6: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  a=np.zeros_like(x,dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.ones(27)\n",
    "x = x.reshape((3, 3, 3))\n",
    "x\n",
    "\n",
    "a=np.zeros_like(x,dtype=np.bool)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73f3dca9-2b13-4a83-ba78-aa54efd6af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1:2, 0:2, 1:2]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1decfca-889e-4dd5-a2a9-356a6807f780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[False,  True, False],\n",
       "        [False,  True, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be5212-704e-4923-bb9a-63205e9b9193",
   "metadata": {},
   "source": [
    "## データセット\n",
    "\n",
    "生成するデータは複数のチャネルを持つ二次元画像。結節として注目しているスライス断面に隣接しているスライス画像をチャネルに割り当てる。一つのデータのスライスは数枚になる。\n",
    "\n",
    "訓練データと検証データのサイズが異なる\n",
    "\n",
    "今回はフルサイズのデータで訓練したら成績はよくなかった。これは画像全体に比較して結節のサイズが小さく、陽性サンプルが陰性サンプルに埋もれてしまう現象と同じことが起きたと考えられる。そのため訓練時は結節の周囲のみクロップした画像で学習することで陰性サンプルに対して陽性サンプルの割合を増やす。\n",
    "\n",
    "セグメンテーションモデルではピクセル単位で処理をするため、任意の画像サイズを扱うことができ、訓練と検証で画像サイズが異なってもよい。検証では訓練と同じ重みの畳み込み計算を、より多くのピクセルを持つ大きな画像に対し適用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b752a9-3730-4d99-bc69-bce466d2ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(\n",
    "        self,\n",
    "        val_stride=0,\n",
    "        isValSet_bool=None,\n",
    "        series_uid=None,\n",
    "        contextSlices_count=3,\n",
    "        fullCt_bool=False,\n",
    "    ):\n",
    "        self.contextSlices_count = contextSlices_count\n",
    "        self.fullCt_bool = fullCt_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21d806-6486-4a83-a8fe-7c8ec689a3f6",
   "metadata": {},
   "source": [
    "検証では結節のあるなし関係なく全ての画像リストからフルサイズで画像を取得、\n",
    "contextSlices_countは適当に指定し、指定した結節の断面の上下数枚のデータを取得する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8262f-8be3-4381-b6ce-0916ef735d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ取得（検証）\n",
    "#　\n",
    "def __getitem__(self, ndx):\n",
    "        series_uid, slice_ndx = self.sample_list[ndx % len(self.sample_list)]\n",
    "        return self.getitem_fullSlice(series_uid, slice_ndx)\n",
    "    \n",
    "def getitem_fullSlice(self, series_uid, slice_ndx):\n",
    "        ct = getCt(series_uid)\n",
    "        ct_t = torch.zeros((self.contextSlices_count * 2 + 1, 512, 512))\n",
    "\n",
    "        start_ndx = slice_ndx - self.contextSlices_count\n",
    "        end_ndx = slice_ndx + self.contextSlices_count + 1\n",
    "        for i, context_ndx in enumerate(range(start_ndx, end_ndx)):\n",
    "            context_ndx = max(context_ndx, 0)\n",
    "            context_ndx = min(context_ndx, ct.hu_a.shape[0] - 1)\n",
    "            ct_t[i] = torch.from_numpy(ct.hu_a[context_ndx].astype(np.float32))\n",
    "\n",
    "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
    "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
    "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
    "        # The upper bound nukes any weird hotspots and clamps bone down\n",
    "        ct_t.clamp_(-1000, 1000)\n",
    "\n",
    "        pos_t = torch.from_numpy(ct.positive_mask[slice_ndx]).unsqueeze(0)\n",
    "\n",
    "        return ct_t, pos_t, ct.series_uid, slice_ndx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd8c00c-508a-4f3a-8f5f-53ed9286e1ec",
   "metadata": {},
   "source": [
    "訓練では結節のリストから7枚で画像サイズは中心から96*96で取得する。\n",
    "その後ランダムに64*64にクロップする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c1f95-6e53-45be-a6be-cabb8a5056ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ取得（訓練）\n",
    "\n",
    "def __getitem__(self, ndx):\n",
    "        candidateInfo_tup = self.pos_list[ndx % len(self.pos_list)]\n",
    "        return self.getitem_trainingCrop(candidateInfo_tup)\n",
    "\n",
    "def getitem_trainingCrop(self, candidateInfo_tup):\n",
    "        ct_a, pos_a, center_irc = getCtRawCandidate(\n",
    "            candidateInfo_tup.series_uid,\n",
    "            candidateInfo_tup.center_xyz,\n",
    "            (7, 96, 96),\n",
    "        )\n",
    "        \n",
    "        # スライスは固定\n",
    "        pos_a = pos_a[3:4]\n",
    "\n",
    "        row_offset = random.randrange(0, 32)\n",
    "        col_offset = random.randrange(0, 32)\n",
    "        ct_t = torch.from_numpy(\n",
    "            ct_a[:, row_offset : row_offset + 64, col_offset : col_offset + 64]\n",
    "        ).to(torch.float32)\n",
    "        pos_t = torch.from_numpy(\n",
    "            pos_a[:, row_offset : row_offset + 64, col_offset : col_offset + 64]\n",
    "        ).to(torch.long)\n",
    "\n",
    "        slice_ndx = center_irc.index\n",
    "\n",
    "        return ct_t, pos_t, candidateInfo_tup.series_uid, slice_ndx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c142ef-61a2-419e-a3dc-d424209ad264",
   "metadata": {},
   "source": [
    "## データオーギュメンテーションはGPUで\n",
    "\n",
    "ボトルネックは一般的に下記で\n",
    "\n",
    "1：データ読み込みパイプライン中のデータ展開時（RAM）\n",
    "\n",
    "2：CPUでのデータ前処理（正規化、オーギュメンテーション）\n",
    "\n",
    "3：GPUでの訓練ループ\n",
    "ここがボトルネックになるようにする\n",
    "\n",
    "内容は12章と同じ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ef236-cfa0-4792-bdb0-e29314a86ca1",
   "metadata": {},
   "source": [
    "##　訓練\n",
    "\n",
    "モデルのインスタンス化（セグメンテーションモデルとオーギュメンテーションモデル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3f0d9-8864-4578-8b92-401ff003d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initModel(self):\n",
    "        segmentation_model = UNetWrapper(\n",
    "            in_channels=7,\n",
    "            n_classes=1,\n",
    "            depth=3,\n",
    "            wf=4,\n",
    "            padding=True,\n",
    "            batch_norm=True,\n",
    "            up_mode='upconv',\n",
    "        )\n",
    "\n",
    "        augmentation_model = SegmentationAugmentation(**self.augmentation_dict)\n",
    "\n",
    "        if self.use_cuda:\n",
    "            log.info(\"Using CUDA; {} devices.\".format(\n",
    "                torch.cuda.device_count()))\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                segmentation_model = nn.DataParallel(segmentation_model)\n",
    "                augmentation_model = nn.DataParallel(augmentation_model)\n",
    "            segmentation_model = segmentation_model.to(self.device)\n",
    "            augmentation_model = augmentation_model.to(self.device)\n",
    "\n",
    "        return segmentation_model, augmentation_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b85ba-4bd6-47f0-bf15-d1d162f185a1",
   "metadata": {},
   "source": [
    "## 最適化\n",
    "\n",
    "Adamはパラメータごとに学習率を調整し更新していく。基本的に学習率はデフォルト値以外を使用。\n",
    "\n",
    "SGDの方が成績が良い場合もあるがハイパーパラメータの探索に時間がかかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132579ad-9790-4f26-97aa-70accfac9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initOptimizer(self):\n",
    "        return Adam(self.segmentation_model.parameters())\n",
    "        # return SGD(self.segmentation_model.parameters(), lr=0.001, momentum=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe7e68-8fd0-49a5-88c4-40d57145157b",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "\n",
    "Dice係数\n",
    "\n",
    "画像全体に対して比較的狭い領域だけが陽性になる場合に使用。ピクセル単位のF1値のようなもの。\n",
    "Dice係数は高い方が良い値で（最大1）、pytorchでは1ーDice係数として損失に利用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c3885-d440-4c2e-a14a-5df2dbc754a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diceLoss(self, prediction_g, label_g, epsilon=1):\n",
    "        diceLabel_g = label_g.sum(dim=[1, 2, 3])\n",
    "        dicePrediction_g = prediction_g.sum(dim=[1, 2, 3])\n",
    "        diceCorrect_g = (prediction_g * label_g).sum(dim=[1, 2, 3])\n",
    "\n",
    "        diceRatio_g = (2 * diceCorrect_g + epsilon) \\\n",
    "            / (dicePrediction_g + diceLabel_g + epsilon)\n",
    "\n",
    "        return 1 - diceRatio_g\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
