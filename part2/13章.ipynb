{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3d4f213-c3b6-4537-b3a6-2f9965c28625",
   "metadata": {},
   "source": [
    "##　セグメンテーション\n",
    "9から12章では分類機のモデル訓練に使用するデータは人手でアノテーションしている。\n",
    "モデルへの入力の結節を自動で生成したい。\n",
    "\n",
    "現実世界のアプローチでは複数の問題を個々のステップで解決するが、ディープラーニングの研究では複数の問題から構成されている問題を単一のモデルによって解決させる傾向がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadac47f-b2ef-45db-8644-4cae61b29e98",
   "metadata": {},
   "source": [
    "１３章ではCTスキャンの元データから結節である可能性がある領域を全て見つける。\n",
    "結節の一部であるボクセルに対してラベルをつけるセグメンテーション処理を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea29db6-3bf8-4cc2-b7bc-b1803266c18e",
   "metadata": {},
   "source": [
    "セマンティックセグメンテーション\n",
    "\n",
    "画像内の個々のピクセルに対してラベルをつける。今回は結節にTrue、正常組織にFalseのラベルをつける。\n",
    "\n",
    "\n",
    "物体検出\n",
    "\n",
    "画像内の対象にバウンディングボックスを設定する\n",
    "こちらの方が計算リソースが必要である"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49a4ac-1b58-450d-ace6-f353089a13dd",
   "metadata": {},
   "source": [
    "## U-NET\n",
    "\n",
    "分類タスクでは画像を畳み込みとダウンサンプリングを繰り返し、各クラスの確率のベクトルにする。\n",
    "\n",
    "セグメンテーションでは入力と出力のサイズは同じにしたい。畳み込みによりテクスチャや色を検出し、ダウンサプリングによって畳み込みの受容野を広げ局所だけでなく広い特徴を掴む。このようにしていくと画像サイズが小さくなっていくので、1つのピクセルをn＊nのブロックに置き換えるアップサンプリングを行う。\n",
    "\n",
    "さらに今回はパディングをおこなことで、画像周辺のピクセルが失われないようにして、かつUのダウンサンプリング時とアップサンプリング時のサイズが同じになる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a955a86-28fc-4251-ae38-49773a23a40a",
   "metadata": {},
   "source": [
    "スキップ接続がないと、ダウンサンプリング時に画像が小さくなり物体境界の正確な位置情報が失われやすくなる。\n",
    "\n",
    "U-Netのスキップ接続はResNetのスキップ接続とは異なり、ダウンサンプリング側の入力を対応する出力側のアップサンプリング側へつなぐ。\n",
    "このことにより、Uの底で広い受容野の情報とネットワーク初期の入力に近い高精細な情報の両方を出力へ繋げていく。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e8a6dd-af39-4308-bc11-99b08fda2c6f",
   "metadata": {},
   "source": [
    "既存コードの再利用は良いアイデアだが、どのようなモデルで、どのような実装、訓練か取り組んでいるプロジェクトに適用できる部分があるか把握しておくことが必要。\n",
    "\n",
    "既存のモデルを変更していく場合は、1つずつ変更し比較していくと良い。\n",
    "\n",
    "今回は既存のU-NETから\n",
    "1：入力をバッチ正規化する\n",
    "2：出力の前にnn.sigmoidを使い[0,1]の範囲にする\n",
    "3：モデルの深さとフィルタを減らす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6203db-15e5-443d-ba30-273252f941ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mUNetWrapper\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# kwargs はコンストラクタに渡される全てのキーワード引数を含む辞書\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class UNetWrapper(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        # kwargs はコンストラクタに渡される全てのキーワード引数を含む辞書\n",
    "        super().__init__()\n",
    "\n",
    "        # BatchNorm2d は入力のチャンネル数を必要とする\n",
    "        # その情報をキーワード引数から取り出す\n",
    "        self.input_batchnorm = nn.BatchNorm2d(kwargs[\"in_channels\"])\n",
    "        # U-Netの取り込み部分はこれだけだが、ほとんどの処理はここで行われる\n",
    "        self.unet = UNet(**kwargs)\n",
    "        self.final = nn.Sigmoid()\n",
    "\n",
    "        # 第11章と同じように独自の重み初期化を行う\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        init_set = {\n",
    "            nn.Conv2d,\n",
    "            nn.Conv3d,\n",
    "            nn.ConvTranspose2d,\n",
    "            nn.ConvTranspose3d,\n",
    "            nn.Linear,\n",
    "        }\n",
    "        for m in self.modules():\n",
    "            if type(m) in init_set:\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight.data, mode=\"fan_out\", nonlinearity=\"relu\", a=0\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(\n",
    "                        m.weight.data\n",
    "                    )\n",
    "                    bound = 1 / math.sqrt(fan_out)\n",
    "                    nn.init.normal_(m.bias, -bound, bound)\n",
    "\n",
    "        # nn.init.constant_(self.unet.last.bias, -4)\n",
    "        # nn.init.constant_(self.unet.last.bias, 4)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        bn_output = self.input_batchnorm(input_batch)\n",
    "        un_output = self.unet(bn_output)\n",
    "        fn_output = self.final(un_output)\n",
    "        return fn_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf54cd5-7890-4fec-8f77-bc3b54057485",
   "metadata": {},
   "source": [
    "今回の画像は三次元だが、nn.Batchnormは2d\n",
    "メモリ使用量を減らすためだが、前後の画像の情報は検出には必要。\n",
    "二次元画像として処理してセグメンテーション処理時には三次元画像として渡す。\n",
    "\n",
    "モデル学習時に自ら隣接していることを学ぶ必要があるが、z軸の画像量が少なく容易と考える。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e7809-fd9e-426d-8c91-546e0a3e3e9c",
   "metadata": {},
   "source": [
    "画像のチャネルをスライス（＋2、＋1、0、-1、-2）に絞り、スライス、x軸、y軸の入力とする。\n",
    "スライス方向の情報を限定的にするが、結節の大きさは小さく今回の問題では十分だと判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0940c46-a95e-4fea-829e-e45dfffdedc4",
   "metadata": {},
   "source": [
    "## モデルの設計\n",
    "\n",
    "どのようなトレードオフを考えなけらばいけないかはフローチャートや経験則はないが、体系的に仮説を検証することが大切であり、\n",
    "思いつきの変更等は堪え、複数の変更を同時にテストすることはだめ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9821d5-9bf3-46a2-bc1d-fb962a12cfea",
   "metadata": {},
   "source": [
    "## 正解データの作成\n",
    "\n",
    "バウンディングボックスを作成し、その後マスクとする\n",
    "\n",
    "結節の中心の位置はわかっているから、閾値以下になるまで左右、上下に探索し閾値以下になったらそこまでの範囲とする。\n",
    "他の組織と隣接している可能性もあるので、片方が低密度に触れたら探索終了となる。片方ずつの独立の探索はできず、結節は中心の情報が必要。\n",
    "\n",
    "下記のループ処理後にバウンディングボックス内の閾値より高い領域を調理和として取り出し、マスクとする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56e528-7492-45ab-af6a-2745e4b7ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildAnnotationMask(self, positiveInfo_list, threshold_hu=-700):\n",
    "        # hu_aと同じ次元のゼロarrayを作成\n",
    "        boundingBox_a = np.zeros_like(self.hu_a, dtype=np.bool)\n",
    "\n",
    "        for candidateInfo_tup in positiveInfo_list:\n",
    "            center_irc = xyz2irc(\n",
    "                candidateInfo_tup.center_xyz,\n",
    "                self.origin_xyz,\n",
    "                self.vxSize_xyz,\n",
    "                self.direction_a,\n",
    "            )\n",
    "            \n",
    "            # 結節中心の位置情報\n",
    "            ci = int(center_irc.index)\n",
    "            cr = int(center_irc.row)\n",
    "            cc = int(center_irc.col)\n",
    "\n",
    "            index_radius = 2\n",
    "            try:\n",
    "                while (\n",
    "                    self.hu_a[ci + index_radius, cr, cc] > threshold_hu\n",
    "                    and self.hu_a[ci - index_radius, cr, cc] > threshold_hu\n",
    "                ):\n",
    "                    index_radius += 1\n",
    "            except IndexError:\n",
    "                index_radius -= 1\n",
    "                \n",
    "            row_radius = 2\n",
    "            try:\n",
    "                while (\n",
    "                    self.hu_a[ci, cr + row_radius, cc] > threshold_hu\n",
    "                    and self.hu_a[ci, cr - row_radius, cc] > threshold_hu\n",
    "                ):\n",
    "                    row_radius += 1\n",
    "            except IndexError:\n",
    "                row_radius -= 1\n",
    "\n",
    "            col_radius = 2\n",
    "            try:\n",
    "                while (\n",
    "                    self.hu_a[ci, cr, cc + col_radius] > threshold_hu\n",
    "                    and self.hu_a[ci, cr, cc - col_radius] > threshold_hu\n",
    "                ):\n",
    "                    col_radius += 1\n",
    "            except IndexError:\n",
    "                col_radius -= 1\n",
    "                \n",
    "                \n",
    "            \n",
    "            boundingBox_a[\n",
    "                ci - index_radius : ci + index_radius + 1,\n",
    "                cr - row_radius : cr + row_radius + 1,\n",
    "                cc - col_radius : cc + col_radius + 1,\n",
    "            ] = True\n",
    "        \n",
    "        # バウンディングボックスからマスクをくり抜く\n",
    "        mask_a = boundingBox_a & (self.hu_a > threshold_hu)\n",
    "\n",
    "        return mask_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9dbc324-b1f6-4538-90b8-5dcc18df3c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_221/1484239419.py:6: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  a=np.zeros_like(x,dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.ones(27)\n",
    "x = x.reshape((3, 3, 3))\n",
    "x\n",
    "\n",
    "a=np.zeros_like(x,dtype=np.bool)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "73f3dca9-2b13-4a83-ba78-aa54efd6af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[1:2, 0:2, 1:2]=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1decfca-889e-4dd5-a2a9-356a6807f780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[False,  True, False],\n",
       "        [False,  True, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be5212-704e-4923-bb9a-63205e9b9193",
   "metadata": {},
   "source": [
    "## データセット\n",
    "\n",
    "生成するデータは複数のチャネルを持つ二次元画像。結節として注目しているスライス断面に隣接しているスライス画像をチャネルに割り当てる。一つのデータのスライスは数枚になる。\n",
    "\n",
    "訓練データと検証データのサイズが異なる\n",
    "\n",
    "今回はフルサイズのデータで訓練したら成績はよくなかった。これは画像全体に比較して結節のサイズが小さく、陽性サンプルが陰性サンプルに埋もれてしまう現象と同じことが起きたと考えられる。そのため訓練時は結節の周囲のみクロップした画像で学習することで陰性サンプルに対して陽性サンプルの割合を増やす。\n",
    "\n",
    "セグメンテーションモデルではピクセル単位で処理をするため、任意の画像サイズを扱うことができ、訓練と検証で画像サイズが異なってもよい。検証では訓練と同じ重みの畳み込み計算を、より多くのピクセルを持つ大きな画像に対し適用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b752a9-3730-4d99-bc69-bce466d2ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(\n",
    "        self,\n",
    "        val_stride=0,\n",
    "        isValSet_bool=None,\n",
    "        series_uid=None,\n",
    "        contextSlices_count=3,\n",
    "        fullCt_bool=False,\n",
    "    ):\n",
    "        self.contextSlices_count = contextSlices_count\n",
    "        self.fullCt_bool = fullCt_bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21d806-6486-4a83-a8fe-7c8ec689a3f6",
   "metadata": {},
   "source": [
    "検証では結節のあるなし関係なく全ての画像リストからフルサイズで画像を取得、\n",
    "contextSlices_countは適当に指定し、指定した結節の断面の上下数枚のデータを取得する。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8262f-8be3-4381-b6ce-0916ef735d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ取得（検証）\n",
    "#　\n",
    "def __getitem__(self, ndx):\n",
    "        series_uid, slice_ndx = self.sample_list[ndx % len(self.sample_list)]\n",
    "        return self.getitem_fullSlice(series_uid, slice_ndx)\n",
    "    \n",
    "def getitem_fullSlice(self, series_uid, slice_ndx):\n",
    "        ct = getCt(series_uid)\n",
    "        ct_t = torch.zeros((self.contextSlices_count * 2 + 1, 512, 512))\n",
    "\n",
    "        start_ndx = slice_ndx - self.contextSlices_count\n",
    "        end_ndx = slice_ndx + self.contextSlices_count + 1\n",
    "        for i, context_ndx in enumerate(range(start_ndx, end_ndx)):\n",
    "            context_ndx = max(context_ndx, 0)\n",
    "            context_ndx = min(context_ndx, ct.hu_a.shape[0] - 1)\n",
    "            ct_t[i] = torch.from_numpy(ct.hu_a[context_ndx].astype(np.float32))\n",
    "\n",
    "        # CTs are natively expressed in https://en.wikipedia.org/wiki/Hounsfield_scale\n",
    "        # HU are scaled oddly, with 0 g/cc (air, approximately) being -1000 and 1 g/cc (water) being 0.\n",
    "        # The lower bound gets rid of negative density stuff used to indicate out-of-FOV\n",
    "        # The upper bound nukes any weird hotspots and clamps bone down\n",
    "        ct_t.clamp_(-1000, 1000)\n",
    "\n",
    "        pos_t = torch.from_numpy(ct.positive_mask[slice_ndx]).unsqueeze(0)\n",
    "\n",
    "        return ct_t, pos_t, ct.series_uid, slice_ndx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd8c00c-508a-4f3a-8f5f-53ed9286e1ec",
   "metadata": {},
   "source": [
    "訓練では結節のリストから7枚で画像サイズは中心から96*96で取得する。\n",
    "その後ランダムに64*64にクロップする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c1f95-6e53-45be-a6be-cabb8a5056ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ取得（訓練）\n",
    "\n",
    "def __getitem__(self, ndx):\n",
    "        candidateInfo_tup = self.pos_list[ndx % len(self.pos_list)]\n",
    "        return self.getitem_trainingCrop(candidateInfo_tup)\n",
    "\n",
    "def getitem_trainingCrop(self, candidateInfo_tup):\n",
    "        ct_a, pos_a, center_irc = getCtRawCandidate(\n",
    "            candidateInfo_tup.series_uid,\n",
    "            candidateInfo_tup.center_xyz,\n",
    "            (7, 96, 96),\n",
    "        )\n",
    "        \n",
    "        # スライスは固定\n",
    "        pos_a = pos_a[3:4]\n",
    "\n",
    "        row_offset = random.randrange(0, 32)\n",
    "        col_offset = random.randrange(0, 32)\n",
    "        ct_t = torch.from_numpy(\n",
    "            ct_a[:, row_offset : row_offset + 64, col_offset : col_offset + 64]\n",
    "        ).to(torch.float32)\n",
    "        pos_t = torch.from_numpy(\n",
    "            pos_a[:, row_offset : row_offset + 64, col_offset : col_offset + 64]\n",
    "        ).to(torch.long)\n",
    "\n",
    "        slice_ndx = center_irc.index\n",
    "\n",
    "        return ct_t, pos_t, candidateInfo_tup.series_uid, slice_ndx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c142ef-61a2-419e-a3dc-d424209ad264",
   "metadata": {},
   "source": [
    "## データオーギュメンテーションはGPUで\n",
    "\n",
    "ボトルネックは一般的に下記で\n",
    "\n",
    "1：データ読み込みパイプライン中のデータ展開時（RAM）\n",
    "\n",
    "2：CPUでのデータ前処理（正規化、オーギュメンテーション）\n",
    "\n",
    "3：GPUでの訓練ループ\n",
    "ここがボトルネックになるようにする\n",
    "\n",
    "内容は12章と同じ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ef236-cfa0-4792-bdb0-e29314a86ca1",
   "metadata": {},
   "source": [
    "##　訓練\n",
    "\n",
    "モデルのインスタンス化（セグメンテーションモデルとオーギュメンテーションモデル）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa3f0d9-8864-4578-8b92-401ff003d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initModel(self):\n",
    "        segmentation_model = UNetWrapper(\n",
    "            in_channels=7,\n",
    "            n_classes=1,\n",
    "            depth=3,\n",
    "            wf=4,\n",
    "            padding=True,\n",
    "            batch_norm=True,\n",
    "            up_mode='upconv',\n",
    "        )\n",
    "\n",
    "        augmentation_model = SegmentationAugmentation(**self.augmentation_dict)\n",
    "\n",
    "        if self.use_cuda:\n",
    "            log.info(\"Using CUDA; {} devices.\".format(\n",
    "                torch.cuda.device_count()))\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                segmentation_model = nn.DataParallel(segmentation_model)\n",
    "                augmentation_model = nn.DataParallel(augmentation_model)\n",
    "            segmentation_model = segmentation_model.to(self.device)\n",
    "            augmentation_model = augmentation_model.to(self.device)\n",
    "\n",
    "        return segmentation_model, augmentation_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9b85ba-4bd6-47f0-bf15-d1d162f185a1",
   "metadata": {},
   "source": [
    "## 最適化\n",
    "\n",
    "Adamはパラメータごとに学習率を調整し更新していく。基本的に学習率はデフォルト値以外を使用。\n",
    "\n",
    "SGDの方が成績が良い場合もあるがハイパーパラメータの探索に時間がかかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132579ad-9790-4f26-97aa-70accfac9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initOptimizer(self):\n",
    "        return Adam(self.segmentation_model.parameters())\n",
    "        # return SGD(self.segmentation_model.parameters(), lr=0.001, momentum=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe7e68-8fd0-49a5-88c4-40d57145157b",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "\n",
    "Dice係数\n",
    "\n",
    "画像全体に対して比較的狭い領域だけが陽性になる場合に使用。ピクセル単位のF1値のようなもの。\n",
    "Dice係数は高い方が良い値で（最大1）、pytorchでは1ーDice係数として損失に利用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c3885-d440-4c2e-a14a-5df2dbc754a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diceLoss(self, prediction_g, label_g, epsilon=1):\n",
    "        diceLabel_g = label_g.sum(dim=[1, 2, 3])\n",
    "        dicePrediction_g = prediction_g.sum(dim=[1, 2, 3])\n",
    "        diceCorrect_g = (prediction_g * label_g).sum(dim=[1, 2, 3])\n",
    "\n",
    "        diceRatio_g = (2 * diceCorrect_g + epsilon) \\\n",
    "            / (dicePrediction_g + diceLabel_g + epsilon)\n",
    "\n",
    "        return 1 - diceRatio_g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e236658-63a6-4105-844c-e53e2710bf90",
   "metadata": {},
   "source": [
    "Dice係数では\n",
    "\n",
    "正解U予測＊２　/  正解＋予測\n",
    "\n",
    "医療では偽陰性は危険。なので再現率（recall)を重要視したい\n",
    "\n",
    "Diceの入力を変更し、　正解U予測　/ 予測　（真陽性　/ 予測）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5401ec88-fc23-4762-aaa0-887510d3c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g,\n",
    "                         classificationThreshold=0.5):\n",
    "        input_t, label_t, series_list, _slice_ndx_list = batch_tup\n",
    "\n",
    "        input_g = input_t.to(self.device, non_blocking=True)\n",
    "        label_g = label_t.to(self.device, non_blocking=True)\n",
    "\n",
    "        if self.segmentation_model.training and self.augmentation_dict:\n",
    "            input_g, label_g = self.augmentation_model(input_g, label_g)\n",
    "\n",
    "        prediction_g = self.segmentation_model(input_g)\n",
    "\n",
    "        diceLoss_g = self.diceLoss(prediction_g, label_g)\n",
    "        fnLoss_g = self.diceLoss(prediction_g * label_g, label_g)\n",
    "\n",
    "        start_ndx = batch_ndx * batch_size\n",
    "        end_ndx = start_ndx + input_t.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictionBool_g = (prediction_g[:, 0:1]\n",
    "                                > classificationThreshold).to(torch.float32)\n",
    "\n",
    "            tp = (predictionBool_g * label_g).sum(dim=[1, 2, 3])\n",
    "            fn = ((1 - predictionBool_g) * label_g).sum(dim=[1, 2, 3])\n",
    "            fp = (predictionBool_g * (~label_g)).sum(dim=[1, 2, 3])\n",
    "\n",
    "            metrics_g[METRICS_LOSS_NDX, start_ndx:end_ndx] = diceLoss_g\n",
    "            metrics_g[METRICS_TP_NDX, start_ndx:end_ndx] = tp\n",
    "            metrics_g[METRICS_FN_NDX, start_ndx:end_ndx] = fn\n",
    "            metrics_g[METRICS_FP_NDX, start_ndx:end_ndx] = fp\n",
    "\n",
    "        return diceLoss_g.mean() + fnLoss_g.mean() * 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d91865-f455-4e1c-bbc3-ae823623e87f",
   "metadata": {},
   "source": [
    "Dice係数にて二つの指標を求めて、recallは８倍して重みをつけて返す。\n",
    "\n",
    "重みが強いので、SGDで最適化すると全てのピクセルを陽性と判断してしまう。\n",
    "Adamにすると学習率を細かく調整してくれるので、偽陰性による損失に過剰に引っ張られなくすむ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbabb86-01e6-417c-b936-88ec6b1510cc",
   "metadata": {},
   "source": [
    "## 画像の出力(Tensorboad)\n",
    "\n",
    "セグメンテーションタスクでは出力を視覚的に確認できる。TensorBoardでは画像にも対応している。\n",
    "関数の表示は　logImage 関数を用いる。\n",
    "\n",
    "訓練においてバランスの調整に注意する。\n",
    "\n",
    "１：短時間で、訓練がうまくいっているのか大まかに把握する。\n",
    "\n",
    "評価指標を表示するlogMetricsを頻繁に実行する。\n",
    "\n",
    "２：GPUの計算時間の多くを検証ではなく、訓練に使う\n",
    "\n",
    "logMetricsで検証する回数を減らす\n",
    "\n",
    "３：検証セットで訓練が正しく進歩しているかを把握する。\n",
    "\n",
    "定期的な検証を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1295a515-02b8-4c9f-a46e-469887402398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初回と5エポックごとに行う\n",
    "# CTは六枚を選んで表示する。\n",
    "\n",
    "def main(self):\n",
    "        log.info(\"Starting {}, {}\".format(type(self).__name__, self.cli_args))\n",
    "\n",
    "        train_dl = self.initTrainDl()\n",
    "        val_dl = self.initValDl()\n",
    "\n",
    "        best_score = 0.0\n",
    "        self.validation_cadence = 5\n",
    "        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
    "            log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "                epoch_ndx,\n",
    "                self.cli_args.epochs,\n",
    "                len(train_dl),\n",
    "                len(val_dl),\n",
    "                self.cli_args.batch_size,\n",
    "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
    "            ))\n",
    "\n",
    "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
    "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "            if epoch_ndx == 1 or epoch_ndx % self.validation_cadence == 0:\n",
    "                # if validation is wanted\n",
    "                valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
    "                score = self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "                best_score = max(score, best_score)\n",
    "\n",
    "                self.saveModel('seg', epoch_ndx, score == best_score)\n",
    "\n",
    "                self.logImages(epoch_ndx, 'trn', train_dl)\n",
    "                self.logImages(epoch_ndx, 'val', val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a1dc6857-2c4e-4448-9444-d919dbba20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.ones([3,3,3])\n",
    "a[0][1]=2\n",
    "p=np.ones([1,3,3,3])\n",
    "p[0][1][1]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "de1d60ae-8fb9-4a59-9d78-c8697b2a70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a= torch.from_numpy(a)\n",
    "p= torch.from_numpy(p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "573448d1-602c-4372-8b4b-d76dcc4715cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=a.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "2d6c05fb-340b-4116-a2cb-3302271c7db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False],\n",
       "         [ True,  True,  True],\n",
       "         [False, False, False]],\n",
       "\n",
       "        [[False, False, False],\n",
       "         [False, False, False],\n",
       "         [False, False, False]],\n",
       "\n",
       "        [[False, False, False],\n",
       "         [False, False, False],\n",
       "         [False, False, False]]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = a[0] >1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3d8afb26-3af0-46cb-b413-496dde0e6a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False],\n",
       "         [False, False, False],\n",
       "         [False, False, False]],\n",
       "\n",
       "        [[False, False, False],\n",
       "         [ True,  True,  True],\n",
       "         [False, False, False]],\n",
       "\n",
       "        [[False, False, False],\n",
       "         [False, False, False],\n",
       "         [False, False, False]]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=p.unsqueeze(0)\n",
    "p =p[0][0] >1\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ceb56d04-21a2-4c57-9c3a-4ba1d34ef02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=p.numpy()\n",
    "a=a.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "31649e5e-21ac-480c-8109-88ac4e18a36d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]],\n",
       "\n",
       "       [[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p &  a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cecea00-3892-480f-bb3d-c5542cf9bdee",
   "metadata": {},
   "source": [
    "画像の表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "048a8565-435c-4b6a-82d2-e3ce2c9f4e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを推論に切り替える\n",
    "# 毎回同じ１２を確認する。そのためrシャッフルされている可能性のあるリストをソートする\n",
    "\n",
    "\n",
    "def logImages(self, epoch_ndx, mode_str, dl):\n",
    "        self.segmentation_model.eval()\n",
    "\n",
    "        images = sorted(dl.dataset.series_list)[:12]\n",
    "        for series_ndx, series_uid in enumerate(images):\n",
    "            ct = getCt(series_uid)\n",
    "\n",
    "            for slice_ndx in range(6):\n",
    "                ct_ndx = slice_ndx * (ct.hu_a.shape[0] - 1) // 5\n",
    "                sample_tup = dl.dataset.getitem_fullSlice(series_uid, ct_ndx)\n",
    "                # 出力はct_t, pos_t, ct.series_uid, slice_ndx\n",
    "\n",
    "                ct_t, label_t, series_uid, ct_ndx = sample_tup\n",
    "\n",
    "                input_g = ct_t.to(self.device).unsqueeze(0)\n",
    "                label_g = pos_g = label_t.to(self.device).unsqueeze(0)\n",
    "                # Unetのラッパーへ入力はバッチを追加、出力はprediction_g\n",
    "                prediction_g = self.segmentation_model(input_g)[0]\n",
    "                # バッチを追加したから一つ目を\n",
    "                prediction_a = prediction_g.to('cpu').detach().numpy()[0] > 0.5\n",
    "                # 追加したバッチと、チャネルの一次元目を（今回は一次元しかない）\n",
    "                label_a = label_g.cpu().numpy()[0][0] > 0.5\n",
    "\n",
    "                ct_t[:-1, :, :] /= 2000\n",
    "                ct_t[:-1, :, :] += 0.5\n",
    "\n",
    "                ctSlice_a = ct_t[dl.dataset.contextSlices_count].numpy()\n",
    "\n",
    "                # カラーで表示する\n",
    "                # 白黒を三つにコピーしてRGBとする\n",
    "                \n",
    "                image_a = np.zeros((512, 512, 3), dtype=np.float32)\n",
    "                image_a[:, :, :] = ctSlice_a.reshape((512, 512, 1))\n",
    "                # 偽陽性は赤\n",
    "                image_a[:, :, 0] += prediction_a & (1 - label_a)\n",
    "                # 偽陰性は０と１に値が入り、オレンジで\n",
    "                image_a[:, :, 0] += (1 - prediction_a) & label_a\n",
    "                image_a[:, :, 1] += ((1 - prediction_a) & label_a) * 0.5\n",
    "                # 真陽性は緑\n",
    "                image_a[:, :, 1] += prediction_a & label_a\n",
    "                image_a *= 0.5\n",
    "                \n",
    "                # データオーギュメンテーションで値が範囲外になっている可能性があり、再度クリップ\n",
    "                image_a.clip(0, 1, image_a)\n",
    "\n",
    "                writer = getattr(self, mode_str + '_writer')\n",
    "                writer.add_image(\n",
    "                    f'{mode_str}/{series_ndx}_prediction_{slice_ndx}',\n",
    "                    image_a,\n",
    "                    self.totalTrainingSamples_count,\n",
    "                    dataformats='HWC',\n",
    "                )\n",
    "\n",
    "                if epoch_ndx == 1:\n",
    "                    image_a = np.zeros((512, 512, 3), dtype=np.float32)\n",
    "                    image_a[:, :, :] = ctSlice_a.reshape((512, 512, 1))\n",
    "                    # image_a[:,:,0] += (1 - label_a) & lung_a # Red\n",
    "                    image_a[:, :, 1] += label_a  # Green\n",
    "                    # image_a[:,:,2] += neg_a  # Blue\n",
    "\n",
    "                    image_a *= 0.5\n",
    "                    image_a[image_a < 0] = 0\n",
    "                    image_a[image_a > 1] = 1\n",
    "                    writer.add_image(\n",
    "                        '{}/{}_label_{}'.format(\n",
    "                            mode_str,\n",
    "                            series_ndx,\n",
    "                            slice_ndx,\n",
    "                        ),\n",
    "                        image_a,\n",
    "                        self.totalTrainingSamples_count,\n",
    "                        # 出力そのままは\"CHW\"pytorchは\"HWC\"で三つ目にカラーがあることを表記する\n",
    "                        dataformats='HWC',\n",
    "                    )\n",
    "                # This flush prevents TB from getting confused about which\n",
    "                # data item belongs where.\n",
    "                writer.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a25e5-eb2b-42cb-803a-620406ed789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "評価はF1とrecallで確認するが、偽陰性を無くしたいので、モデル更新にはrecallの損失を用いる\n",
    "\n",
    "デメリットとして偽陽性は増加することが考えられるが、三ステップの初めなので次の分類ステップで除外されることを期待する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df32ec30-cf38-40f1-91d6-bdecb2966e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    " def logMetrics(self, epoch_ndx, mode_str, metrics_t):\n",
    "        log.info(\"E{} {}\".format(\n",
    "            epoch_ndx,\n",
    "            type(self).__name__,\n",
    "        ))\n",
    "\n",
    "        metrics_a = metrics_t.detach().numpy()\n",
    "        sum_a = metrics_a.sum(axis=1)\n",
    "        assert np.isfinite(metrics_a).all()\n",
    "\n",
    "        allLabel_count = sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]\n",
    "\n",
    "        metrics_dict = {}\n",
    "        metrics_dict['loss/all'] = metrics_a[METRICS_LOSS_NDX].mean()\n",
    "\n",
    "        metrics_dict['percent_all/tp'] = \\\n",
    "            sum_a[METRICS_TP_NDX] / (allLabel_count or 1) * 100\n",
    "        metrics_dict['percent_all/fn'] = \\\n",
    "            sum_a[METRICS_FN_NDX] / (allLabel_count or 1) * 100\n",
    "        metrics_dict['percent_all/fp'] = \\\n",
    "            sum_a[METRICS_FP_NDX] / (allLabel_count or 1) * 100\n",
    "\n",
    "        precision = metrics_dict['pr/precision'] = sum_a[METRICS_TP_NDX] \\\n",
    "            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FP_NDX]) or 1)\n",
    "        recall = metrics_dict['pr/recall'] = sum_a[METRICS_TP_NDX] \\\n",
    "            / ((sum_a[METRICS_TP_NDX] + sum_a[METRICS_FN_NDX]) or 1)\n",
    "\n",
    "        metrics_dict['pr/f1_score'] = 2 * (precision * recall) \\\n",
    "            / ((precision + recall) or 1)\n",
    "\n",
    "        log.info((\"E{} {:8} \"\n",
    "                  + \"{loss/all:.4f} loss, \"\n",
    "                  + \"{pr/precision:.4f} precision, \"\n",
    "                  + \"{pr/recall:.4f} recall, \"\n",
    "                  + \"{pr/f1_score:.4f} f1 score\"\n",
    "                  ).format(\n",
    "            epoch_ndx,\n",
    "            mode_str,\n",
    "            **metrics_dict,\n",
    "        ))\n",
    "        log.info((\"E{} {:8} \"\n",
    "                  + \"{loss/all:.4f} loss, \"\n",
    "                  + \"{percent_all/tp:-5.1f}% tp, {percent_all/fn:-5.1f}% fn, {percent_all/fp:-9.1f}% fp\"\n",
    "                  ).format(\n",
    "            epoch_ndx,\n",
    "            mode_str + '_all',\n",
    "            **metrics_dict,\n",
    "        ))\n",
    "\n",
    "        self.initTensorboardWriters()\n",
    "        writer = getattr(self, mode_str + '_writer')\n",
    "\n",
    "        prefix_str = 'seg_'\n",
    "\n",
    "        for key, value in metrics_dict.items():\n",
    "            writer.add_scalar(prefix_str + key, value,\n",
    "                              self.totalTrainingSamples_count)\n",
    "\n",
    "        writer.flush()\n",
    "\n",
    "        score = metrics_dict['pr/recall']\n",
    "\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6fac781-d2f4-43fe-bd1f-c74118b5931c",
   "metadata": {},
   "source": [
    "## モデルのセーブ\n",
    "\n",
    "mainの中で、scoreを保存し、best Scoreも更新した\n",
    "またepockごとにモデルのセーブを行う。\n",
    "\n",
    "pythonのpickleでもセーブできるが、モデルのインスタンスをそのままセーブするため柔軟性に欠ける\n",
    "そのためモデルのパラメータのみ保存する。\n",
    "\n",
    "パラメータのみであれば、クラスが異なってもパラメータ形状が同じであれば読み込むことが出きる。\n",
    "\n",
    "model.state_dict()でモデルのパラメータを取得する\n",
    "\n",
    "\n",
    "mainではself.saveModel('seg', epoch_ndx, score == best_score)ここで使われる。\n",
    "best_scoreであれば入力にTrueが渡され、modelとは別にbestmodelもセーブされる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90875cd8-aeec-4d5c-8544-c7c287c4589f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def saveModel(self, type_str, epoch_ndx, isBest=False):\n",
    "        file_path = os.path.join(\n",
    "            'data-unversioned',\n",
    "            'part2',\n",
    "            'models',\n",
    "            self.cli_args.tb_prefix,\n",
    "            '{}_{}_{}.{}.state'.format(\n",
    "                type_str,\n",
    "                self.time_str,\n",
    "                self.cli_args.comment,\n",
    "                self.totalTrainingSamples_count,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        os.makedirs(os.path.dirname(file_path), mode=0o755, exist_ok=True)\n",
    "\n",
    "        model = self.segmentation_model\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            model = model.module\n",
    "\n",
    "        state = {\n",
    "            'sys_argv': sys.argv,\n",
    "            'time': str(datetime.datetime.now()),\n",
    "            'model_state': model.state_dict(),\n",
    "            'model_name': type(model).__name__,\n",
    "            'optimizer_state': self.optimizer.state_dict(),\n",
    "            'optimizer_name': type(self.optimizer).__name__,\n",
    "            'epoch': epoch_ndx,\n",
    "            'totalTrainingSamples_count': self.totalTrainingSamples_count,\n",
    "        }\n",
    "        torch.save(state, file_path)\n",
    "\n",
    "        log.info(\"Saved model params to {}\".format(file_path))\n",
    "\n",
    "        if isBest:\n",
    "            best_path = os.path.join(\n",
    "                'data-unversioned', 'part2', 'models',\n",
    "                self.cli_args.tb_prefix,\n",
    "                f'{type_str}_{self.time_str}_{self.cli_args.comment}.best.state')\n",
    "            shutil.copyfile(file_path, best_path)\n",
    "\n",
    "            log.info(\"Saved model params to {}\".format(best_path))\n",
    "\n",
    "        with open(file_path, 'rb') as f:\n",
    "            log.info(\"SHA1: \" + hashlib.sha1(f.read()).hexdigest())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b408262-f193-4a5c-a05c-ba2d8d3b2bb7",
   "metadata": {},
   "source": [
    "## 結果の評価\n",
    "\n",
    "訓練では真陽性は増加、F1も増加、偽陽性と偽陰性は低下していき期待通りの動き。\n",
    "検証では画像サイズが64から512（64倍）となり、真陽性や偽陰性、偽陽性の比率は大きく変わるはず（陰性のピクセルの割合が大きくなるため）\n",
    "しかしモデル更新のためのrecallが10エポックから伸びないため過学習が予測される。\n",
    "\n",
    "ここではNが増えても良いので再現率のみで評価し次に進む。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96c1e5a-3f8d-497e-a0ad-82949081cdd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
