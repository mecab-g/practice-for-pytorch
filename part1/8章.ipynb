{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206a74cb-b50f-43bf-98a1-5e8a68dd6685",
   "metadata": {},
   "source": [
    "## 畳み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef141348-455c-4b11-ab99-bc7cc4b58f9c",
   "metadata": {},
   "source": [
    "畳み込みとは\n",
    "\n",
    "あるピクセルと隣接するピクセルとの荷重和を作成する計算である。\n",
    "この計算により、局所的なパターンが画像内の物体位置に関わらず出力に効果的である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0225d1ac-db26-489f-8e3b-1d9a7736acb3",
   "metadata": {},
   "source": [
    "重み行列（カーネル）のスカラー積を出力していく\n",
    "カーネルサイズは一般的に小さなサイズを使用し、二次元画像には３＊３、RBGの画像には３＊３＊３のカーネルが使用される。\n",
    "\n",
    "カーネルの重みはnn.Linearの重みと同様に学習してい値である。そしてこのカーネルの重みは画像全体に渡り利用される。\n",
    "そのためカーネルの重みは画像全体から影響を受けることになる。\n",
    "\n",
    "全結合層を畳み込みに変更することで、下記のメリットがある\n",
    "\n",
    "１．近傍における局所的な演算処理\n",
    "２．移動不変性\n",
    "３．パラメータを大幅に削減したモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4b128-5758-4d21-b872-e166ee28b033",
   "metadata": {},
   "source": [
    "## 畳み込みの使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89062e4c-15a3-4609-be26-0726003ef865",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb6328-1ff1-4781-9a94-5186e075f26c",
   "metadata": {},
   "source": [
    "出力チャネルのサイズは任意の値が選択できる。\n",
    "このチャネルの数が増えるほどパラメータが増加し、特徴量の検出も増大する。\n",
    "\n",
    "カーネルのサイズはすべての方向に同じことが一般的だが、CTのボクセルのうち一つが異なる解像度であるような場合は、例外的に異なるサイズを使用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46ca71f-ec15-43e7-8974-36bc85649f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(3, 16, kernel_size=3)\n",
    "conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39f098af-ca60-46eb-af77-c687c78e39ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 3, 3, 3]), torch.Size([16]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.weight.shape, conv.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b6ce25e-1e99-4090-a718-63e78ed435fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/p1ch7/\"\n",
    "#引数は保存のパス、訓練用かテスト用か、pytorchで保存して良いか\n",
    "#他にもSVHN ,COCO,Ominglotなどがある\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "339322ae-9d9a-45bc-825c-3ce6b48648c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10 = datasets.CIFAR10(\n",
    "            data_path, train=True, download=False, \n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                     (0.1470, 0.2435, 0.2626))\n",
    "            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bff76a8-5769-4871-8fbe-1f27b4d96fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16084e3d-6b95-4124-8320-37d13e8a3bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cifar10の中から飛行機と鳥のみのdatasetsにする\n",
    "label_map = {0:0, 2:1}\n",
    "class_names = ['airplane', 'bird']\n",
    "cifar2 = [(img, label_map[label]) for img, label in cifar10 if label in [0,2]]\n",
    "cifar2_val = [(img, label_map[label]) for img, label in cifar10_val if label in [0,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdde8ad0-c36d-466d-a01d-9efd43a2df14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img, _ = cifar2[0]\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9ffc773-c831-4af5-a87c-5c1bfaef735f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 16, 30, 30]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "img.unsqueeze((0)).shape, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eeeb722-93b5-4272-bb31-b8cd35e5734a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 30])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ead3d316-5856-4106-be82-411b76e4df8f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "_peLHw3CTeVC",
    "outputId": "950db33c-99fd-45d8-b274-6d66225182c6"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x345.6 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 4.8))  # 本では省略\n",
    "ax1 = plt.subplot(1, 2, 1)   # 本では省略\n",
    "plt.title('output')   # 本では省略\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)  # 本では省略\n",
    "plt.imshow(img.mean(0), cmap='gray')  \n",
    "plt.title('input')  # 本では省略\n",
    "#plt.savefig('Ch8_F2_PyTorch.png')  # 本では省略\n",
    "plt.show()\n",
    "\n",
    "#sharex = XXX：x軸の共有設定\n",
    "#sharey = XXX：y軸の共有設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba6848-48c3-45d9-b4a8-d988d10befa1",
   "metadata": {},
   "source": [
    "入力に対して出力の画像サイズが小さくなっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d517a79-f790-4065-bfd5-6166b812e6e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 境界のパディング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8471318-5f88-4142-91bd-ae77a18a7056",
   "metadata": {},
   "source": [
    "畳み込み処理では全ての方向に隣接したピクセルが存在する必要がある。\n",
    "角の位置では上下どちらか、左右どちらかが存在しない。デフォルトではPyTorchではwidth - kernl_width +1 の位置までを取得する。\n",
    "奇数サイズのカーネルではカーネルの幅の半分（今回は3//2=1)だけ各境界が小さくなった画像が出力された。\n",
    "\n",
    "pytorchでは畳み込み時に境界領域にゼロになｋる架空のピクセルを作成して画像をパディングできる。\n",
    "今回の件ではkernel_size=3*3, padeing=1 を指定するともとの画像と同じサイズの出力が得られる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73f3df62-e207-48ea-9bf4-839795171ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "output = conv(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a042ed4-eb43-490d-be69-f61dfb8f906a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 1, 32, 32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.unsqueeze(0).shape, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c871475-c0d3-4d9a-9073-1bfcbf6778f6",
   "metadata": {},
   "source": [
    "画像サイズが変化しないことはスキップ接続や複雑な構造のモデルでテンソル同士の加減算が可能になる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810c58b7-9e45-4e97-b6a2-cb41f49438e1",
   "metadata": {},
   "source": [
    "## 畳み込みの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14e9a9-fd4d-4023-b675-9b82419036a4",
   "metadata": {},
   "source": [
    "手動でのパラメータ設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d6d06-5619-4ff3-b3f2-f53103a1e94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#バイアスをゼロにする\n",
    "with torch.no_grad():\n",
    "    conv.bias.zero_()\n",
    "    \n",
    "with torch.no_grad():\n",
    "    conv.weight.fill_(1.0 / 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f9515-03f2-4536-88be-034c4a0f0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f0de35-65fe-4bfc-a08c-b2ec9f23db88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "VUQhUyEoTeVT",
    "outputId": "024b9a6a-2d2e-4863-ea9a-a26b7820192c"
   },
   "outputs": [],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.figure(figsize=(10, 4.8))  # 本では省略\n",
    "ax1 = plt.subplot(1, 2, 1)   # 本では省略\n",
    "plt.title('output_padding')   # 本では省略\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)  # 本では省略\n",
    "plt.imshow(img.mean(0), cmap='gray')  # 本では省略\n",
    "plt.title('input')  # 本では省略\n",
    "#plt.savefig('Ch8_F4_PyTorch.png')  # 本では省略\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe5b4e-1501-4bd2-a497-b5a173746a67",
   "metadata": {},
   "source": [
    "convの一定の重み（フィルタ）によってボケた画像が出力された。\n",
    "畳み込みは出力ピクセル間に相関が生まれ、スムーズに画像が変化する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfd4753-f449-496d-b86b-3c17471a111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09045477-1f06-4b11-9aab-b9430dec94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#右側のピクセルから左側のピクセルを減算するフィルタ\n",
    "#強度が異なる領域の垂直方向の境界になっているピクセルに対しては出力の値は大きな値となる。\n",
    "#一方一様な領域であれば0に近い値となる。\n",
    "\n",
    "#つまり垂直方向のエッジ検出のカーネルとなる\n",
    "conv = nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "with torch.no_grad():\n",
    "    conv.weight[:] = torch.tensor([[-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0],\n",
    "                                   [-1.0, 0.0, 1.0]])\n",
    "    conv.bias.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6deed91-a931-4380-b63d-821af7872626",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e408be0-467d-47a9-aabe-173022526ebb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "VUQhUyEoTeVT",
    "outputId": "024b9a6a-2d2e-4863-ea9a-a26b7820192c"
   },
   "outputs": [],
   "source": [
    "output = conv(img.unsqueeze(0))\n",
    "plt.figure(figsize=(10, 4.8))  # 本では省略\n",
    "ax1 = plt.subplot(1, 2, 1)   # 本では省略\n",
    "plt.title('output_padding')   # 本では省略\n",
    "plt.imshow(output[0, 0].detach(), cmap='gray')\n",
    "plt.subplot(1, 2, 2, sharex=ax1, sharey=ax1)  # 本では省略\n",
    "plt.imshow(img.mean(0), cmap='gray')  # 本では省略\n",
    "plt.title('input')  # 本では省略\n",
    "#plt.savefig('Ch8_F4_PyTorch.png')  # 本では省略\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b99283-9585-48c4-9b4c-eadee1fb9783",
   "metadata": {},
   "source": [
    "ディープラーニングでは最も効果的なカーネルの値を推定する。\n",
    "学習は入力と出力の間のクロスエントロピー誤差を最小化するカーネルの値を推測していく。\n",
    "\n",
    "畳み込みニューラルネットワークは複数チャネルの画像を別の複数チャネルの画像に変換する連続した層のフィルタ群を推定することである。\n",
    "この異なるチャネルは異なる特徴量に対応する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fb7802-dd64-4374-9605-f4f4f67cad8d",
   "metadata": {},
   "source": [
    "## 深さとプーリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cc64dc-7596-436d-8380-b4ffc1dbc926",
   "metadata": {},
   "source": [
    "CIFAR10の画像は小さく小さなカーネルで局所的な特徴を得た。\n",
    "大きな画像で広い範囲の構造を把握したい場合はどうすれば良いか？\n",
    "\n",
    "大きなカーネルを使用することで解決できるが、今度は元々の畳み込みの利点がなくなる。\n",
    "\n",
    "畳み込みの利点を活かし、かつ大きな範囲の構造も把握するためには畳み込み後に畳み込みを繰り返しその間にダウンサンプリングをする方法がある"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a966e-83d5-4ca1-b656-f4ba4e110393",
   "metadata": {},
   "source": [
    "## ダウンサンプリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473a2d4-025b-407a-b5b1-5a1123196539",
   "metadata": {},
   "source": [
    "画像を半分に縮小することは隣接している4つのピクセルを1つのピクセルにする\n",
    "\n",
    "1.アベレージプーリング\n",
    "2.マックスプーリング\n",
    "3.ストライド畳み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9cbe6c33-e776-41dc-a283-dcef574ae5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85042ab7-95d4-4fa8-ab0d-cfced71abaa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 32, 32]), torch.Size([1, 3, 16, 16]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_pooling\n",
    "#画像を半分にしたいときはインスタンス引数に2を入れる\n",
    "\n",
    "pool = nn.MaxPool2d(2)\n",
    "output = pool(img.unsqueeze(0))\n",
    "img.unsqueeze(0).shape , output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fbcbad-3645-49fa-9b3c-37f30fb95752",
   "metadata": {},
   "source": [
    "はじめに小さなカーネルで畳み込みを行い、局所の特徴を掴み、\n",
    "ダウンサンプリングした縮小後の画像で畳み込みを行う（元の画像で考えると倍の広い領域の特徴を掴む）\n",
    "\n",
    "1つ目のカーネルは低レベルの特徴を、2つ目のカーネルは広い領域で効果的に動作し、前の特徴量を合成した特徴量を生成する。\n",
    "\n",
    "このため複雑なケースでも対応できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0324e7d-3edb-4a85-a66c-c79b7a6e2df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダウンサンプリングを入れたモデル\n",
    "model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2),\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb4ed49c-7a95-49fb-8305-52dddb881eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 8, 8])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 出力は8チャネルの8＊8の画像\n",
    "model(img.unsqueeze(0)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1996cdde-3138-4e5c-8491-8c46054cec08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Tanh()\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Tanh()\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Linear(in_features=512, out_features=32, bias=True)\n",
       "  (7): Tanh()\n",
       "  (8): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 出力はとりか飛行機の2値にする必要がある\n",
    "# 画像を32のベクトルにして、最後2値にする\n",
    "model = nn.Sequential(\n",
    "        nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Conv2d(16, 8, kernel_size=3, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.Linear(8 * 8 * 8, 32),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(32, 2)\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df6306dc-9014-402e-bd3e-cab453fa9e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パラメータ確認\n",
    "num_list = [p.numel() for p in model.parameters()]\n",
    "sum(num_list), num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1edc5ef-b30c-4a74-8ddd-7ff4fff2097d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x8 and 512x32)"
     ]
    }
   ],
   "source": [
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a43077c-fd2e-4a99-aa98-bdbc104d1980",
   "metadata": {},
   "source": [
    "## nn.Moduleを継承してモデルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62504e0d-d4b8-4e91-8214-6b32bd0834e3",
   "metadata": {},
   "source": [
    "モデル内で入力が二次元から一次元へと形式が変化するような場合はnn.sequeintal()では作成できない\n",
    "（次元数が変化しないモデルを作成し。最後にLinearする手もある：キカガク参考）\n",
    "\n",
    "nn.Moduleのサブクラス(子クラス）を用いて作成する\n",
    "\n",
    "nn.Moduleを用いる場合はfoward関数を定義して、入力か出力への流れを記載する。\n",
    "backwardはtorchのテンソルでは自動でおこなってくれる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "525974cd-7b6c-408e-9af6-e5bfcdb3930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチ数が不明のためviewの引数は-1\n",
    "# CNNの有名なモデルRESNETなどでは解像度を落としながら、チャネルを増やしていく（結果的にはサイズは縮小する）　\n",
    "nn.Moduleのインスタンスをnn.Moduleの\n",
    "\n",
    "nn.Moduleの\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ffa8d1e4-af22-4a1b-9aa5-16deee5cabb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18090, [432, 16, 1152, 8, 16384, 32, 64, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abf1170f-a58c-4e6d-8bf8-abcf5bca6091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0828, -0.1059]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c894c4bd-b8b3-4028-b97c-1b744f805853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
