{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9778228-0187-4158-9c09-56f42438e879",
   "metadata": {},
   "source": [
    "## モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d7a0434-d149-4ad9-bf4a-b41f4d4edbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの収集\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acfd7014-31cf-4b68-beda-93283dbce705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_cは摂氏\n",
    "#t_uは単位が知りたい値\n",
    "t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\n",
    "t_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\n",
    "t_c = torch.tensor(t_c)\n",
    "t_u = torch.tensor(t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0da51c26-80c8-448d-9cce-abe1552d3c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f9bcb5a7940>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(t_c, t_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ac0a958-e483-4824-bc90-6a8f6825de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#データを収集したら、可視化、線形モデルをためす。\n",
    "#線形モデル　t_c = w * t_u + b\n",
    "#予測値と測定値の誤差が小さくなるようにモデルのパラメータ（ｗ：重みとｂ：バイアス）を推定する\n",
    "#誤差の測定方法に損失関数を用いる\n",
    "#この損失関数が最小になるようなw,bを見つけることを目標とする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436d561-406b-4364-a939-6ae842d31d78",
   "metadata": {},
   "source": [
    "## パラメータの推定と損失関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ceec7d2-05e5-4ee5-8db8-5448ef174b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデル作成\n",
    "def model (t_u, w, b):\n",
    "    return w* t_u + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "896e0b03-2b33-4438-bd6b-00d6338c9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4f4da0a-7c66-4472-b693-71bfc1d0e8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#パラメータの初期化\n",
    "w = torch.ones(())\n",
    "b = torch.zeros(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77934ae6-367e-4d68-8efe-964cfbfa28c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b52e40bb-c368-41ae-a8e1-d124132d7f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u ,w ,b)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "416ee3b3-6f85-4403-9f5d-6516f151f3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#この段階で損失\n",
    "loss = loss_fn(t_p, t_c)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e58bb7-4aa0-4d95-baf2-dfd0dc05dfe1",
   "metadata": {},
   "source": [
    "# ブロードキャスティング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5aeed0-9ccd-43ed-8f59-0accbc859184",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910dc9b-9dc5-4220-a57c-36b7c9c988fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1522e87a-14fc-4924-bb3d-e8fb152ff15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: x:torch.Size([]) y:torch.Size([3, 1])\n",
      "z:torch.Size([1, 3]), a:torch.Size([2, 1, 1])\n",
      "x * y: torch.Size([3, 1])\n",
      "y * z: torch.Size([3, 3])\n",
      "x * z * a: torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "#あるテンソルの１つが、他のテンソルよりも大きい次元の場合に他のテンソル全体にあるテンソルの次元に沿った\n",
    "#計算がされる\n",
    "import torch\n",
    "x = torch.ones(())\n",
    "y = torch.ones(3,1)\n",
    "z = torch.ones(1,3)\n",
    "a = torch.ones(2,1,1)\n",
    "print(f\"shape: x:{x.shape} y:{y.shape}\")\n",
    "print(f\"z:{z.shape}, a:{a.shape}\")\n",
    "print(\"x * y:\", (x*y).shape)\n",
    "print(\"y * z:\", (y*z).shape)\n",
    "print(\"x * z * a:\", (y*z*a).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8ccb73b-9893-4467-8442-a92edd4e9bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.]]),\n",
       " tensor([[[1.]],\n",
       " \n",
       "         [[1.]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd727cd8-4610-46b4-a062-f5486068633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失を小さくするためには"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479cafc-809d-41fe-b77f-1617701e9915",
   "metadata": {},
   "source": [
    "# 勾配降下法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe694f73-428a-48ea-8409-bcd1e8bb9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#各パラメータに対する損失の変化率を計算し、損失が減少する方向に\n",
    "#各パラメータを補正する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f32ac6d2-5db2-4136-963c-bc950297239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "\n",
    "#wの変化が損失につながる\n",
    "loss_rate_of_change_w = \\\n",
    "(loss_fn(model(t_u, w + delta, b),t_c)-\n",
    " loss_fn(model(t_u, w - delta, b), t_c)) /(2.0 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15d9c957-fb7d-46b0-98a0-bf4105f02bfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4517.2974)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rate_of_change_w \n",
    "#この損失の変化量が負の場合は損失を最小化するためにwを増加させる必要がある\n",
    "#変化量が正の場合はwを減少させる必要がある\n",
    "\n",
    "#どのくらい変化させた方がいいか\n",
    "#損失の変化量に比例した変更をwに適用したらよいのではないか\n",
    "#変化量を調整する係数を学習率と呼ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab0831e3-01f1-43e8-95f2-5282c0de4f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-2\n",
    "w = w - learning_rate * loss_rate_of_change_w\n",
    "\n",
    "loss_rate_of_change_b = \\\n",
    "(loss_fn(model(t_u, w, b + delta),t_c)-\n",
    " loss_fn(model(t_u, w, b - delta),t_c))/(2.0 * delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be44ac14-c98c-4127-8180-3ba7e663438a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(46.)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = b - learning_rate * loss_rate_of_change_b\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d48359-83ba-408d-af97-f29e8dc44253",
   "metadata": {},
   "source": [
    "パラメータが複数あるモデルでは各パラメータの損失の個々の導関数を計算する\n",
    "この計算結果を導関数のベクトル、すなわち勾配と呼ぶ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188c19fc-de2b-4e77-8289-de554b8ea21d",
   "metadata": {},
   "source": [
    "導関数の計算\n",
    "損失の導関数を計算するために、連鎖律をつかう\n",
    "\n",
    "(入力）モデルの出力に関する導関数を、パラメータに関するモデルの導関数と掛け算してもとめる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214f7a2-b5e9-4103-85b6-7efead2efcc6",
   "metadata": {},
   "source": [
    "d loss_fn / dw = (d loss_fn / d t_p) * (d t_p / dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "814ae84e-ccf8-4c97-b2ec-0b5b3f759bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#損失関数を再び定義\n",
    "def loss_fn(t_p, t_c):\n",
    "    squred_differs= (t_p -t_c)**2\n",
    "    return squred_differs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48026639-d60d-4934-8b3b-ad633c41924a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1763.8848)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(t_p, t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dbd305f-1e09-4ef8-b481-806c8e7bb76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルは線形 dx*2 /dx = 2x\n",
    "#損失の導関数は\n",
    "def dloss_fn(t_p, t_c):\n",
    "    dsq_diffs = 2*(t_p - t_c) / t_p.size(0)#除算は平均化のため\n",
    "    return dsq_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f12257be-3269-4e5a-b5b9-9dd6cae78732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.4000, 7.6182, 7.8545, 9.8000, 8.2364, 7.4364, 5.6182, 4.6909, 7.7091,\n",
       "         8.6182, 8.6182]),\n",
       " torch.Size([11]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dloss_fn(t_p, t_c), t_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6b18c40-9b62-4431-8281-46e03fb351dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model　は　w*t_u + b \n",
    "\n",
    "#このmodelの導関数は\n",
    "def dmodel_dw(t_u, w, b):\n",
    "    return t_u\n",
    "\n",
    "def dmodel_db(t_u, w, b):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4bffc01a-8a15-4e51-9f1e-5a1ff2f50415",
   "metadata": {},
   "outputs": [],
   "source": [
    "#勾配の関数\n",
    "#損失の導関数に導関数を掛ける（連鎖律）\n",
    "def  grad_fn(t_u, t_c, t_p, w, b):\n",
    "    dloss_dtp = dloss_fn(t_p, t_c)#損失の導関数\n",
    "    dloss_dw  = dloss_dtp * dmodel_dw(t_u, w, b)#損失の導関数＊weightの導関数　ー＞　パラメータに対する損失の導関数\n",
    "    dloss_db  = dloss_dtp * dmodel_db(t_u, w, b)\n",
    "    return torch.stack([dloss_dw.sum(), dloss_db.sum()])\n",
    "#各パラメータの導関数を求める\n",
    "#この導関数の集まり（ベクトル）を勾配と呼ぶ\n",
    "#この出力（パラメータの勾配）を(学習率をかけてから）各パラメータに加えることでモデルを最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f02e3bc9-42f8-448f-863f-373b4fc35b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#反復して繰り返す必要がある\n",
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs +1):\n",
    "        w, b = params\n",
    "        #順伝播\n",
    "        t_p = model(t_u, w, b)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        #逆伝播\n",
    "        grad = grad_fn(t_u, t_c, t_p, w, b)\n",
    "        \n",
    "      \n",
    "        \n",
    "        #パラメータの更新\n",
    "        params = params - learning_rate * grad\n",
    "        \n",
    "        print('Epoch %d, Loss %f' % ( epoch, float(loss)))\n",
    "        print(f\"grad:{grad}\")\n",
    "        print(f\"params:{params}\")\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b82355f0-3893-4f43-a379-1d9a281c2254",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "grad:tensor([4517.2964,   82.6000])\n",
      "params:tensor([-44.1730,  -0.8260])\n",
      "Epoch 2, Loss 5802484.500000\n",
      "grad:tensor([-261257.4062,   -4598.9702])\n",
      "params:tensor([2568.4011,   45.1637])\n",
      "Epoch 3, Loss 19408029696.000000\n",
      "grad:tensor([15109614.0000,   266155.6875])\n",
      "params:tensor([-148527.7344,   -2616.3931])\n",
      "Epoch 4, Loss 64915905708032.000000\n",
      "grad:tensor([-8.7385e+08, -1.5393e+07])\n",
      "params:tensor([8589999.0000,  151310.8906])\n",
      "Epoch 5, Loss 217130525461053440.000000\n",
      "grad:tensor([5.0539e+10, 8.9023e+08])\n",
      "params:tensor([-4.9680e+08, -8.7510e+06])\n",
      "Epoch 6, Loss 726257583152928129024.000000\n",
      "grad:tensor([-2.9229e+12, -5.1486e+10])\n",
      "params:tensor([2.8732e+10, 5.0610e+08])\n",
      "Epoch 7, Loss 2429183416467662896627712.000000\n",
      "grad:tensor([1.6904e+14, 2.9776e+12])\n",
      "params:tensor([-1.6617e+12, -2.9270e+10])\n",
      "Epoch 8, Loss 8125122549611731432050262016.000000\n",
      "grad:tensor([-9.7764e+15, -1.7221e+14])\n",
      "params:tensor([9.6102e+13, 1.6928e+12])\n",
      "Epoch 9, Loss 27176882120842590626938030653440.000000\n",
      "grad:tensor([5.6541e+17, 9.9596e+15])\n",
      "params:tensor([-5.5580e+15, -9.7903e+13])\n",
      "Epoch 10, Loss 90901105189019073810297959556841472.000000\n",
      "grad:tensor([-3.2700e+19, -5.7600e+17])\n",
      "params:tensor([3.2144e+17, 5.6621e+15])\n",
      "Epoch 11, Loss inf\n",
      "grad:tensor([1.8912e+21, 3.3313e+19])\n",
      "params:tensor([-1.8590e+19, -3.2746e+17])\n",
      "Epoch 12, Loss inf\n",
      "grad:tensor([-1.0937e+23, -1.9266e+21])\n",
      "params:tensor([1.0752e+21, 1.8939e+19])\n",
      "Epoch 13, Loss inf\n",
      "grad:tensor([6.3256e+24, 1.1142e+23])\n",
      "params:tensor([-6.2181e+22, -1.0953e+21])\n",
      "Epoch 14, Loss inf\n",
      "grad:tensor([-3.6584e+26, -6.4441e+24])\n",
      "params:tensor([3.5962e+24, 6.3346e+22])\n",
      "Epoch 15, Loss inf\n",
      "grad:tensor([2.1158e+28, 3.7269e+26])\n",
      "params:tensor([-2.0798e+26, -3.6636e+24])\n",
      "Epoch 16, Loss inf\n",
      "grad:tensor([-1.2236e+30, -2.1554e+28])\n",
      "params:tensor([1.2028e+28, 2.1188e+26])\n",
      "Epoch 17, Loss inf\n",
      "grad:tensor([7.0769e+31, 1.2466e+30])\n",
      "params:tensor([-6.9566e+29, -1.2254e+28])\n",
      "Epoch 18, Loss inf\n",
      "grad:tensor([-4.0929e+33, -7.2095e+31])\n",
      "params:tensor([4.0233e+31, 7.0869e+29])\n",
      "Epoch 19, Loss inf\n",
      "grad:tensor([2.3671e+35, 4.1695e+33])\n",
      "params:tensor([-2.3268e+33, -4.0987e+31])\n",
      "Epoch 20, Loss inf\n",
      "grad:tensor([-1.3690e+37, -2.4114e+35])\n",
      "params:tensor([1.3457e+35, 2.3704e+33])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.3457e+35, 2.3704e+33])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 20,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3902bf96-3e70-410b-a7f7-c687fb767b57",
   "metadata": {},
   "source": [
    "\n",
    "損失が無限大になった"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e52f25d-dad4-42ab-9045-ef48fb543f8b",
   "metadata": {},
   "source": [
    "この訓練では変数が大きすぎる更新を受け取ったことを示す兆候である\n",
    "各更新がオーバーシュートして、次の更新がさらに過大評価され、最適化が収束する代わりに発散する\n",
    "本来はどんどん小さくしていきたい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53baf8-70d1-4dc0-a56f-954847202ff3",
   "metadata": {},
   "source": [
    "learning_rateは学習がうまくいかなかったときに変更することが多いパラメータの一つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7247d76e-1175-4b93-8a42-245435fdb9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 1763.884766\n",
      "grad:tensor([4517.2964,   82.6000])\n",
      "params:tensor([ 0.5483, -0.0083])\n",
      "Epoch 2, Loss 323.090515\n",
      "grad:tensor([1859.5493,   35.7843])\n",
      "params:tensor([ 0.3623, -0.0118])\n",
      "Epoch 3, Loss 78.929634\n",
      "grad:tensor([765.4666,  16.5122])\n",
      "params:tensor([ 0.2858, -0.0135])\n",
      "Epoch 4, Loss 37.552845\n",
      "grad:tensor([315.0790,   8.5787])\n",
      "params:tensor([ 0.2543, -0.0143])\n",
      "Epoch 5, Loss 30.540283\n",
      "grad:tensor([129.6733,   5.3127])\n",
      "params:tensor([ 0.2413, -0.0149])\n",
      "Epoch 6, Loss 29.351154\n",
      "grad:tensor([53.3495,  3.9682])\n",
      "params:tensor([ 0.2360, -0.0153])\n",
      "Epoch 7, Loss 29.148884\n",
      "grad:tensor([21.9304,  3.4148])\n",
      "params:tensor([ 0.2338, -0.0156])\n",
      "Epoch 8, Loss 29.113848\n",
      "grad:tensor([8.9964, 3.1869])\n",
      "params:tensor([ 0.2329, -0.0159])\n",
      "Epoch 9, Loss 29.107145\n",
      "grad:tensor([3.6721, 3.0930])\n",
      "params:tensor([ 0.2325, -0.0162])\n",
      "Epoch 10, Loss 29.105247\n",
      "grad:tensor([1.4803, 3.0544])\n",
      "params:tensor([ 0.2324, -0.0166])\n",
      "Epoch 11, Loss 29.104168\n",
      "grad:tensor([0.5781, 3.0384])\n",
      "params:tensor([ 0.2323, -0.0169])\n",
      "Epoch 12, Loss 29.103222\n",
      "grad:tensor([0.2066, 3.0318])\n",
      "params:tensor([ 0.2323, -0.0172])\n",
      "Epoch 13, Loss 29.102295\n",
      "grad:tensor([0.0537, 3.0291])\n",
      "params:tensor([ 0.2323, -0.0175])\n",
      "Epoch 14, Loss 29.101379\n",
      "grad:tensor([-0.0093,  3.0279])\n",
      "params:tensor([ 0.2323, -0.0178])\n",
      "Epoch 15, Loss 29.100466\n",
      "grad:tensor([-0.0353,  3.0274])\n",
      "params:tensor([ 0.2323, -0.0181])\n",
      "Epoch 16, Loss 29.099548\n",
      "grad:tensor([-0.0459,  3.0272])\n",
      "params:tensor([ 0.2323, -0.0184])\n",
      "Epoch 17, Loss 29.098631\n",
      "grad:tensor([-0.0502,  3.0270])\n",
      "params:tensor([ 0.2323, -0.0187])\n",
      "Epoch 18, Loss 29.097717\n",
      "grad:tensor([-0.0520,  3.0270])\n",
      "params:tensor([ 0.2323, -0.0190])\n",
      "Epoch 19, Loss 29.096796\n",
      "grad:tensor([-0.0528,  3.0269])\n",
      "params:tensor([ 0.2323, -0.0193])\n",
      "Epoch 20, Loss 29.095881\n",
      "grad:tensor([-0.0531,  3.0268])\n",
      "params:tensor([ 0.2323, -0.0196])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2323, -0.0196])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 20,\n",
    "    learning_rate = 1e-4,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_u,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa05f327-89c7-4a3d-b5de-abe3abed76a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 今度はlearnig_rateが小さく、更新も小さく途中で終わってしまった\n",
    "adaptiveな変更が良いのではないか\n",
    "\n",
    "もう一つ問題がある"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19b215-b8c5-4984-868e-915bd5d8d249",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 入力の正規化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2613b8-fd2a-467e-afc5-f30b98357127",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "勾配の大きさが重みと、バイアスで約50倍の差がある\n",
    "これはこれらのパラメータのスケールが異なるため\n",
    "\n",
    "片方のスケールに合わせた学習率では片方では大きすぎるか、または小さすぎる可能性がある。\n",
    "\n",
    "解決には問題の定式化の変更が必要\n",
    "入力の範囲を―１から１の範囲に収めればよいのではないか"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6dad3254-9b11-42e5-a3de-3b3e22dc1755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5000, 14.0000, 15.0000, 28.0000, 11.0000,  8.0000,  3.0000, -4.0000,\n",
       "         6.0000, 13.0000, 21.0000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#目的\n",
    "t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ea63801-036f-412e-9d91-c2aac2973e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([35.7000, 55.9000, 58.2000, 81.9000, 56.3000, 48.9000, 33.9000, 21.8000,\n",
       "        48.4000, 60.4000, 68.4000])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#入力\n",
    "t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1fd326b-4e07-4533-ae30-296b5ae05244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力値の変更\n",
    "#normalized:n\n",
    "t_un = 0.1 * t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbac6e66-5558-42bd-aeb4-6d745d1872f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "grad:tensor([-77.6140, -10.6400])\n",
      "params:tensor([1.7761, 0.1064])\n",
      "Epoch 2, Loss 37.574913\n",
      "grad:tensor([-30.8623,  -2.3864])\n",
      "params:tensor([2.0848, 0.1303])\n",
      "Epoch 3, Loss 30.871077\n",
      "grad:tensor([-12.4631,   0.8587])\n",
      "params:tensor([2.2094, 0.1217])\n",
      "Epoch 4, Loss 29.756193\n",
      "grad:tensor([-5.2218,  2.1327])\n",
      "params:tensor([2.2616, 0.1004])\n",
      "Epoch 5, Loss 29.507153\n",
      "grad:tensor([-2.3715,  2.6310])\n",
      "params:tensor([2.2853, 0.0740])\n",
      "Epoch 6, Loss 29.392456\n",
      "grad:tensor([-1.2492,  2.8241])\n",
      "params:tensor([2.2978, 0.0458])\n",
      "Epoch 7, Loss 29.298828\n",
      "grad:tensor([-0.8071,  2.8970])\n",
      "params:tensor([2.3059, 0.0168])\n",
      "Epoch 8, Loss 29.208717\n",
      "grad:tensor([-0.6325,  2.9227])\n",
      "params:tensor([ 2.3122, -0.0124])\n",
      "Epoch 9, Loss 29.119415\n",
      "grad:tensor([-0.5633,  2.9298])\n",
      "params:tensor([ 2.3178, -0.0417])\n",
      "Epoch 10, Loss 29.030489\n",
      "grad:tensor([-0.5355,  2.9295])\n",
      "params:tensor([ 2.3232, -0.0710])\n",
      "Epoch 11, Loss 28.941877\n",
      "grad:tensor([-0.5240,  2.9264])\n",
      "params:tensor([ 2.3284, -0.1003])\n",
      "Epoch 12, Loss 28.853565\n",
      "grad:tensor([-0.5190,  2.9222])\n",
      "params:tensor([ 2.3336, -0.1295])\n",
      "Epoch 13, Loss 28.765553\n",
      "grad:tensor([-0.5165,  2.9175])\n",
      "params:tensor([ 2.3388, -0.1587])\n",
      "Epoch 14, Loss 28.677851\n",
      "grad:tensor([-0.5150,  2.9126])\n",
      "params:tensor([ 2.3439, -0.1878])\n",
      "Epoch 15, Loss 28.590431\n",
      "grad:tensor([-0.5138,  2.9077])\n",
      "params:tensor([ 2.3491, -0.2169])\n",
      "Epoch 16, Loss 28.503319\n",
      "grad:tensor([-0.5129,  2.9028])\n",
      "params:tensor([ 2.3542, -0.2459])\n",
      "Epoch 17, Loss 28.416498\n",
      "grad:tensor([-0.5120,  2.8979])\n",
      "params:tensor([ 2.3593, -0.2749])\n",
      "Epoch 18, Loss 28.329973\n",
      "grad:tensor([-0.5111,  2.8930])\n",
      "params:tensor([ 2.3644, -0.3038])\n",
      "Epoch 19, Loss 28.243742\n",
      "grad:tensor([-0.5102,  2.8881])\n",
      "params:tensor([ 2.3695, -0.3327])\n",
      "Epoch 20, Loss 28.157804\n",
      "grad:tensor([-0.5093,  2.8832])\n",
      "params:tensor([ 2.3746, -0.3615])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 2.3746, -0.3615])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 20 ,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609616d-8634-4f41-a6fd-cdd1b48ab3f5",
   "metadata": {},
   "source": [
    "学習率が1e-2でもパラメータが爆発的に大きくならず、lossも落ち着いている\n",
    "勾配もどちらの値も大きな差がない\n",
    "\n",
    "今回は単純に入力値を1/10にして出力値に近づけたが、きちんとした正規化の方が良い働きをする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f8cc9404-6def-4572-a899-9e19285e47dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 80.364342\n",
      "grad:tensor([-77.6140, -10.6400])\n",
      "params:tensor([1.7761, 0.1064])\n",
      "Epoch 2, Loss 37.574913\n",
      "grad:tensor([-30.8623,  -2.3864])\n",
      "params:tensor([2.0848, 0.1303])\n",
      "Epoch 3, Loss 30.871077\n",
      "grad:tensor([-12.4631,   0.8587])\n",
      "params:tensor([2.2094, 0.1217])\n",
      "Epoch 4, Loss 29.756193\n",
      "grad:tensor([-5.2218,  2.1327])\n",
      "params:tensor([2.2616, 0.1004])\n",
      "Epoch 5, Loss 29.507153\n",
      "grad:tensor([-2.3715,  2.6310])\n",
      "params:tensor([2.2853, 0.0740])\n",
      "Epoch 6, Loss 29.392456\n",
      "grad:tensor([-1.2492,  2.8241])\n",
      "params:tensor([2.2978, 0.0458])\n",
      "Epoch 7, Loss 29.298828\n",
      "grad:tensor([-0.8071,  2.8970])\n",
      "params:tensor([2.3059, 0.0168])\n",
      "Epoch 8, Loss 29.208717\n",
      "grad:tensor([-0.6325,  2.9227])\n",
      "params:tensor([ 2.3122, -0.0124])\n",
      "Epoch 9, Loss 29.119415\n",
      "grad:tensor([-0.5633,  2.9298])\n",
      "params:tensor([ 2.3178, -0.0417])\n",
      "Epoch 10, Loss 29.030489\n",
      "grad:tensor([-0.5355,  2.9295])\n",
      "params:tensor([ 2.3232, -0.0710])\n",
      "Epoch 11, Loss 28.941877\n",
      "grad:tensor([-0.5240,  2.9264])\n",
      "params:tensor([ 2.3284, -0.1003])\n",
      "Epoch 12, Loss 28.853565\n",
      "grad:tensor([-0.5190,  2.9222])\n",
      "params:tensor([ 2.3336, -0.1295])\n",
      "Epoch 13, Loss 28.765553\n",
      "grad:tensor([-0.5165,  2.9175])\n",
      "params:tensor([ 2.3388, -0.1587])\n",
      "Epoch 14, Loss 28.677851\n",
      "grad:tensor([-0.5150,  2.9126])\n",
      "params:tensor([ 2.3439, -0.1878])\n",
      "Epoch 15, Loss 28.590431\n",
      "grad:tensor([-0.5138,  2.9077])\n",
      "params:tensor([ 2.3491, -0.2169])\n",
      "Epoch 16, Loss 28.503319\n",
      "grad:tensor([-0.5129,  2.9028])\n",
      "params:tensor([ 2.3542, -0.2459])\n",
      "Epoch 17, Loss 28.416498\n",
      "grad:tensor([-0.5120,  2.8979])\n",
      "params:tensor([ 2.3593, -0.2749])\n",
      "Epoch 18, Loss 28.329973\n",
      "grad:tensor([-0.5111,  2.8930])\n",
      "params:tensor([ 2.3644, -0.3038])\n",
      "Epoch 19, Loss 28.243742\n",
      "grad:tensor([-0.5102,  2.8881])\n",
      "params:tensor([ 2.3695, -0.3327])\n",
      "Epoch 20, Loss 28.157804\n",
      "grad:tensor([-0.5093,  2.8832])\n",
      "params:tensor([ 2.3746, -0.3615])\n",
      "Epoch 21, Loss 28.072151\n",
      "grad:tensor([-0.5084,  2.8783])\n",
      "params:tensor([ 2.3797, -0.3903])\n",
      "Epoch 22, Loss 27.986797\n",
      "grad:tensor([-0.5076,  2.8734])\n",
      "params:tensor([ 2.3848, -0.4190])\n",
      "Epoch 23, Loss 27.901728\n",
      "grad:tensor([-0.5067,  2.8685])\n",
      "params:tensor([ 2.3899, -0.4477])\n",
      "Epoch 24, Loss 27.816950\n",
      "grad:tensor([-0.5059,  2.8636])\n",
      "params:tensor([ 2.3949, -0.4763])\n",
      "Epoch 25, Loss 27.732464\n",
      "grad:tensor([-0.5050,  2.8588])\n",
      "params:tensor([ 2.4000, -0.5049])\n",
      "Epoch 26, Loss 27.648256\n",
      "grad:tensor([-0.5042,  2.8539])\n",
      "params:tensor([ 2.4050, -0.5335])\n",
      "Epoch 27, Loss 27.564344\n",
      "grad:tensor([-0.5033,  2.8490])\n",
      "params:tensor([ 2.4101, -0.5620])\n",
      "Epoch 28, Loss 27.480707\n",
      "grad:tensor([-0.5024,  2.8442])\n",
      "params:tensor([ 2.4151, -0.5904])\n",
      "Epoch 29, Loss 27.397362\n",
      "grad:tensor([-0.5016,  2.8394])\n",
      "params:tensor([ 2.4201, -0.6188])\n",
      "Epoch 30, Loss 27.314295\n",
      "grad:tensor([-0.5007,  2.8346])\n",
      "params:tensor([ 2.4251, -0.6471])\n",
      "Epoch 31, Loss 27.231512\n",
      "grad:tensor([-0.4999,  2.8297])\n",
      "params:tensor([ 2.4301, -0.6754])\n",
      "Epoch 32, Loss 27.149010\n",
      "grad:tensor([-0.4990,  2.8249])\n",
      "params:tensor([ 2.4351, -0.7037])\n",
      "Epoch 33, Loss 27.066790\n",
      "grad:tensor([-0.4982,  2.8201])\n",
      "params:tensor([ 2.4401, -0.7319])\n",
      "Epoch 34, Loss 26.984844\n",
      "grad:tensor([-0.4973,  2.8153])\n",
      "params:tensor([ 2.4450, -0.7600])\n",
      "Epoch 35, Loss 26.903175\n",
      "grad:tensor([-0.4965,  2.8106])\n",
      "params:tensor([ 2.4500, -0.7881])\n",
      "Epoch 36, Loss 26.821791\n",
      "grad:tensor([-0.4957,  2.8058])\n",
      "params:tensor([ 2.4550, -0.8162])\n",
      "Epoch 37, Loss 26.740679\n",
      "grad:tensor([-0.4948,  2.8010])\n",
      "params:tensor([ 2.4599, -0.8442])\n",
      "Epoch 38, Loss 26.659838\n",
      "grad:tensor([-0.4940,  2.7963])\n",
      "params:tensor([ 2.4649, -0.8722])\n",
      "Epoch 39, Loss 26.579279\n",
      "grad:tensor([-0.4931,  2.7915])\n",
      "params:tensor([ 2.4698, -0.9001])\n",
      "Epoch 40, Loss 26.498987\n",
      "grad:tensor([-0.4923,  2.7868])\n",
      "params:tensor([ 2.4747, -0.9280])\n",
      "Epoch 41, Loss 26.418974\n",
      "grad:tensor([-0.4915,  2.7820])\n",
      "params:tensor([ 2.4796, -0.9558])\n",
      "Epoch 42, Loss 26.339228\n",
      "grad:tensor([-0.4906,  2.7773])\n",
      "params:tensor([ 2.4845, -0.9836])\n",
      "Epoch 43, Loss 26.259754\n",
      "grad:tensor([-0.4898,  2.7726])\n",
      "params:tensor([ 2.4894, -1.0113])\n",
      "Epoch 44, Loss 26.180548\n",
      "grad:tensor([-0.4890,  2.7679])\n",
      "params:tensor([ 2.4943, -1.0390])\n",
      "Epoch 45, Loss 26.101616\n",
      "grad:tensor([-0.4881,  2.7632])\n",
      "params:tensor([ 2.4992, -1.0666])\n",
      "Epoch 46, Loss 26.022947\n",
      "grad:tensor([-0.4873,  2.7585])\n",
      "params:tensor([ 2.5041, -1.0942])\n",
      "Epoch 47, Loss 25.944544\n",
      "grad:tensor([-0.4865,  2.7538])\n",
      "params:tensor([ 2.5089, -1.1217])\n",
      "Epoch 48, Loss 25.866417\n",
      "grad:tensor([-0.4856,  2.7491])\n",
      "params:tensor([ 2.5138, -1.1492])\n",
      "Epoch 49, Loss 25.788549\n",
      "grad:tensor([-0.4848,  2.7444])\n",
      "params:tensor([ 2.5186, -1.1766])\n",
      "Epoch 50, Loss 25.710938\n",
      "grad:tensor([-0.4840,  2.7398])\n",
      "params:tensor([ 2.5235, -1.2040])\n",
      "Epoch 51, Loss 25.633600\n",
      "grad:tensor([-0.4832,  2.7351])\n",
      "params:tensor([ 2.5283, -1.2314])\n",
      "Epoch 52, Loss 25.556524\n",
      "grad:tensor([-0.4823,  2.7305])\n",
      "params:tensor([ 2.5331, -1.2587])\n",
      "Epoch 53, Loss 25.479700\n",
      "grad:tensor([-0.4815,  2.7258])\n",
      "params:tensor([ 2.5379, -1.2860])\n",
      "Epoch 54, Loss 25.403149\n",
      "grad:tensor([-0.4807,  2.7212])\n",
      "params:tensor([ 2.5428, -1.3132])\n",
      "Epoch 55, Loss 25.326851\n",
      "grad:tensor([-0.4799,  2.7166])\n",
      "params:tensor([ 2.5476, -1.3403])\n",
      "Epoch 56, Loss 25.250811\n",
      "grad:tensor([-0.4791,  2.7120])\n",
      "params:tensor([ 2.5523, -1.3675])\n",
      "Epoch 57, Loss 25.175035\n",
      "grad:tensor([-0.4783,  2.7074])\n",
      "params:tensor([ 2.5571, -1.3945])\n",
      "Epoch 58, Loss 25.099512\n",
      "grad:tensor([-0.4775,  2.7028])\n",
      "params:tensor([ 2.5619, -1.4216])\n",
      "Epoch 59, Loss 25.024248\n",
      "grad:tensor([-0.4766,  2.6982])\n",
      "params:tensor([ 2.5667, -1.4485])\n",
      "Epoch 60, Loss 24.949236\n",
      "grad:tensor([-0.4758,  2.6936])\n",
      "params:tensor([ 2.5714, -1.4755])\n",
      "Epoch 61, Loss 24.874483\n",
      "grad:tensor([-0.4750,  2.6890])\n",
      "params:tensor([ 2.5762, -1.5024])\n",
      "Epoch 62, Loss 24.799976\n",
      "grad:tensor([-0.4742,  2.6845])\n",
      "params:tensor([ 2.5809, -1.5292])\n",
      "Epoch 63, Loss 24.725737\n",
      "grad:tensor([-0.4734,  2.6799])\n",
      "params:tensor([ 2.5857, -1.5560])\n",
      "Epoch 64, Loss 24.651739\n",
      "grad:tensor([-0.4726,  2.6753])\n",
      "params:tensor([ 2.5904, -1.5828])\n",
      "Epoch 65, Loss 24.577986\n",
      "grad:tensor([-0.4718,  2.6708])\n",
      "params:tensor([ 2.5951, -1.6095])\n",
      "Epoch 66, Loss 24.504494\n",
      "grad:tensor([-0.4710,  2.6663])\n",
      "params:tensor([ 2.5998, -1.6361])\n",
      "Epoch 67, Loss 24.431252\n",
      "grad:tensor([-0.4702,  2.6617])\n",
      "params:tensor([ 2.6045, -1.6628])\n",
      "Epoch 68, Loss 24.358257\n",
      "grad:tensor([-0.4694,  2.6572])\n",
      "params:tensor([ 2.6092, -1.6893])\n",
      "Epoch 69, Loss 24.285505\n",
      "grad:tensor([-0.4686,  2.6527])\n",
      "params:tensor([ 2.6139, -1.7159])\n",
      "Epoch 70, Loss 24.212999\n",
      "grad:tensor([-0.4678,  2.6482])\n",
      "params:tensor([ 2.6186, -1.7423])\n",
      "Epoch 71, Loss 24.140741\n",
      "grad:tensor([-0.4670,  2.6437])\n",
      "params:tensor([ 2.6232, -1.7688])\n",
      "Epoch 72, Loss 24.068733\n",
      "grad:tensor([-0.4662,  2.6392])\n",
      "params:tensor([ 2.6279, -1.7952])\n",
      "Epoch 73, Loss 23.996971\n",
      "grad:tensor([-0.4654,  2.6347])\n",
      "params:tensor([ 2.6326, -1.8215])\n",
      "Epoch 74, Loss 23.925446\n",
      "grad:tensor([-0.4646,  2.6302])\n",
      "params:tensor([ 2.6372, -1.8478])\n",
      "Epoch 75, Loss 23.854168\n",
      "grad:tensor([-0.4638,  2.6258])\n",
      "params:tensor([ 2.6418, -1.8741])\n",
      "Epoch 76, Loss 23.783125\n",
      "grad:tensor([-0.4631,  2.6213])\n",
      "params:tensor([ 2.6465, -1.9003])\n",
      "Epoch 77, Loss 23.712328\n",
      "grad:tensor([-0.4623,  2.6169])\n",
      "params:tensor([ 2.6511, -1.9265])\n",
      "Epoch 78, Loss 23.641773\n",
      "grad:tensor([-0.4615,  2.6124])\n",
      "params:tensor([ 2.6557, -1.9526])\n",
      "Epoch 79, Loss 23.571455\n",
      "grad:tensor([-0.4607,  2.6080])\n",
      "params:tensor([ 2.6603, -1.9787])\n",
      "Epoch 80, Loss 23.501379\n",
      "grad:tensor([-0.4599,  2.6035])\n",
      "params:tensor([ 2.6649, -2.0047])\n",
      "Epoch 81, Loss 23.431538\n",
      "grad:tensor([-0.4591,  2.5991])\n",
      "params:tensor([ 2.6695, -2.0307])\n",
      "Epoch 82, Loss 23.361937\n",
      "grad:tensor([-0.4584,  2.5947])\n",
      "params:tensor([ 2.6741, -2.0566])\n",
      "Epoch 83, Loss 23.292570\n",
      "grad:tensor([-0.4576,  2.5903])\n",
      "params:tensor([ 2.6787, -2.0825])\n",
      "Epoch 84, Loss 23.223436\n",
      "grad:tensor([-0.4568,  2.5859])\n",
      "params:tensor([ 2.6832, -2.1084])\n",
      "Epoch 85, Loss 23.154541\n",
      "grad:tensor([-0.4560,  2.5815])\n",
      "params:tensor([ 2.6878, -2.1342])\n",
      "Epoch 86, Loss 23.085882\n",
      "grad:tensor([-0.4553,  2.5771])\n",
      "params:tensor([ 2.6923, -2.1600])\n",
      "Epoch 87, Loss 23.017447\n",
      "grad:tensor([-0.4545,  2.5727])\n",
      "params:tensor([ 2.6969, -2.1857])\n",
      "Epoch 88, Loss 22.949251\n",
      "grad:tensor([-0.4537,  2.5684])\n",
      "params:tensor([ 2.7014, -2.2114])\n",
      "Epoch 89, Loss 22.881283\n",
      "grad:tensor([-0.4529,  2.5640])\n",
      "params:tensor([ 2.7060, -2.2370])\n",
      "Epoch 90, Loss 22.813549\n",
      "grad:tensor([-0.4522,  2.5597])\n",
      "params:tensor([ 2.7105, -2.2626])\n",
      "Epoch 91, Loss 22.746044\n",
      "grad:tensor([-0.4514,  2.5553])\n",
      "params:tensor([ 2.7150, -2.2882])\n",
      "Epoch 92, Loss 22.678766\n",
      "grad:tensor([-0.4506,  2.5510])\n",
      "params:tensor([ 2.7195, -2.3137])\n",
      "Epoch 93, Loss 22.611717\n",
      "grad:tensor([-0.4499,  2.5466])\n",
      "params:tensor([ 2.7240, -2.3392])\n",
      "Epoch 94, Loss 22.544899\n",
      "grad:tensor([-0.4491,  2.5423])\n",
      "params:tensor([ 2.7285, -2.3646])\n",
      "Epoch 95, Loss 22.478306\n",
      "grad:tensor([-0.4483,  2.5380])\n",
      "params:tensor([ 2.7330, -2.3900])\n",
      "Epoch 96, Loss 22.411934\n",
      "grad:tensor([-0.4476,  2.5337])\n",
      "params:tensor([ 2.7374, -2.4153])\n",
      "Epoch 97, Loss 22.345793\n",
      "grad:tensor([-0.4468,  2.5294])\n",
      "params:tensor([ 2.7419, -2.4406])\n",
      "Epoch 98, Loss 22.279875\n",
      "grad:tensor([-0.4461,  2.5251])\n",
      "params:tensor([ 2.7464, -2.4658])\n",
      "Epoch 99, Loss 22.214186\n",
      "grad:tensor([-0.4453,  2.5208])\n",
      "params:tensor([ 2.7508, -2.4910])\n",
      "Epoch 100, Loss 22.148710\n",
      "grad:tensor([-0.4446,  2.5165])\n",
      "params:tensor([ 2.7553, -2.5162])\n",
      "Epoch 101, Loss 22.083464\n",
      "grad:tensor([-0.4438,  2.5122])\n",
      "params:tensor([ 2.7597, -2.5413])\n",
      "Epoch 102, Loss 22.018436\n",
      "grad:tensor([-0.4430,  2.5080])\n",
      "params:tensor([ 2.7641, -2.5664])\n",
      "Epoch 103, Loss 21.953632\n",
      "grad:tensor([-0.4423,  2.5037])\n",
      "params:tensor([ 2.7686, -2.5914])\n",
      "Epoch 104, Loss 21.889046\n",
      "grad:tensor([-0.4415,  2.4994])\n",
      "params:tensor([ 2.7730, -2.6164])\n",
      "Epoch 105, Loss 21.824677\n",
      "grad:tensor([-0.4408,  2.4952])\n",
      "params:tensor([ 2.7774, -2.6414])\n",
      "Epoch 106, Loss 21.760529\n",
      "grad:tensor([-0.4400,  2.4910])\n",
      "params:tensor([ 2.7818, -2.6663])\n",
      "Epoch 107, Loss 21.696600\n",
      "grad:tensor([-0.4393,  2.4867])\n",
      "params:tensor([ 2.7862, -2.6912])\n",
      "Epoch 108, Loss 21.632883\n",
      "grad:tensor([-0.4385,  2.4825])\n",
      "params:tensor([ 2.7906, -2.7160])\n",
      "Epoch 109, Loss 21.569389\n",
      "grad:tensor([-0.4378,  2.4783])\n",
      "params:tensor([ 2.7949, -2.7408])\n",
      "Epoch 110, Loss 21.506102\n",
      "grad:tensor([-0.4370,  2.4741])\n",
      "params:tensor([ 2.7993, -2.7655])\n",
      "Epoch 111, Loss 21.443037\n",
      "grad:tensor([-0.4363,  2.4699])\n",
      "params:tensor([ 2.8037, -2.7902])\n",
      "Epoch 112, Loss 21.380186\n",
      "grad:tensor([-0.4356,  2.4657])\n",
      "params:tensor([ 2.8080, -2.8149])\n",
      "Epoch 113, Loss 21.317549\n",
      "grad:tensor([-0.4348,  2.4615])\n",
      "params:tensor([ 2.8124, -2.8395])\n",
      "Epoch 114, Loss 21.255117\n",
      "grad:tensor([-0.4341,  2.4573])\n",
      "params:tensor([ 2.8167, -2.8641])\n",
      "Epoch 115, Loss 21.192907\n",
      "grad:tensor([-0.4334,  2.4531])\n",
      "params:tensor([ 2.8211, -2.8886])\n",
      "Epoch 116, Loss 21.130898\n",
      "grad:tensor([-0.4326,  2.4490])\n",
      "params:tensor([ 2.8254, -2.9131])\n",
      "Epoch 117, Loss 21.069105\n",
      "grad:tensor([-0.4319,  2.4448])\n",
      "params:tensor([ 2.8297, -2.9375])\n",
      "Epoch 118, Loss 21.007526\n",
      "grad:tensor([-0.4311,  2.4407])\n",
      "params:tensor([ 2.8340, -2.9619])\n",
      "Epoch 119, Loss 20.946150\n",
      "grad:tensor([-0.4304,  2.4365])\n",
      "params:tensor([ 2.8383, -2.9863])\n",
      "Epoch 120, Loss 20.884981\n",
      "grad:tensor([-0.4297,  2.4324])\n",
      "params:tensor([ 2.8426, -3.0106])\n",
      "Epoch 121, Loss 20.824024\n",
      "grad:tensor([-0.4290,  2.4282])\n",
      "params:tensor([ 2.8469, -3.0349])\n",
      "Epoch 122, Loss 20.763273\n",
      "grad:tensor([-0.4282,  2.4241])\n",
      "params:tensor([ 2.8512, -3.0592])\n",
      "Epoch 123, Loss 20.702728\n",
      "grad:tensor([-0.4275,  2.4200])\n",
      "params:tensor([ 2.8555, -3.0834])\n",
      "Epoch 124, Loss 20.642384\n",
      "grad:tensor([-0.4268,  2.4159])\n",
      "params:tensor([ 2.8597, -3.1075])\n",
      "Epoch 125, Loss 20.582249\n",
      "grad:tensor([-0.4260,  2.4118])\n",
      "params:tensor([ 2.8640, -3.1316])\n",
      "Epoch 126, Loss 20.522322\n",
      "grad:tensor([-0.4253,  2.4077])\n",
      "params:tensor([ 2.8682, -3.1557])\n",
      "Epoch 127, Loss 20.462593\n",
      "grad:tensor([-0.4246,  2.4036])\n",
      "params:tensor([ 2.8725, -3.1797])\n",
      "Epoch 128, Loss 20.403069\n",
      "grad:tensor([-0.4239,  2.3995])\n",
      "params:tensor([ 2.8767, -3.2037])\n",
      "Epoch 129, Loss 20.343742\n",
      "grad:tensor([-0.4232,  2.3954])\n",
      "params:tensor([ 2.8810, -3.2277])\n",
      "Epoch 130, Loss 20.284624\n",
      "grad:tensor([-0.4224,  2.3914])\n",
      "params:tensor([ 2.8852, -3.2516])\n",
      "Epoch 131, Loss 20.225702\n",
      "grad:tensor([-0.4217,  2.3873])\n",
      "params:tensor([ 2.8894, -3.2755])\n",
      "Epoch 132, Loss 20.166981\n",
      "grad:tensor([-0.4210,  2.3832])\n",
      "params:tensor([ 2.8936, -3.2993])\n",
      "Epoch 133, Loss 20.108461\n",
      "grad:tensor([-0.4203,  2.3792])\n",
      "params:tensor([ 2.8978, -3.3231])\n",
      "Epoch 134, Loss 20.050137\n",
      "grad:tensor([-0.4196,  2.3752])\n",
      "params:tensor([ 2.9020, -3.3469])\n",
      "Epoch 135, Loss 19.992016\n",
      "grad:tensor([-0.4189,  2.3711])\n",
      "params:tensor([ 2.9062, -3.3706])\n",
      "Epoch 136, Loss 19.934086\n",
      "grad:tensor([-0.4182,  2.3671])\n",
      "params:tensor([ 2.9104, -3.3942])\n",
      "Epoch 137, Loss 19.876352\n",
      "grad:tensor([-0.4174,  2.3631])\n",
      "params:tensor([ 2.9146, -3.4179])\n",
      "Epoch 138, Loss 19.818823\n",
      "grad:tensor([-0.4167,  2.3591])\n",
      "params:tensor([ 2.9187, -3.4415])\n",
      "Epoch 139, Loss 19.761480\n",
      "grad:tensor([-0.4160,  2.3550])\n",
      "params:tensor([ 2.9229, -3.4650])\n",
      "Epoch 140, Loss 19.704336\n",
      "grad:tensor([-0.4153,  2.3510])\n",
      "params:tensor([ 2.9270, -3.4885])\n",
      "Epoch 141, Loss 19.647385\n",
      "grad:tensor([-0.4146,  2.3471])\n",
      "params:tensor([ 2.9312, -3.5120])\n",
      "Epoch 142, Loss 19.590626\n",
      "grad:tensor([-0.4139,  2.3431])\n",
      "params:tensor([ 2.9353, -3.5354])\n",
      "Epoch 143, Loss 19.534061\n",
      "grad:tensor([-0.4132,  2.3391])\n",
      "params:tensor([ 2.9395, -3.5588])\n",
      "Epoch 144, Loss 19.477690\n",
      "grad:tensor([-0.4125,  2.3351])\n",
      "params:tensor([ 2.9436, -3.5822])\n",
      "Epoch 145, Loss 19.421507\n",
      "grad:tensor([-0.4118,  2.3311])\n",
      "params:tensor([ 2.9477, -3.6055])\n",
      "Epoch 146, Loss 19.365515\n",
      "grad:tensor([-0.4111,  2.3272])\n",
      "params:tensor([ 2.9518, -3.6287])\n",
      "Epoch 147, Loss 19.309715\n",
      "grad:tensor([-0.4104,  2.3232])\n",
      "params:tensor([ 2.9559, -3.6520])\n",
      "Epoch 148, Loss 19.254107\n",
      "grad:tensor([-0.4097,  2.3193])\n",
      "params:tensor([ 2.9600, -3.6752])\n",
      "Epoch 149, Loss 19.198685\n",
      "grad:tensor([-0.4090,  2.3153])\n",
      "params:tensor([ 2.9641, -3.6983])\n",
      "Epoch 150, Loss 19.143446\n",
      "grad:tensor([-0.4083,  2.3114])\n",
      "params:tensor([ 2.9682, -3.7214])\n",
      "Epoch 151, Loss 19.088402\n",
      "grad:tensor([-0.4076,  2.3075])\n",
      "params:tensor([ 2.9723, -3.7445])\n",
      "Epoch 152, Loss 19.033543\n",
      "grad:tensor([-0.4069,  2.3036])\n",
      "params:tensor([ 2.9763, -3.7675])\n",
      "Epoch 153, Loss 18.978868\n",
      "grad:tensor([-0.4062,  2.2997])\n",
      "params:tensor([ 2.9804, -3.7905])\n",
      "Epoch 154, Loss 18.924377\n",
      "grad:tensor([-0.4056,  2.2957])\n",
      "params:tensor([ 2.9844, -3.8135])\n",
      "Epoch 155, Loss 18.870081\n",
      "grad:tensor([-0.4049,  2.2918])\n",
      "params:tensor([ 2.9885, -3.8364])\n",
      "Epoch 156, Loss 18.815960\n",
      "grad:tensor([-0.4042,  2.2880])\n",
      "params:tensor([ 2.9925, -3.8593])\n",
      "Epoch 157, Loss 18.762022\n",
      "grad:tensor([-0.4035,  2.2841])\n",
      "params:tensor([ 2.9966, -3.8821])\n",
      "Epoch 158, Loss 18.708271\n",
      "grad:tensor([-0.4028,  2.2802])\n",
      "params:tensor([ 3.0006, -3.9049])\n",
      "Epoch 159, Loss 18.654699\n",
      "grad:tensor([-0.4021,  2.2763])\n",
      "params:tensor([ 3.0046, -3.9277])\n",
      "Epoch 160, Loss 18.601313\n",
      "grad:tensor([-0.4014,  2.2724])\n",
      "params:tensor([ 3.0086, -3.9504])\n",
      "Epoch 161, Loss 18.548109\n",
      "grad:tensor([-0.4007,  2.2686])\n",
      "params:tensor([ 3.0126, -3.9731])\n",
      "Epoch 162, Loss 18.495085\n",
      "grad:tensor([-0.4001,  2.2647])\n",
      "params:tensor([ 3.0166, -3.9958])\n",
      "Epoch 163, Loss 18.442236\n",
      "grad:tensor([-0.3994,  2.2609])\n",
      "params:tensor([ 3.0206, -4.0184])\n",
      "Epoch 164, Loss 18.389570\n",
      "grad:tensor([-0.3987,  2.2570])\n",
      "params:tensor([ 3.0246, -4.0409])\n",
      "Epoch 165, Loss 18.337080\n",
      "grad:tensor([-0.3980,  2.2532])\n",
      "params:tensor([ 3.0286, -4.0635])\n",
      "Epoch 166, Loss 18.284777\n",
      "grad:tensor([-0.3974,  2.2494])\n",
      "params:tensor([ 3.0326, -4.0860])\n",
      "Epoch 167, Loss 18.232641\n",
      "grad:tensor([-0.3967,  2.2456])\n",
      "params:tensor([ 3.0365, -4.1084])\n",
      "Epoch 168, Loss 18.180685\n",
      "grad:tensor([-0.3960,  2.2417])\n",
      "params:tensor([ 3.0405, -4.1308])\n",
      "Epoch 169, Loss 18.128906\n",
      "grad:tensor([-0.3953,  2.2379])\n",
      "params:tensor([ 3.0445, -4.1532])\n",
      "Epoch 170, Loss 18.077301\n",
      "grad:tensor([-0.3947,  2.2341])\n",
      "params:tensor([ 3.0484, -4.1756])\n",
      "Epoch 171, Loss 18.025877\n",
      "grad:tensor([-0.3940,  2.2303])\n",
      "params:tensor([ 3.0523, -4.1979])\n",
      "Epoch 172, Loss 17.974623\n",
      "grad:tensor([-0.3933,  2.2266])\n",
      "params:tensor([ 3.0563, -4.2201])\n",
      "Epoch 173, Loss 17.923546\n",
      "grad:tensor([-0.3927,  2.2228])\n",
      "params:tensor([ 3.0602, -4.2424])\n",
      "Epoch 174, Loss 17.872643\n",
      "grad:tensor([-0.3920,  2.2190])\n",
      "params:tensor([ 3.0641, -4.2646])\n",
      "Epoch 175, Loss 17.821909\n",
      "grad:tensor([-0.3913,  2.2152])\n",
      "params:tensor([ 3.0680, -4.2867])\n",
      "Epoch 176, Loss 17.771345\n",
      "grad:tensor([-0.3907,  2.2115])\n",
      "params:tensor([ 3.0719, -4.3088])\n",
      "Epoch 177, Loss 17.720955\n",
      "grad:tensor([-0.3900,  2.2077])\n",
      "params:tensor([ 3.0758, -4.3309])\n",
      "Epoch 178, Loss 17.670738\n",
      "grad:tensor([-0.3893,  2.2040])\n",
      "params:tensor([ 3.0797, -4.3529])\n",
      "Epoch 179, Loss 17.620689\n",
      "grad:tensor([-0.3887,  2.2002])\n",
      "params:tensor([ 3.0836, -4.3749])\n",
      "Epoch 180, Loss 17.570814\n",
      "grad:tensor([-0.3880,  2.1965])\n",
      "params:tensor([ 3.0875, -4.3969])\n",
      "Epoch 181, Loss 17.521103\n",
      "grad:tensor([-0.3873,  2.1927])\n",
      "params:tensor([ 3.0914, -4.4188])\n",
      "Epoch 182, Loss 17.471565\n",
      "grad:tensor([-0.3867,  2.1890])\n",
      "params:tensor([ 3.0952, -4.4407])\n",
      "Epoch 183, Loss 17.422192\n",
      "grad:tensor([-0.3860,  2.1853])\n",
      "params:tensor([ 3.0991, -4.4626])\n",
      "Epoch 184, Loss 17.372993\n",
      "grad:tensor([-0.3854,  2.1816])\n",
      "params:tensor([ 3.1030, -4.4844])\n",
      "Epoch 185, Loss 17.323954\n",
      "grad:tensor([-0.3847,  2.1779])\n",
      "params:tensor([ 3.1068, -4.5062])\n",
      "Epoch 186, Loss 17.275084\n",
      "grad:tensor([-0.3841,  2.1742])\n",
      "params:tensor([ 3.1106, -4.5279])\n",
      "Epoch 187, Loss 17.226379\n",
      "grad:tensor([-0.3834,  2.1705])\n",
      "params:tensor([ 3.1145, -4.5496])\n",
      "Epoch 188, Loss 17.177839\n",
      "grad:tensor([-0.3828,  2.1668])\n",
      "params:tensor([ 3.1183, -4.5713])\n",
      "Epoch 189, Loss 17.129463\n",
      "grad:tensor([-0.3821,  2.1631])\n",
      "params:tensor([ 3.1221, -4.5929])\n",
      "Epoch 190, Loss 17.081255\n",
      "grad:tensor([-0.3815,  2.1594])\n",
      "params:tensor([ 3.1259, -4.6145])\n",
      "Epoch 191, Loss 17.033209\n",
      "grad:tensor([-0.3808,  2.1558])\n",
      "params:tensor([ 3.1298, -4.6361])\n",
      "Epoch 192, Loss 16.985327\n",
      "grad:tensor([-0.3802,  2.1521])\n",
      "params:tensor([ 3.1336, -4.6576])\n",
      "Epoch 193, Loss 16.937605\n",
      "grad:tensor([-0.3795,  2.1485])\n",
      "params:tensor([ 3.1374, -4.6791])\n",
      "Epoch 194, Loss 16.890047\n",
      "grad:tensor([-0.3789,  2.1448])\n",
      "params:tensor([ 3.1411, -4.7005])\n",
      "Epoch 195, Loss 16.842649\n",
      "grad:tensor([-0.3782,  2.1412])\n",
      "params:tensor([ 3.1449, -4.7219])\n",
      "Epoch 196, Loss 16.795412\n",
      "grad:tensor([-0.3776,  2.1375])\n",
      "params:tensor([ 3.1487, -4.7433])\n",
      "Epoch 197, Loss 16.748339\n",
      "grad:tensor([-0.3770,  2.1339])\n",
      "params:tensor([ 3.1525, -4.7646])\n",
      "Epoch 198, Loss 16.701422\n",
      "grad:tensor([-0.3763,  2.1303])\n",
      "params:tensor([ 3.1562, -4.7859])\n",
      "Epoch 199, Loss 16.654661\n",
      "grad:tensor([-0.3757,  2.1267])\n",
      "params:tensor([ 3.1600, -4.8072])\n",
      "Epoch 200, Loss 16.608067\n",
      "grad:tensor([-0.3750,  2.1230])\n",
      "params:tensor([ 3.1637, -4.8284])\n",
      "Epoch 201, Loss 16.561623\n",
      "grad:tensor([-0.3744,  2.1194])\n",
      "params:tensor([ 3.1675, -4.8496])\n",
      "Epoch 202, Loss 16.515343\n",
      "grad:tensor([-0.3738,  2.1158])\n",
      "params:tensor([ 3.1712, -4.8708])\n",
      "Epoch 203, Loss 16.469219\n",
      "grad:tensor([-0.3731,  2.1122])\n",
      "params:tensor([ 3.1750, -4.8919])\n",
      "Epoch 204, Loss 16.423248\n",
      "grad:tensor([-0.3725,  2.1087])\n",
      "params:tensor([ 3.1787, -4.9130])\n",
      "Epoch 205, Loss 16.377434\n",
      "grad:tensor([-0.3719,  2.1051])\n",
      "params:tensor([ 3.1824, -4.9341])\n",
      "Epoch 206, Loss 16.331776\n",
      "grad:tensor([-0.3712,  2.1015])\n",
      "params:tensor([ 3.1861, -4.9551])\n",
      "Epoch 207, Loss 16.286276\n",
      "grad:tensor([-0.3706,  2.0979])\n",
      "params:tensor([ 3.1898, -4.9760])\n",
      "Epoch 208, Loss 16.240929\n",
      "grad:tensor([-0.3700,  2.0944])\n",
      "params:tensor([ 3.1935, -4.9970])\n",
      "Epoch 209, Loss 16.195732\n",
      "grad:tensor([-0.3694,  2.0908])\n",
      "params:tensor([ 3.1972, -5.0179])\n",
      "Epoch 210, Loss 16.150694\n",
      "grad:tensor([-0.3687,  2.0873])\n",
      "params:tensor([ 3.2009, -5.0388])\n",
      "Epoch 211, Loss 16.105806\n",
      "grad:tensor([-0.3681,  2.0837])\n",
      "params:tensor([ 3.2046, -5.0596])\n",
      "Epoch 212, Loss 16.061073\n",
      "grad:tensor([-0.3675,  2.0802])\n",
      "params:tensor([ 3.2082, -5.0804])\n",
      "Epoch 213, Loss 16.016487\n",
      "grad:tensor([-0.3668,  2.0766])\n",
      "params:tensor([ 3.2119, -5.1012])\n",
      "Epoch 214, Loss 15.972058\n",
      "grad:tensor([-0.3662,  2.0731])\n",
      "params:tensor([ 3.2156, -5.1219])\n",
      "Epoch 215, Loss 15.927776\n",
      "grad:tensor([-0.3656,  2.0696])\n",
      "params:tensor([ 3.2192, -5.1426])\n",
      "Epoch 216, Loss 15.883645\n",
      "grad:tensor([-0.3650,  2.0661])\n",
      "params:tensor([ 3.2229, -5.1633])\n",
      "Epoch 217, Loss 15.839664\n",
      "grad:tensor([-0.3644,  2.0626])\n",
      "params:tensor([ 3.2265, -5.1839])\n",
      "Epoch 218, Loss 15.795832\n",
      "grad:tensor([-0.3637,  2.0591])\n",
      "params:tensor([ 3.2302, -5.2045])\n",
      "Epoch 219, Loss 15.752152\n",
      "grad:tensor([-0.3631,  2.0556])\n",
      "params:tensor([ 3.2338, -5.2250])\n",
      "Epoch 220, Loss 15.708612\n",
      "grad:tensor([-0.3625,  2.0521])\n",
      "params:tensor([ 3.2374, -5.2456])\n",
      "Epoch 221, Loss 15.665226\n",
      "grad:tensor([-0.3619,  2.0486])\n",
      "params:tensor([ 3.2410, -5.2660])\n",
      "Epoch 222, Loss 15.621990\n",
      "grad:tensor([-0.3613,  2.0451])\n",
      "params:tensor([ 3.2447, -5.2865])\n",
      "Epoch 223, Loss 15.578897\n",
      "grad:tensor([-0.3607,  2.0416])\n",
      "params:tensor([ 3.2483, -5.3069])\n",
      "Epoch 224, Loss 15.535950\n",
      "grad:tensor([-0.3601,  2.0382])\n",
      "params:tensor([ 3.2519, -5.3273])\n",
      "Epoch 225, Loss 15.493150\n",
      "grad:tensor([-0.3594,  2.0347])\n",
      "params:tensor([ 3.2555, -5.3476])\n",
      "Epoch 226, Loss 15.450495\n",
      "grad:tensor([-0.3588,  2.0312])\n",
      "params:tensor([ 3.2590, -5.3680])\n",
      "Epoch 227, Loss 15.407981\n",
      "grad:tensor([-0.3582,  2.0278])\n",
      "params:tensor([ 3.2626, -5.3882])\n",
      "Epoch 228, Loss 15.365616\n",
      "grad:tensor([-0.3576,  2.0243])\n",
      "params:tensor([ 3.2662, -5.4085])\n",
      "Epoch 229, Loss 15.323396\n",
      "grad:tensor([-0.3570,  2.0209])\n",
      "params:tensor([ 3.2698, -5.4287])\n",
      "Epoch 230, Loss 15.281317\n",
      "grad:tensor([-0.3564,  2.0175])\n",
      "params:tensor([ 3.2733, -5.4489])\n",
      "Epoch 231, Loss 15.239380\n",
      "grad:tensor([-0.3558,  2.0140])\n",
      "params:tensor([ 3.2769, -5.4690])\n",
      "Epoch 232, Loss 15.197585\n",
      "grad:tensor([-0.3552,  2.0106])\n",
      "params:tensor([ 3.2804, -5.4891])\n",
      "Epoch 233, Loss 15.155932\n",
      "grad:tensor([-0.3546,  2.0072])\n",
      "params:tensor([ 3.2840, -5.5092])\n",
      "Epoch 234, Loss 15.114425\n",
      "grad:tensor([-0.3540,  2.0038])\n",
      "params:tensor([ 3.2875, -5.5292])\n",
      "Epoch 235, Loss 15.073055\n",
      "grad:tensor([-0.3534,  2.0004])\n",
      "params:tensor([ 3.2911, -5.5492])\n",
      "Epoch 236, Loss 15.031823\n",
      "grad:tensor([-0.3528,  1.9970])\n",
      "params:tensor([ 3.2946, -5.5692])\n",
      "Epoch 237, Loss 14.990734\n",
      "grad:tensor([-0.3522,  1.9936])\n",
      "params:tensor([ 3.2981, -5.5891])\n",
      "Epoch 238, Loss 14.949784\n",
      "grad:tensor([-0.3516,  1.9902])\n",
      "params:tensor([ 3.3016, -5.6090])\n",
      "Epoch 239, Loss 14.908973\n",
      "grad:tensor([-0.3510,  1.9868])\n",
      "params:tensor([ 3.3051, -5.6289])\n",
      "Epoch 240, Loss 14.868304\n",
      "grad:tensor([-0.3504,  1.9835])\n",
      "params:tensor([ 3.3086, -5.6487])\n",
      "Epoch 241, Loss 14.827767\n",
      "grad:tensor([-0.3498,  1.9801])\n",
      "params:tensor([ 3.3121, -5.6685])\n",
      "Epoch 242, Loss 14.787370\n",
      "grad:tensor([-0.3492,  1.9767])\n",
      "params:tensor([ 3.3156, -5.6883])\n",
      "Epoch 243, Loss 14.747109\n",
      "grad:tensor([-0.3486,  1.9734])\n",
      "params:tensor([ 3.3191, -5.7080])\n",
      "Epoch 244, Loss 14.706989\n",
      "grad:tensor([-0.3480,  1.9700])\n",
      "params:tensor([ 3.3226, -5.7277])\n",
      "Epoch 245, Loss 14.667002\n",
      "grad:tensor([-0.3474,  1.9667])\n",
      "params:tensor([ 3.3261, -5.7474])\n",
      "Epoch 246, Loss 14.627151\n",
      "grad:tensor([-0.3468,  1.9633])\n",
      "params:tensor([ 3.3295, -5.7670])\n",
      "Epoch 247, Loss 14.587436\n",
      "grad:tensor([-0.3462,  1.9600])\n",
      "params:tensor([ 3.3330, -5.7866])\n",
      "Epoch 248, Loss 14.547855\n",
      "grad:tensor([-0.3456,  1.9567])\n",
      "params:tensor([ 3.3365, -5.8062])\n",
      "Epoch 249, Loss 14.508409\n",
      "grad:tensor([-0.3451,  1.9533])\n",
      "params:tensor([ 3.3399, -5.8257])\n",
      "Epoch 250, Loss 14.469097\n",
      "grad:tensor([-0.3445,  1.9500])\n",
      "params:tensor([ 3.3434, -5.8452])\n",
      "Epoch 251, Loss 14.429920\n",
      "grad:tensor([-0.3439,  1.9467])\n",
      "params:tensor([ 3.3468, -5.8647])\n",
      "Epoch 252, Loss 14.390870\n",
      "grad:tensor([-0.3433,  1.9434])\n",
      "params:tensor([ 3.3502, -5.8841])\n",
      "Epoch 253, Loss 14.351956\n",
      "grad:tensor([-0.3427,  1.9401])\n",
      "params:tensor([ 3.3537, -5.9035])\n",
      "Epoch 254, Loss 14.313177\n",
      "grad:tensor([-0.3421,  1.9368])\n",
      "params:tensor([ 3.3571, -5.9229])\n",
      "Epoch 255, Loss 14.274529\n",
      "grad:tensor([-0.3416,  1.9335])\n",
      "params:tensor([ 3.3605, -5.9422])\n",
      "Epoch 256, Loss 14.236009\n",
      "grad:tensor([-0.3410,  1.9302])\n",
      "params:tensor([ 3.3639, -5.9615])\n",
      "Epoch 257, Loss 14.197620\n",
      "grad:tensor([-0.3404,  1.9269])\n",
      "params:tensor([ 3.3673, -5.9808])\n",
      "Epoch 258, Loss 14.159363\n",
      "grad:tensor([-0.3398,  1.9237])\n",
      "params:tensor([ 3.3707, -6.0000])\n",
      "Epoch 259, Loss 14.121234\n",
      "grad:tensor([-0.3392,  1.9204])\n",
      "params:tensor([ 3.3741, -6.0192])\n",
      "Epoch 260, Loss 14.083236\n",
      "grad:tensor([-0.3387,  1.9171])\n",
      "params:tensor([ 3.3775, -6.0384])\n",
      "Epoch 261, Loss 14.045367\n",
      "grad:tensor([-0.3381,  1.9139])\n",
      "params:tensor([ 3.3809, -6.0576])\n",
      "Epoch 262, Loss 14.007627\n",
      "grad:tensor([-0.3375,  1.9106])\n",
      "params:tensor([ 3.3842, -6.0767])\n",
      "Epoch 263, Loss 13.970016\n",
      "grad:tensor([-0.3369,  1.9074])\n",
      "params:tensor([ 3.3876, -6.0957])\n",
      "Epoch 264, Loss 13.932531\n",
      "grad:tensor([-0.3364,  1.9041])\n",
      "params:tensor([ 3.3910, -6.1148])\n",
      "Epoch 265, Loss 13.895172\n",
      "grad:tensor([-0.3358,  1.9009])\n",
      "params:tensor([ 3.3943, -6.1338])\n",
      "Epoch 266, Loss 13.857944\n",
      "grad:tensor([-0.3352,  1.8977])\n",
      "params:tensor([ 3.3977, -6.1528])\n",
      "Epoch 267, Loss 13.820837\n",
      "grad:tensor([-0.3347,  1.8945])\n",
      "params:tensor([ 3.4010, -6.1717])\n",
      "Epoch 268, Loss 13.783858\n",
      "grad:tensor([-0.3341,  1.8912])\n",
      "params:tensor([ 3.4044, -6.1906])\n",
      "Epoch 269, Loss 13.747006\n",
      "grad:tensor([-0.3335,  1.8880])\n",
      "params:tensor([ 3.4077, -6.2095])\n",
      "Epoch 270, Loss 13.710278\n",
      "grad:tensor([-0.3330,  1.8848])\n",
      "params:tensor([ 3.4110, -6.2284])\n",
      "Epoch 271, Loss 13.673676\n",
      "grad:tensor([-0.3324,  1.8816])\n",
      "params:tensor([ 3.4144, -6.2472])\n",
      "Epoch 272, Loss 13.637196\n",
      "grad:tensor([-0.3318,  1.8784])\n",
      "params:tensor([ 3.4177, -6.2660])\n",
      "Epoch 273, Loss 13.600842\n",
      "grad:tensor([-0.3313,  1.8752])\n",
      "params:tensor([ 3.4210, -6.2847])\n",
      "Epoch 274, Loss 13.564609\n",
      "grad:tensor([-0.3307,  1.8720])\n",
      "params:tensor([ 3.4243, -6.3034])\n",
      "Epoch 275, Loss 13.528501\n",
      "grad:tensor([-0.3301,  1.8689])\n",
      "params:tensor([ 3.4276, -6.3221])\n",
      "Epoch 276, Loss 13.492514\n",
      "grad:tensor([-0.3296,  1.8657])\n",
      "params:tensor([ 3.4309, -6.3408])\n",
      "Epoch 277, Loss 13.456651\n",
      "grad:tensor([-0.3290,  1.8625])\n",
      "params:tensor([ 3.4342, -6.3594])\n",
      "Epoch 278, Loss 13.420910\n",
      "grad:tensor([-0.3285,  1.8594])\n",
      "params:tensor([ 3.4375, -6.3780])\n",
      "Epoch 279, Loss 13.385287\n",
      "grad:tensor([-0.3279,  1.8562])\n",
      "params:tensor([ 3.4407, -6.3966])\n",
      "Epoch 280, Loss 13.349789\n",
      "grad:tensor([-0.3274,  1.8530])\n",
      "params:tensor([ 3.4440, -6.4151])\n",
      "Epoch 281, Loss 13.314407\n",
      "grad:tensor([-0.3268,  1.8499])\n",
      "params:tensor([ 3.4473, -6.4336])\n",
      "Epoch 282, Loss 13.279150\n",
      "grad:tensor([-0.3262,  1.8468])\n",
      "params:tensor([ 3.4506, -6.4520])\n",
      "Epoch 283, Loss 13.244009\n",
      "grad:tensor([-0.3257,  1.8436])\n",
      "params:tensor([ 3.4538, -6.4705])\n",
      "Epoch 284, Loss 13.208991\n",
      "grad:tensor([-0.3251,  1.8405])\n",
      "params:tensor([ 3.4571, -6.4889])\n",
      "Epoch 285, Loss 13.174088\n",
      "grad:tensor([-0.3246,  1.8374])\n",
      "params:tensor([ 3.4603, -6.5073])\n",
      "Epoch 286, Loss 13.139307\n",
      "grad:tensor([-0.3240,  1.8342])\n",
      "params:tensor([ 3.4635, -6.5256])\n",
      "Epoch 287, Loss 13.104639\n",
      "grad:tensor([-0.3235,  1.8311])\n",
      "params:tensor([ 3.4668, -6.5439])\n",
      "Epoch 288, Loss 13.070092\n",
      "grad:tensor([-0.3229,  1.8280])\n",
      "params:tensor([ 3.4700, -6.5622])\n",
      "Epoch 289, Loss 13.035664\n",
      "grad:tensor([-0.3224,  1.8249])\n",
      "params:tensor([ 3.4732, -6.5804])\n",
      "Epoch 290, Loss 13.001349\n",
      "grad:tensor([-0.3218,  1.8218])\n",
      "params:tensor([ 3.4765, -6.5987])\n",
      "Epoch 291, Loss 12.967152\n",
      "grad:tensor([-0.3213,  1.8187])\n",
      "params:tensor([ 3.4797, -6.6169])\n",
      "Epoch 292, Loss 12.933075\n",
      "grad:tensor([-0.3207,  1.8156])\n",
      "params:tensor([ 3.4829, -6.6350])\n",
      "Epoch 293, Loss 12.899109\n",
      "grad:tensor([-0.3202,  1.8125])\n",
      "params:tensor([ 3.4861, -6.6531])\n",
      "Epoch 294, Loss 12.865259\n",
      "grad:tensor([-0.3196,  1.8095])\n",
      "params:tensor([ 3.4893, -6.6712])\n",
      "Epoch 295, Loss 12.831525\n",
      "grad:tensor([-0.3191,  1.8064])\n",
      "params:tensor([ 3.4925, -6.6893])\n",
      "Epoch 296, Loss 12.797904\n",
      "grad:tensor([-0.3186,  1.8033])\n",
      "params:tensor([ 3.4956, -6.7073])\n",
      "Epoch 297, Loss 12.764399\n",
      "grad:tensor([-0.3180,  1.8003])\n",
      "params:tensor([ 3.4988, -6.7253])\n",
      "Epoch 298, Loss 12.731007\n",
      "grad:tensor([-0.3175,  1.7972])\n",
      "params:tensor([ 3.5020, -6.7433])\n",
      "Epoch 299, Loss 12.697727\n",
      "grad:tensor([-0.3169,  1.7941])\n",
      "params:tensor([ 3.5052, -6.7612])\n",
      "Epoch 300, Loss 12.664559\n",
      "grad:tensor([-0.3164,  1.7911])\n",
      "params:tensor([ 3.5083, -6.7792])\n",
      "Epoch 301, Loss 12.631507\n",
      "grad:tensor([-0.3159,  1.7881])\n",
      "params:tensor([ 3.5115, -6.7970])\n",
      "Epoch 302, Loss 12.598568\n",
      "grad:tensor([-0.3153,  1.7850])\n",
      "params:tensor([ 3.5146, -6.8149])\n",
      "Epoch 303, Loss 12.565738\n",
      "grad:tensor([-0.3148,  1.7820])\n",
      "params:tensor([ 3.5178, -6.8327])\n",
      "Epoch 304, Loss 12.533021\n",
      "grad:tensor([-0.3143,  1.7790])\n",
      "params:tensor([ 3.5209, -6.8505])\n",
      "Epoch 305, Loss 12.500413\n",
      "grad:tensor([-0.3137,  1.7759])\n",
      "params:tensor([ 3.5241, -6.8683])\n",
      "Epoch 306, Loss 12.467919\n",
      "grad:tensor([-0.3132,  1.7729])\n",
      "params:tensor([ 3.5272, -6.8860])\n",
      "Epoch 307, Loss 12.435532\n",
      "grad:tensor([-0.3127,  1.7699])\n",
      "params:tensor([ 3.5303, -6.9037])\n",
      "Epoch 308, Loss 12.403256\n",
      "grad:tensor([-0.3121,  1.7669])\n",
      "params:tensor([ 3.5335, -6.9213])\n",
      "Epoch 309, Loss 12.371090\n",
      "grad:tensor([-0.3116,  1.7639])\n",
      "params:tensor([ 3.5366, -6.9390])\n",
      "Epoch 310, Loss 12.339031\n",
      "grad:tensor([-0.3111,  1.7609])\n",
      "params:tensor([ 3.5397, -6.9566])\n",
      "Epoch 311, Loss 12.307082\n",
      "grad:tensor([-0.3105,  1.7579])\n",
      "params:tensor([ 3.5428, -6.9742])\n",
      "Epoch 312, Loss 12.275247\n",
      "grad:tensor([-0.3100,  1.7549])\n",
      "params:tensor([ 3.5459, -6.9917])\n",
      "Epoch 313, Loss 12.243509\n",
      "grad:tensor([-0.3095,  1.7519])\n",
      "params:tensor([ 3.5490, -7.0092])\n",
      "Epoch 314, Loss 12.211887\n",
      "grad:tensor([-0.3090,  1.7490])\n",
      "params:tensor([ 3.5521, -7.0267])\n",
      "Epoch 315, Loss 12.180370\n",
      "grad:tensor([-0.3084,  1.7460])\n",
      "params:tensor([ 3.5552, -7.0442])\n",
      "Epoch 316, Loss 12.148962\n",
      "grad:tensor([-0.3079,  1.7430])\n",
      "params:tensor([ 3.5582, -7.0616])\n",
      "Epoch 317, Loss 12.117657\n",
      "grad:tensor([-0.3074,  1.7401])\n",
      "params:tensor([ 3.5613, -7.0790])\n",
      "Epoch 318, Loss 12.086462\n",
      "grad:tensor([-0.3069,  1.7371])\n",
      "params:tensor([ 3.5644, -7.0964])\n",
      "Epoch 319, Loss 12.055373\n",
      "grad:tensor([-0.3063,  1.7342])\n",
      "params:tensor([ 3.5674, -7.1137])\n",
      "Epoch 320, Loss 12.024384\n",
      "grad:tensor([-0.3058,  1.7312])\n",
      "params:tensor([ 3.5705, -7.1310])\n",
      "Epoch 321, Loss 11.993508\n",
      "grad:tensor([-0.3053,  1.7283])\n",
      "params:tensor([ 3.5736, -7.1483])\n",
      "Epoch 322, Loss 11.962731\n",
      "grad:tensor([-0.3048,  1.7253])\n",
      "params:tensor([ 3.5766, -7.1656])\n",
      "Epoch 323, Loss 11.932056\n",
      "grad:tensor([-0.3043,  1.7224])\n",
      "params:tensor([ 3.5796, -7.1828])\n",
      "Epoch 324, Loss 11.901492\n",
      "grad:tensor([-0.3037,  1.7195])\n",
      "params:tensor([ 3.5827, -7.2000])\n",
      "Epoch 325, Loss 11.871029\n",
      "grad:tensor([-0.3032,  1.7166])\n",
      "params:tensor([ 3.5857, -7.2172])\n",
      "Epoch 326, Loss 11.840671\n",
      "grad:tensor([-0.3027,  1.7136])\n",
      "params:tensor([ 3.5887, -7.2343])\n",
      "Epoch 327, Loss 11.810413\n",
      "grad:tensor([-0.3022,  1.7107])\n",
      "params:tensor([ 3.5918, -7.2514])\n",
      "Epoch 328, Loss 11.780257\n",
      "grad:tensor([-0.3017,  1.7078])\n",
      "params:tensor([ 3.5948, -7.2685])\n",
      "Epoch 329, Loss 11.750208\n",
      "grad:tensor([-0.3012,  1.7049])\n",
      "params:tensor([ 3.5978, -7.2855])\n",
      "Epoch 330, Loss 11.720258\n",
      "grad:tensor([-0.3007,  1.7020])\n",
      "params:tensor([ 3.6008, -7.3026])\n",
      "Epoch 331, Loss 11.690412\n",
      "grad:tensor([-0.3002,  1.6991])\n",
      "params:tensor([ 3.6038, -7.3196])\n",
      "Epoch 332, Loss 11.660664\n",
      "grad:tensor([-0.2996,  1.6963])\n",
      "params:tensor([ 3.6068, -7.3365])\n",
      "Epoch 333, Loss 11.631015\n",
      "grad:tensor([-0.2991,  1.6934])\n",
      "params:tensor([ 3.6098, -7.3535])\n",
      "Epoch 334, Loss 11.601473\n",
      "grad:tensor([-0.2986,  1.6905])\n",
      "params:tensor([ 3.6128, -7.3704])\n",
      "Epoch 335, Loss 11.572030\n",
      "grad:tensor([-0.2981,  1.6876])\n",
      "params:tensor([ 3.6158, -7.3872])\n",
      "Epoch 336, Loss 11.542686\n",
      "grad:tensor([-0.2976,  1.6848])\n",
      "params:tensor([ 3.6187, -7.4041])\n",
      "Epoch 337, Loss 11.513440\n",
      "grad:tensor([-0.2971,  1.6819])\n",
      "params:tensor([ 3.6217, -7.4209])\n",
      "Epoch 338, Loss 11.484293\n",
      "grad:tensor([-0.2966,  1.6790])\n",
      "params:tensor([ 3.6247, -7.4377])\n",
      "Epoch 339, Loss 11.455246\n",
      "grad:tensor([-0.2961,  1.6762])\n",
      "params:tensor([ 3.6276, -7.4545])\n",
      "Epoch 340, Loss 11.426300\n",
      "grad:tensor([-0.2956,  1.6733])\n",
      "params:tensor([ 3.6306, -7.4712])\n",
      "Epoch 341, Loss 11.397448\n",
      "grad:tensor([-0.2951,  1.6705])\n",
      "params:tensor([ 3.6335, -7.4879])\n",
      "Epoch 342, Loss 11.368696\n",
      "grad:tensor([-0.2946,  1.6677])\n",
      "params:tensor([ 3.6365, -7.5046])\n",
      "Epoch 343, Loss 11.340043\n",
      "grad:tensor([-0.2941,  1.6648])\n",
      "params:tensor([ 3.6394, -7.5212])\n",
      "Epoch 344, Loss 11.311487\n",
      "grad:tensor([-0.2936,  1.6620])\n",
      "params:tensor([ 3.6424, -7.5378])\n",
      "Epoch 345, Loss 11.283028\n",
      "grad:tensor([-0.2931,  1.6592])\n",
      "params:tensor([ 3.6453, -7.5544])\n",
      "Epoch 346, Loss 11.254662\n",
      "grad:tensor([-0.2926,  1.6564])\n",
      "params:tensor([ 3.6482, -7.5710])\n",
      "Epoch 347, Loss 11.226396\n",
      "grad:tensor([-0.2921,  1.6535])\n",
      "params:tensor([ 3.6511, -7.5875])\n",
      "Epoch 348, Loss 11.198220\n",
      "grad:tensor([-0.2916,  1.6507])\n",
      "params:tensor([ 3.6541, -7.6040])\n",
      "Epoch 349, Loss 11.170150\n",
      "grad:tensor([-0.2911,  1.6479])\n",
      "params:tensor([ 3.6570, -7.6205])\n",
      "Epoch 350, Loss 11.142170\n",
      "grad:tensor([-0.2906,  1.6451])\n",
      "params:tensor([ 3.6599, -7.6370])\n",
      "Epoch 351, Loss 11.114282\n",
      "grad:tensor([-0.2901,  1.6423])\n",
      "params:tensor([ 3.6628, -7.6534])\n",
      "Epoch 352, Loss 11.086491\n",
      "grad:tensor([-0.2896,  1.6395])\n",
      "params:tensor([ 3.6657, -7.6698])\n",
      "Epoch 353, Loss 11.058797\n",
      "grad:tensor([-0.2892,  1.6368])\n",
      "params:tensor([ 3.6686, -7.6861])\n",
      "Epoch 354, Loss 11.031193\n",
      "grad:tensor([-0.2886,  1.6340])\n",
      "params:tensor([ 3.6714, -7.7025])\n",
      "Epoch 355, Loss 11.003686\n",
      "grad:tensor([-0.2882,  1.6312])\n",
      "params:tensor([ 3.6743, -7.7188])\n",
      "Epoch 356, Loss 10.976270\n",
      "grad:tensor([-0.2877,  1.6284])\n",
      "params:tensor([ 3.6772, -7.7351])\n",
      "Epoch 357, Loss 10.948948\n",
      "grad:tensor([-0.2872,  1.6257])\n",
      "params:tensor([ 3.6801, -7.7513])\n",
      "Epoch 358, Loss 10.921719\n",
      "grad:tensor([-0.2867,  1.6229])\n",
      "params:tensor([ 3.6829, -7.7676])\n",
      "Epoch 359, Loss 10.894581\n",
      "grad:tensor([-0.2862,  1.6201])\n",
      "params:tensor([ 3.6858, -7.7838])\n",
      "Epoch 360, Loss 10.867537\n",
      "grad:tensor([-0.2857,  1.6174])\n",
      "params:tensor([ 3.6887, -7.7999])\n",
      "Epoch 361, Loss 10.840583\n",
      "grad:tensor([-0.2852,  1.6146])\n",
      "params:tensor([ 3.6915, -7.8161])\n",
      "Epoch 362, Loss 10.813721\n",
      "grad:tensor([-0.2847,  1.6119])\n",
      "params:tensor([ 3.6944, -7.8322])\n",
      "Epoch 363, Loss 10.786950\n",
      "grad:tensor([-0.2843,  1.6092])\n",
      "params:tensor([ 3.6972, -7.8483])\n",
      "Epoch 364, Loss 10.760270\n",
      "grad:tensor([-0.2838,  1.6064])\n",
      "params:tensor([ 3.7000, -7.8644])\n",
      "Epoch 365, Loss 10.733681\n",
      "grad:tensor([-0.2833,  1.6037])\n",
      "params:tensor([ 3.7029, -7.8804])\n",
      "Epoch 366, Loss 10.707184\n",
      "grad:tensor([-0.2828,  1.6010])\n",
      "params:tensor([ 3.7057, -7.8964])\n",
      "Epoch 367, Loss 10.680775\n",
      "grad:tensor([-0.2823,  1.5983])\n",
      "params:tensor([ 3.7085, -7.9124])\n",
      "Epoch 368, Loss 10.654454\n",
      "grad:tensor([-0.2819,  1.5955])\n",
      "params:tensor([ 3.7113, -7.9284])\n",
      "Epoch 369, Loss 10.628225\n",
      "grad:tensor([-0.2814,  1.5928])\n",
      "params:tensor([ 3.7142, -7.9443])\n",
      "Epoch 370, Loss 10.602086\n",
      "grad:tensor([-0.2809,  1.5901])\n",
      "params:tensor([ 3.7170, -7.9602])\n",
      "Epoch 371, Loss 10.576034\n",
      "grad:tensor([-0.2804,  1.5874])\n",
      "params:tensor([ 3.7198, -7.9761])\n",
      "Epoch 372, Loss 10.550071\n",
      "grad:tensor([-0.2799,  1.5847])\n",
      "params:tensor([ 3.7226, -7.9919])\n",
      "Epoch 373, Loss 10.524194\n",
      "grad:tensor([-0.2795,  1.5820])\n",
      "params:tensor([ 3.7254, -8.0077])\n",
      "Epoch 374, Loss 10.498409\n",
      "grad:tensor([-0.2790,  1.5794])\n",
      "params:tensor([ 3.7282, -8.0235])\n",
      "Epoch 375, Loss 10.472707\n",
      "grad:tensor([-0.2785,  1.5767])\n",
      "params:tensor([ 3.7309, -8.0393])\n",
      "Epoch 376, Loss 10.447093\n",
      "grad:tensor([-0.2780,  1.5740])\n",
      "params:tensor([ 3.7337, -8.0550])\n",
      "Epoch 377, Loss 10.421569\n",
      "grad:tensor([-0.2776,  1.5713])\n",
      "params:tensor([ 3.7365, -8.0707])\n",
      "Epoch 378, Loss 10.396132\n",
      "grad:tensor([-0.2771,  1.5686])\n",
      "params:tensor([ 3.7393, -8.0864])\n",
      "Epoch 379, Loss 10.370779\n",
      "grad:tensor([-0.2766,  1.5660])\n",
      "params:tensor([ 3.7420, -8.1021])\n",
      "Epoch 380, Loss 10.345510\n",
      "grad:tensor([-0.2762,  1.5633])\n",
      "params:tensor([ 3.7448, -8.1177])\n",
      "Epoch 381, Loss 10.320328\n",
      "grad:tensor([-0.2757,  1.5607])\n",
      "params:tensor([ 3.7476, -8.1333])\n",
      "Epoch 382, Loss 10.295234\n",
      "grad:tensor([-0.2752,  1.5580])\n",
      "params:tensor([ 3.7503, -8.1489])\n",
      "Epoch 383, Loss 10.270224\n",
      "grad:tensor([-0.2748,  1.5554])\n",
      "params:tensor([ 3.7531, -8.1645])\n",
      "Epoch 384, Loss 10.245296\n",
      "grad:tensor([-0.2743,  1.5527])\n",
      "params:tensor([ 3.7558, -8.1800])\n",
      "Epoch 385, Loss 10.220457\n",
      "grad:tensor([-0.2738,  1.5501])\n",
      "params:tensor([ 3.7585, -8.1955])\n",
      "Epoch 386, Loss 10.195701\n",
      "grad:tensor([-0.2734,  1.5475])\n",
      "params:tensor([ 3.7613, -8.2110])\n",
      "Epoch 387, Loss 10.171029\n",
      "grad:tensor([-0.2729,  1.5448])\n",
      "params:tensor([ 3.7640, -8.2264])\n",
      "Epoch 388, Loss 10.146438\n",
      "grad:tensor([-0.2724,  1.5422])\n",
      "params:tensor([ 3.7667, -8.2418])\n",
      "Epoch 389, Loss 10.121935\n",
      "grad:tensor([-0.2720,  1.5396])\n",
      "params:tensor([ 3.7694, -8.2572])\n",
      "Epoch 390, Loss 10.097512\n",
      "grad:tensor([-0.2715,  1.5370])\n",
      "params:tensor([ 3.7722, -8.2726])\n",
      "Epoch 391, Loss 10.073173\n",
      "grad:tensor([-0.2711,  1.5344])\n",
      "params:tensor([ 3.7749, -8.2879])\n",
      "Epoch 392, Loss 10.048919\n",
      "grad:tensor([-0.2706,  1.5317])\n",
      "params:tensor([ 3.7776, -8.3033])\n",
      "Epoch 393, Loss 10.024743\n",
      "grad:tensor([-0.2701,  1.5291])\n",
      "params:tensor([ 3.7803, -8.3185])\n",
      "Epoch 394, Loss 10.000652\n",
      "grad:tensor([-0.2697,  1.5265])\n",
      "params:tensor([ 3.7830, -8.3338])\n",
      "Epoch 395, Loss 9.976640\n",
      "grad:tensor([-0.2692,  1.5240])\n",
      "params:tensor([ 3.7857, -8.3491])\n",
      "Epoch 396, Loss 9.952712\n",
      "grad:tensor([-0.2688,  1.5214])\n",
      "params:tensor([ 3.7884, -8.3643])\n",
      "Epoch 397, Loss 9.928862\n",
      "grad:tensor([-0.2683,  1.5188])\n",
      "params:tensor([ 3.7910, -8.3795])\n",
      "Epoch 398, Loss 9.905093\n",
      "grad:tensor([-0.2678,  1.5162])\n",
      "params:tensor([ 3.7937, -8.3946])\n",
      "Epoch 399, Loss 9.881409\n",
      "grad:tensor([-0.2674,  1.5136])\n",
      "params:tensor([ 3.7964, -8.4098])\n",
      "Epoch 400, Loss 9.857804\n",
      "grad:tensor([-0.2669,  1.5111])\n",
      "params:tensor([ 3.7991, -8.4249])\n",
      "Epoch 401, Loss 9.834277\n",
      "grad:tensor([-0.2665,  1.5085])\n",
      "params:tensor([ 3.8017, -8.4399])\n",
      "Epoch 402, Loss 9.810831\n",
      "grad:tensor([-0.2660,  1.5059])\n",
      "params:tensor([ 3.8044, -8.4550])\n",
      "Epoch 403, Loss 9.787466\n",
      "grad:tensor([-0.2656,  1.5034])\n",
      "params:tensor([ 3.8070, -8.4700])\n",
      "Epoch 404, Loss 9.764176\n",
      "grad:tensor([-0.2651,  1.5008])\n",
      "params:tensor([ 3.8097, -8.4851])\n",
      "Epoch 405, Loss 9.740973\n",
      "grad:tensor([-0.2647,  1.4983])\n",
      "params:tensor([ 3.8123, -8.5000])\n",
      "Epoch 406, Loss 9.717843\n",
      "grad:tensor([-0.2642,  1.4957])\n",
      "params:tensor([ 3.8150, -8.5150])\n",
      "Epoch 407, Loss 9.694793\n",
      "grad:tensor([-0.2638,  1.4932])\n",
      "params:tensor([ 3.8176, -8.5299])\n",
      "Epoch 408, Loss 9.671824\n",
      "grad:tensor([-0.2633,  1.4906])\n",
      "params:tensor([ 3.8202, -8.5448])\n",
      "Epoch 409, Loss 9.648926\n",
      "grad:tensor([-0.2629,  1.4881])\n",
      "params:tensor([ 3.8229, -8.5597])\n",
      "Epoch 410, Loss 9.626110\n",
      "grad:tensor([-0.2624,  1.4856])\n",
      "params:tensor([ 3.8255, -8.5746])\n",
      "Epoch 411, Loss 9.603373\n",
      "grad:tensor([-0.2620,  1.4831])\n",
      "params:tensor([ 3.8281, -8.5894])\n",
      "Epoch 412, Loss 9.580709\n",
      "grad:tensor([-0.2615,  1.4805])\n",
      "params:tensor([ 3.8307, -8.6042])\n",
      "Epoch 413, Loss 9.558125\n",
      "grad:tensor([-0.2611,  1.4780])\n",
      "params:tensor([ 3.8333, -8.6190])\n",
      "Epoch 414, Loss 9.535617\n",
      "grad:tensor([-0.2606,  1.4755])\n",
      "params:tensor([ 3.8360, -8.6337])\n",
      "Epoch 415, Loss 9.513184\n",
      "grad:tensor([-0.2602,  1.4730])\n",
      "params:tensor([ 3.8386, -8.6485])\n",
      "Epoch 416, Loss 9.490829\n",
      "grad:tensor([-0.2598,  1.4705])\n",
      "params:tensor([ 3.8412, -8.6632])\n",
      "Epoch 417, Loss 9.468551\n",
      "grad:tensor([-0.2593,  1.4680])\n",
      "params:tensor([ 3.8437, -8.6779])\n",
      "Epoch 418, Loss 9.446347\n",
      "grad:tensor([-0.2589,  1.4655])\n",
      "params:tensor([ 3.8463, -8.6925])\n",
      "Epoch 419, Loss 9.424216\n",
      "grad:tensor([-0.2584,  1.4630])\n",
      "params:tensor([ 3.8489, -8.7071])\n",
      "Epoch 420, Loss 9.402164\n",
      "grad:tensor([-0.2580,  1.4605])\n",
      "params:tensor([ 3.8515, -8.7217])\n",
      "Epoch 421, Loss 9.380184\n",
      "grad:tensor([-0.2576,  1.4581])\n",
      "params:tensor([ 3.8541, -8.7363])\n",
      "Epoch 422, Loss 9.358282\n",
      "grad:tensor([-0.2571,  1.4556])\n",
      "params:tensor([ 3.8566, -8.7509])\n",
      "Epoch 423, Loss 9.336448\n",
      "grad:tensor([-0.2567,  1.4531])\n",
      "params:tensor([ 3.8592, -8.7654])\n",
      "Epoch 424, Loss 9.314695\n",
      "grad:tensor([-0.2563,  1.4506])\n",
      "params:tensor([ 3.8618, -8.7799])\n",
      "Epoch 425, Loss 9.293012\n",
      "grad:tensor([-0.2558,  1.4482])\n",
      "params:tensor([ 3.8643, -8.7944])\n",
      "Epoch 426, Loss 9.271403\n",
      "grad:tensor([-0.2554,  1.4457])\n",
      "params:tensor([ 3.8669, -8.8089])\n",
      "Epoch 427, Loss 9.249871\n",
      "grad:tensor([-0.2550,  1.4433])\n",
      "params:tensor([ 3.8694, -8.8233])\n",
      "Epoch 428, Loss 9.228410\n",
      "grad:tensor([-0.2545,  1.4408])\n",
      "params:tensor([ 3.8720, -8.8377])\n",
      "Epoch 429, Loss 9.207022\n",
      "grad:tensor([-0.2541,  1.4384])\n",
      "params:tensor([ 3.8745, -8.8521])\n",
      "Epoch 430, Loss 9.185704\n",
      "grad:tensor([-0.2537,  1.4359])\n",
      "params:tensor([ 3.8771, -8.8664])\n",
      "Epoch 431, Loss 9.164462\n",
      "grad:tensor([-0.2532,  1.4335])\n",
      "params:tensor([ 3.8796, -8.8808])\n",
      "Epoch 432, Loss 9.143289\n",
      "grad:tensor([-0.2528,  1.4310])\n",
      "params:tensor([ 3.8821, -8.8951])\n",
      "Epoch 433, Loss 9.122189\n",
      "grad:tensor([-0.2524,  1.4286])\n",
      "params:tensor([ 3.8846, -8.9094])\n",
      "Epoch 434, Loss 9.101160\n",
      "grad:tensor([-0.2519,  1.4262])\n",
      "params:tensor([ 3.8872, -8.9236])\n",
      "Epoch 435, Loss 9.080204\n",
      "grad:tensor([-0.2515,  1.4238])\n",
      "params:tensor([ 3.8897, -8.9379])\n",
      "Epoch 436, Loss 9.059318\n",
      "grad:tensor([-0.2511,  1.4213])\n",
      "params:tensor([ 3.8922, -8.9521])\n",
      "Epoch 437, Loss 9.038502\n",
      "grad:tensor([-0.2507,  1.4189])\n",
      "params:tensor([ 3.8947, -8.9663])\n",
      "Epoch 438, Loss 9.017757\n",
      "grad:tensor([-0.2502,  1.4165])\n",
      "params:tensor([ 3.8972, -8.9804])\n",
      "Epoch 439, Loss 8.997084\n",
      "grad:tensor([-0.2498,  1.4141])\n",
      "params:tensor([ 3.8997, -8.9946])\n",
      "Epoch 440, Loss 8.976479\n",
      "grad:tensor([-0.2494,  1.4117])\n",
      "params:tensor([ 3.9022, -9.0087])\n",
      "Epoch 441, Loss 8.955944\n",
      "grad:tensor([-0.2489,  1.4093])\n",
      "params:tensor([ 3.9047, -9.0228])\n",
      "Epoch 442, Loss 8.935480\n",
      "grad:tensor([-0.2485,  1.4069])\n",
      "params:tensor([ 3.9072, -9.0369])\n",
      "Epoch 443, Loss 8.915089\n",
      "grad:tensor([-0.2481,  1.4045])\n",
      "params:tensor([ 3.9096, -9.0509])\n",
      "Epoch 444, Loss 8.894762\n",
      "grad:tensor([-0.2477,  1.4021])\n",
      "params:tensor([ 3.9121, -9.0649])\n",
      "Epoch 445, Loss 8.874508\n",
      "grad:tensor([-0.2473,  1.3998])\n",
      "params:tensor([ 3.9146, -9.0789])\n",
      "Epoch 446, Loss 8.854318\n",
      "grad:tensor([-0.2468,  1.3974])\n",
      "params:tensor([ 3.9171, -9.0929])\n",
      "Epoch 447, Loss 8.834197\n",
      "grad:tensor([-0.2464,  1.3950])\n",
      "params:tensor([ 3.9195, -9.1068])\n",
      "Epoch 448, Loss 8.814149\n",
      "grad:tensor([-0.2460,  1.3926])\n",
      "params:tensor([ 3.9220, -9.1208])\n",
      "Epoch 449, Loss 8.794162\n",
      "grad:tensor([-0.2456,  1.3903])\n",
      "params:tensor([ 3.9244, -9.1347])\n",
      "Epoch 450, Loss 8.774253\n",
      "grad:tensor([-0.2452,  1.3879])\n",
      "params:tensor([ 3.9269, -9.1486])\n",
      "Epoch 451, Loss 8.754405\n",
      "grad:tensor([-0.2448,  1.3856])\n",
      "params:tensor([ 3.9293, -9.1624])\n",
      "Epoch 452, Loss 8.734623\n",
      "grad:tensor([-0.2443,  1.3832])\n",
      "params:tensor([ 3.9318, -9.1762])\n",
      "Epoch 453, Loss 8.714911\n",
      "grad:tensor([-0.2439,  1.3808])\n",
      "params:tensor([ 3.9342, -9.1901])\n",
      "Epoch 454, Loss 8.695266\n",
      "grad:tensor([-0.2435,  1.3785])\n",
      "params:tensor([ 3.9367, -9.2038])\n",
      "Epoch 455, Loss 8.675688\n",
      "grad:tensor([-0.2431,  1.3762])\n",
      "params:tensor([ 3.9391, -9.2176])\n",
      "Epoch 456, Loss 8.656173\n",
      "grad:tensor([-0.2427,  1.3738])\n",
      "params:tensor([ 3.9415, -9.2313])\n",
      "Epoch 457, Loss 8.636729\n",
      "grad:tensor([-0.2423,  1.3715])\n",
      "params:tensor([ 3.9439, -9.2451])\n",
      "Epoch 458, Loss 8.617347\n",
      "grad:tensor([-0.2419,  1.3692])\n",
      "params:tensor([ 3.9464, -9.2587])\n",
      "Epoch 459, Loss 8.598029\n",
      "grad:tensor([-0.2414,  1.3668])\n",
      "params:tensor([ 3.9488, -9.2724])\n",
      "Epoch 460, Loss 8.578781\n",
      "grad:tensor([-0.2410,  1.3645])\n",
      "params:tensor([ 3.9512, -9.2861])\n",
      "Epoch 461, Loss 8.559597\n",
      "grad:tensor([-0.2406,  1.3622])\n",
      "params:tensor([ 3.9536, -9.2997])\n",
      "Epoch 462, Loss 8.540479\n",
      "grad:tensor([-0.2402,  1.3599])\n",
      "params:tensor([ 3.9560, -9.3133])\n",
      "Epoch 463, Loss 8.521426\n",
      "grad:tensor([-0.2398,  1.3576])\n",
      "params:tensor([ 3.9584, -9.3269])\n",
      "Epoch 464, Loss 8.502437\n",
      "grad:tensor([-0.2394,  1.3553])\n",
      "params:tensor([ 3.9608, -9.3404])\n",
      "Epoch 465, Loss 8.483517\n",
      "grad:tensor([-0.2390,  1.3530])\n",
      "params:tensor([ 3.9632, -9.3539])\n",
      "Epoch 466, Loss 8.464652\n",
      "grad:tensor([-0.2386,  1.3507])\n",
      "params:tensor([ 3.9656, -9.3674])\n",
      "Epoch 467, Loss 8.445858\n",
      "grad:tensor([-0.2382,  1.3484])\n",
      "params:tensor([ 3.9679, -9.3809])\n",
      "Epoch 468, Loss 8.427128\n",
      "grad:tensor([-0.2378,  1.3461])\n",
      "params:tensor([ 3.9703, -9.3944])\n",
      "Epoch 469, Loss 8.408454\n",
      "grad:tensor([-0.2374,  1.3438])\n",
      "params:tensor([ 3.9727, -9.4078])\n",
      "Epoch 470, Loss 8.389848\n",
      "grad:tensor([-0.2370,  1.3415])\n",
      "params:tensor([ 3.9751, -9.4212])\n",
      "Epoch 471, Loss 8.371305\n",
      "grad:tensor([-0.2366,  1.3392])\n",
      "params:tensor([ 3.9774, -9.4346])\n",
      "Epoch 472, Loss 8.352828\n",
      "grad:tensor([-0.2362,  1.3370])\n",
      "params:tensor([ 3.9798, -9.4480])\n",
      "Epoch 473, Loss 8.334409\n",
      "grad:tensor([-0.2358,  1.3347])\n",
      "params:tensor([ 3.9822, -9.4614])\n",
      "Epoch 474, Loss 8.316054\n",
      "grad:tensor([-0.2354,  1.3324])\n",
      "params:tensor([ 3.9845, -9.4747])\n",
      "Epoch 475, Loss 8.297764\n",
      "grad:tensor([-0.2350,  1.3301])\n",
      "params:tensor([ 3.9869, -9.4880])\n",
      "Epoch 476, Loss 8.279534\n",
      "grad:tensor([-0.2346,  1.3279])\n",
      "params:tensor([ 3.9892, -9.5013])\n",
      "Epoch 477, Loss 8.261369\n",
      "grad:tensor([-0.2342,  1.3256])\n",
      "params:tensor([ 3.9915, -9.5145])\n",
      "Epoch 478, Loss 8.243259\n",
      "grad:tensor([-0.2338,  1.3234])\n",
      "params:tensor([ 3.9939, -9.5277])\n",
      "Epoch 479, Loss 8.225213\n",
      "grad:tensor([-0.2334,  1.3211])\n",
      "params:tensor([ 3.9962, -9.5410])\n",
      "Epoch 480, Loss 8.207231\n",
      "grad:tensor([-0.2330,  1.3189])\n",
      "params:tensor([ 3.9985, -9.5541])\n",
      "Epoch 481, Loss 8.189310\n",
      "grad:tensor([-0.2326,  1.3166])\n",
      "params:tensor([ 4.0009, -9.5673])\n",
      "Epoch 482, Loss 8.171452\n",
      "grad:tensor([-0.2322,  1.3144])\n",
      "params:tensor([ 4.0032, -9.5805])\n",
      "Epoch 483, Loss 8.153647\n",
      "grad:tensor([-0.2318,  1.3122])\n",
      "params:tensor([ 4.0055, -9.5936])\n",
      "Epoch 484, Loss 8.135906\n",
      "grad:tensor([-0.2314,  1.3100])\n",
      "params:tensor([ 4.0078, -9.6067])\n",
      "Epoch 485, Loss 8.118226\n",
      "grad:tensor([-0.2310,  1.3077])\n",
      "params:tensor([ 4.0101, -9.6198])\n",
      "Epoch 486, Loss 8.100607\n",
      "grad:tensor([-0.2306,  1.3055])\n",
      "params:tensor([ 4.0124, -9.6328])\n",
      "Epoch 487, Loss 8.083045\n",
      "grad:tensor([-0.2302,  1.3033])\n",
      "params:tensor([ 4.0147, -9.6458])\n",
      "Epoch 488, Loss 8.065548\n",
      "grad:tensor([-0.2298,  1.3011])\n",
      "params:tensor([ 4.0170, -9.6589])\n",
      "Epoch 489, Loss 8.048104\n",
      "grad:tensor([-0.2295,  1.2989])\n",
      "params:tensor([ 4.0193, -9.6718])\n",
      "Epoch 490, Loss 8.030724\n",
      "grad:tensor([-0.2291,  1.2967])\n",
      "params:tensor([ 4.0216, -9.6848])\n",
      "Epoch 491, Loss 8.013401\n",
      "grad:tensor([-0.2287,  1.2945])\n",
      "params:tensor([ 4.0239, -9.6978])\n",
      "Epoch 492, Loss 7.996137\n",
      "grad:tensor([-0.2283,  1.2923])\n",
      "params:tensor([ 4.0262, -9.7107])\n",
      "Epoch 493, Loss 7.978930\n",
      "grad:tensor([-0.2279,  1.2901])\n",
      "params:tensor([ 4.0285, -9.7236])\n",
      "Epoch 494, Loss 7.961783\n",
      "grad:tensor([-0.2275,  1.2879])\n",
      "params:tensor([ 4.0308, -9.7365])\n",
      "Epoch 495, Loss 7.944690\n",
      "grad:tensor([-0.2271,  1.2857])\n",
      "params:tensor([ 4.0330, -9.7493])\n",
      "Epoch 496, Loss 7.927663\n",
      "grad:tensor([-0.2267,  1.2835])\n",
      "params:tensor([ 4.0353, -9.7621])\n",
      "Epoch 497, Loss 7.910690\n",
      "grad:tensor([-0.2263,  1.2813])\n",
      "params:tensor([ 4.0376, -9.7750])\n",
      "Epoch 498, Loss 7.893775\n",
      "grad:tensor([-0.2260,  1.2791])\n",
      "params:tensor([ 4.0398, -9.7878])\n",
      "Epoch 499, Loss 7.876915\n",
      "grad:tensor([-0.2256,  1.2770])\n",
      "params:tensor([ 4.0421, -9.8005])\n",
      "Epoch 500, Loss 7.860115\n",
      "grad:tensor([-0.2252,  1.2748])\n",
      "params:tensor([ 4.0443, -9.8133])\n",
      "Epoch 501, Loss 7.843369\n",
      "grad:tensor([-0.2248,  1.2726])\n",
      "params:tensor([ 4.0466, -9.8260])\n",
      "Epoch 502, Loss 7.826683\n",
      "grad:tensor([-0.2244,  1.2705])\n",
      "params:tensor([ 4.0488, -9.8387])\n",
      "Epoch 503, Loss 7.810053\n",
      "grad:tensor([-0.2241,  1.2683])\n",
      "params:tensor([ 4.0511, -9.8514])\n",
      "Epoch 504, Loss 7.793481\n",
      "grad:tensor([-0.2237,  1.2662])\n",
      "params:tensor([ 4.0533, -9.8640])\n",
      "Epoch 505, Loss 7.776962\n",
      "grad:tensor([-0.2233,  1.2640])\n",
      "params:tensor([ 4.0555, -9.8767])\n",
      "Epoch 506, Loss 7.760498\n",
      "grad:tensor([-0.2229,  1.2619])\n",
      "params:tensor([ 4.0578, -9.8893])\n",
      "Epoch 507, Loss 7.744092\n",
      "grad:tensor([-0.2225,  1.2597])\n",
      "params:tensor([ 4.0600, -9.9019])\n",
      "Epoch 508, Loss 7.727745\n",
      "grad:tensor([-0.2222,  1.2576])\n",
      "params:tensor([ 4.0622, -9.9145])\n",
      "Epoch 509, Loss 7.711449\n",
      "grad:tensor([-0.2218,  1.2554])\n",
      "params:tensor([ 4.0644, -9.9270])\n",
      "Epoch 510, Loss 7.695211\n",
      "grad:tensor([-0.2214,  1.2533])\n",
      "params:tensor([ 4.0666, -9.9396])\n",
      "Epoch 511, Loss 7.679024\n",
      "grad:tensor([-0.2210,  1.2512])\n",
      "params:tensor([ 4.0688, -9.9521])\n",
      "Epoch 512, Loss 7.662896\n",
      "grad:tensor([-0.2207,  1.2490])\n",
      "params:tensor([ 4.0710, -9.9646])\n",
      "Epoch 513, Loss 7.646820\n",
      "grad:tensor([-0.2203,  1.2469])\n",
      "params:tensor([ 4.0733, -9.9770])\n",
      "Epoch 514, Loss 7.630803\n",
      "grad:tensor([-0.2199,  1.2448])\n",
      "params:tensor([ 4.0754, -9.9895])\n",
      "Epoch 515, Loss 7.614836\n",
      "grad:tensor([-0.2195,  1.2427])\n",
      "params:tensor([  4.0776, -10.0019])\n",
      "Epoch 516, Loss 7.598925\n",
      "grad:tensor([-0.2192,  1.2406])\n",
      "params:tensor([  4.0798, -10.0143])\n",
      "Epoch 517, Loss 7.583069\n",
      "grad:tensor([-0.2188,  1.2385])\n",
      "params:tensor([  4.0820, -10.0267])\n",
      "Epoch 518, Loss 7.567265\n",
      "grad:tensor([-0.2184,  1.2364])\n",
      "params:tensor([  4.0842, -10.0391])\n",
      "Epoch 519, Loss 7.551515\n",
      "grad:tensor([-0.2180,  1.2343])\n",
      "params:tensor([  4.0864, -10.0514])\n",
      "Epoch 520, Loss 7.535818\n",
      "grad:tensor([-0.2177,  1.2322])\n",
      "params:tensor([  4.0886, -10.0637])\n",
      "Epoch 521, Loss 7.520176\n",
      "grad:tensor([-0.2173,  1.2301])\n",
      "params:tensor([  4.0907, -10.0760])\n",
      "Epoch 522, Loss 7.504587\n",
      "grad:tensor([-0.2169,  1.2280])\n",
      "params:tensor([  4.0929, -10.0883])\n",
      "Epoch 523, Loss 7.489048\n",
      "grad:tensor([-0.2165,  1.2259])\n",
      "params:tensor([  4.0951, -10.1006])\n",
      "Epoch 524, Loss 7.473566\n",
      "grad:tensor([-0.2162,  1.2238])\n",
      "params:tensor([  4.0972, -10.1128])\n",
      "Epoch 525, Loss 7.458135\n",
      "grad:tensor([-0.2158,  1.2217])\n",
      "params:tensor([  4.0994, -10.1250])\n",
      "Epoch 526, Loss 7.442750\n",
      "grad:tensor([-0.2155,  1.2197])\n",
      "params:tensor([  4.1015, -10.1372])\n",
      "Epoch 527, Loss 7.427427\n",
      "grad:tensor([-0.2151,  1.2176])\n",
      "params:tensor([  4.1037, -10.1494])\n",
      "Epoch 528, Loss 7.412152\n",
      "grad:tensor([-0.2147,  1.2155])\n",
      "params:tensor([  4.1058, -10.1616])\n",
      "Epoch 529, Loss 7.396928\n",
      "grad:tensor([-0.2144,  1.2135])\n",
      "params:tensor([  4.1080, -10.1737])\n",
      "Epoch 530, Loss 7.381757\n",
      "grad:tensor([-0.2140,  1.2114])\n",
      "params:tensor([  4.1101, -10.1858])\n",
      "Epoch 531, Loss 7.366637\n",
      "grad:tensor([-0.2136,  1.2093])\n",
      "params:tensor([  4.1123, -10.1979])\n",
      "Epoch 532, Loss 7.351567\n",
      "grad:tensor([-0.2133,  1.2073])\n",
      "params:tensor([  4.1144, -10.2100])\n",
      "Epoch 533, Loss 7.336549\n",
      "grad:tensor([-0.2129,  1.2052])\n",
      "params:tensor([  4.1165, -10.2220])\n",
      "Epoch 534, Loss 7.321584\n",
      "grad:tensor([-0.2125,  1.2032])\n",
      "params:tensor([  4.1187, -10.2340])\n",
      "Epoch 535, Loss 7.306671\n",
      "grad:tensor([-0.2122,  1.2012])\n",
      "params:tensor([  4.1208, -10.2461])\n",
      "Epoch 536, Loss 7.291804\n",
      "grad:tensor([-0.2118,  1.1991])\n",
      "params:tensor([  4.1229, -10.2581])\n",
      "Epoch 537, Loss 7.276989\n",
      "grad:tensor([-0.2115,  1.1971])\n",
      "params:tensor([  4.1250, -10.2700])\n",
      "Epoch 538, Loss 7.262227\n",
      "grad:tensor([-0.2111,  1.1950])\n",
      "params:tensor([  4.1271, -10.2820])\n",
      "Epoch 539, Loss 7.247512\n",
      "grad:tensor([-0.2108,  1.1930])\n",
      "params:tensor([  4.1292, -10.2939])\n",
      "Epoch 540, Loss 7.232845\n",
      "grad:tensor([-0.2104,  1.1910])\n",
      "params:tensor([  4.1313, -10.3058])\n",
      "Epoch 541, Loss 7.218231\n",
      "grad:tensor([-0.2100,  1.1890])\n",
      "params:tensor([  4.1334, -10.3177])\n",
      "Epoch 542, Loss 7.203665\n",
      "grad:tensor([-0.2097,  1.1869])\n",
      "params:tensor([  4.1355, -10.3296])\n",
      "Epoch 543, Loss 7.189151\n",
      "grad:tensor([-0.2093,  1.1849])\n",
      "params:tensor([  4.1376, -10.3414])\n",
      "Epoch 544, Loss 7.174683\n",
      "grad:tensor([-0.2090,  1.1829])\n",
      "params:tensor([  4.1397, -10.3533])\n",
      "Epoch 545, Loss 7.160266\n",
      "grad:tensor([-0.2086,  1.1809])\n",
      "params:tensor([  4.1418, -10.3651])\n",
      "Epoch 546, Loss 7.145897\n",
      "grad:tensor([-0.2083,  1.1789])\n",
      "params:tensor([  4.1439, -10.3769])\n",
      "Epoch 547, Loss 7.131581\n",
      "grad:tensor([-0.2079,  1.1769])\n",
      "params:tensor([  4.1460, -10.3886])\n",
      "Epoch 548, Loss 7.117305\n",
      "grad:tensor([-0.2075,  1.1749])\n",
      "params:tensor([  4.1480, -10.4004])\n",
      "Epoch 549, Loss 7.103083\n",
      "grad:tensor([-0.2072,  1.1729])\n",
      "params:tensor([  4.1501, -10.4121])\n",
      "Epoch 550, Loss 7.088911\n",
      "grad:tensor([-0.2068,  1.1709])\n",
      "params:tensor([  4.1522, -10.4238])\n",
      "Epoch 551, Loss 7.074785\n",
      "grad:tensor([-0.2065,  1.1689])\n",
      "params:tensor([  4.1542, -10.4355])\n",
      "Epoch 552, Loss 7.060707\n",
      "grad:tensor([-0.2062,  1.1669])\n",
      "params:tensor([  4.1563, -10.4472])\n",
      "Epoch 553, Loss 7.046676\n",
      "grad:tensor([-0.2058,  1.1649])\n",
      "params:tensor([  4.1584, -10.4588])\n",
      "Epoch 554, Loss 7.032695\n",
      "grad:tensor([-0.2054,  1.1630])\n",
      "params:tensor([  4.1604, -10.4704])\n",
      "Epoch 555, Loss 7.018755\n",
      "grad:tensor([-0.2051,  1.1610])\n",
      "params:tensor([  4.1625, -10.4821])\n",
      "Epoch 556, Loss 7.004870\n",
      "grad:tensor([-0.2047,  1.1590])\n",
      "params:tensor([  4.1645, -10.4936])\n",
      "Epoch 557, Loss 6.991028\n",
      "grad:tensor([-0.2044,  1.1571])\n",
      "params:tensor([  4.1666, -10.5052])\n",
      "Epoch 558, Loss 6.977232\n",
      "grad:tensor([-0.2041,  1.1551])\n",
      "params:tensor([  4.1686, -10.5168])\n",
      "Epoch 559, Loss 6.963488\n",
      "grad:tensor([-0.2037,  1.1531])\n",
      "params:tensor([  4.1706, -10.5283])\n",
      "Epoch 560, Loss 6.949787\n",
      "grad:tensor([-0.2034,  1.1512])\n",
      "params:tensor([  4.1727, -10.5398])\n",
      "Epoch 561, Loss 6.936135\n",
      "grad:tensor([-0.2030,  1.1492])\n",
      "params:tensor([  4.1747, -10.5513])\n",
      "Epoch 562, Loss 6.922528\n",
      "grad:tensor([-0.2027,  1.1473])\n",
      "params:tensor([  4.1767, -10.5628])\n",
      "Epoch 563, Loss 6.908967\n",
      "grad:tensor([-0.2023,  1.1453])\n",
      "params:tensor([  4.1787, -10.5742])\n",
      "Epoch 564, Loss 6.895452\n",
      "grad:tensor([-0.2020,  1.1434])\n",
      "params:tensor([  4.1808, -10.5857])\n",
      "Epoch 565, Loss 6.881980\n",
      "grad:tensor([-0.2016,  1.1414])\n",
      "params:tensor([  4.1828, -10.5971])\n",
      "Epoch 566, Loss 6.868559\n",
      "grad:tensor([-0.2013,  1.1395])\n",
      "params:tensor([  4.1848, -10.6085])\n",
      "Epoch 567, Loss 6.855180\n",
      "grad:tensor([-0.2010,  1.1375])\n",
      "params:tensor([  4.1868, -10.6198])\n",
      "Epoch 568, Loss 6.841848\n",
      "grad:tensor([-0.2006,  1.1356])\n",
      "params:tensor([  4.1888, -10.6312])\n",
      "Epoch 569, Loss 6.828561\n",
      "grad:tensor([-0.2003,  1.1337])\n",
      "params:tensor([  4.1908, -10.6425])\n",
      "Epoch 570, Loss 6.815319\n",
      "grad:tensor([-0.1999,  1.1318])\n",
      "params:tensor([  4.1928, -10.6539])\n",
      "Epoch 571, Loss 6.802118\n",
      "grad:tensor([-0.1996,  1.1298])\n",
      "params:tensor([  4.1948, -10.6652])\n",
      "Epoch 572, Loss 6.788968\n",
      "grad:tensor([-0.1993,  1.1279])\n",
      "params:tensor([  4.1968, -10.6764])\n",
      "Epoch 573, Loss 6.775864\n",
      "grad:tensor([-0.1989,  1.1260])\n",
      "params:tensor([  4.1988, -10.6877])\n",
      "Epoch 574, Loss 6.762797\n",
      "grad:tensor([-0.1986,  1.1241])\n",
      "params:tensor([  4.2008, -10.6989])\n",
      "Epoch 575, Loss 6.749779\n",
      "grad:tensor([-0.1982,  1.1222])\n",
      "params:tensor([  4.2028, -10.7102])\n",
      "Epoch 576, Loss 6.736804\n",
      "grad:tensor([-0.1979,  1.1203])\n",
      "params:tensor([  4.2047, -10.7214])\n",
      "Epoch 577, Loss 6.723876\n",
      "grad:tensor([-0.1976,  1.1184])\n",
      "params:tensor([  4.2067, -10.7325])\n",
      "Epoch 578, Loss 6.710987\n",
      "grad:tensor([-0.1972,  1.1165])\n",
      "params:tensor([  4.2087, -10.7437])\n",
      "Epoch 579, Loss 6.698142\n",
      "grad:tensor([-0.1969,  1.1146])\n",
      "params:tensor([  4.2107, -10.7549])\n",
      "Epoch 580, Loss 6.685345\n",
      "grad:tensor([-0.1966,  1.1127])\n",
      "params:tensor([  4.2126, -10.7660])\n",
      "Epoch 581, Loss 6.672589\n",
      "grad:tensor([-0.1962,  1.1108])\n",
      "params:tensor([  4.2146, -10.7771])\n",
      "Epoch 582, Loss 6.659873\n",
      "grad:tensor([-0.1959,  1.1089])\n",
      "params:tensor([  4.2165, -10.7882])\n",
      "Epoch 583, Loss 6.647207\n",
      "grad:tensor([-0.1956,  1.1070])\n",
      "params:tensor([  4.2185, -10.7992])\n",
      "Epoch 584, Loss 6.634578\n",
      "grad:tensor([-0.1952,  1.1051])\n",
      "params:tensor([  4.2204, -10.8103])\n",
      "Epoch 585, Loss 6.621994\n",
      "grad:tensor([-0.1949,  1.1033])\n",
      "params:tensor([  4.2224, -10.8213])\n",
      "Epoch 586, Loss 6.609454\n",
      "grad:tensor([-0.1946,  1.1014])\n",
      "params:tensor([  4.2243, -10.8323])\n",
      "Epoch 587, Loss 6.596953\n",
      "grad:tensor([-0.1942,  1.0995])\n",
      "params:tensor([  4.2263, -10.8433])\n",
      "Epoch 588, Loss 6.584499\n",
      "grad:tensor([-0.1939,  1.0976])\n",
      "params:tensor([  4.2282, -10.8543])\n",
      "Epoch 589, Loss 6.572087\n",
      "grad:tensor([-0.1936,  1.0958])\n",
      "params:tensor([  4.2302, -10.8653])\n",
      "Epoch 590, Loss 6.559712\n",
      "grad:tensor([-0.1932,  1.0939])\n",
      "params:tensor([  4.2321, -10.8762])\n",
      "Epoch 591, Loss 6.547384\n",
      "grad:tensor([-0.1929,  1.0921])\n",
      "params:tensor([  4.2340, -10.8871])\n",
      "Epoch 592, Loss 6.535097\n",
      "grad:tensor([-0.1926,  1.0902])\n",
      "params:tensor([  4.2359, -10.8980])\n",
      "Epoch 593, Loss 6.522851\n",
      "grad:tensor([-0.1923,  1.0884])\n",
      "params:tensor([  4.2379, -10.9089])\n",
      "Epoch 594, Loss 6.510646\n",
      "grad:tensor([-0.1919,  1.0865])\n",
      "params:tensor([  4.2398, -10.9198])\n",
      "Epoch 595, Loss 6.498482\n",
      "grad:tensor([-0.1916,  1.0847])\n",
      "params:tensor([  4.2417, -10.9306])\n",
      "Epoch 596, Loss 6.486361\n",
      "grad:tensor([-0.1913,  1.0828])\n",
      "params:tensor([  4.2436, -10.9415])\n",
      "Epoch 597, Loss 6.474282\n",
      "grad:tensor([-0.1910,  1.0810])\n",
      "params:tensor([  4.2455, -10.9523])\n",
      "Epoch 598, Loss 6.462241\n",
      "grad:tensor([-0.1906,  1.0791])\n",
      "params:tensor([  4.2474, -10.9631])\n",
      "Epoch 599, Loss 6.450243\n",
      "grad:tensor([-0.1903,  1.0773])\n",
      "params:tensor([  4.2493, -10.9738])\n",
      "Epoch 600, Loss 6.438284\n",
      "grad:tensor([-0.1900,  1.0755])\n",
      "params:tensor([  4.2512, -10.9846])\n",
      "Epoch 601, Loss 6.426368\n",
      "grad:tensor([-0.1897,  1.0737])\n",
      "params:tensor([  4.2531, -10.9953])\n",
      "Epoch 602, Loss 6.414490\n",
      "grad:tensor([-0.1893,  1.0718])\n",
      "params:tensor([  4.2550, -11.0060])\n",
      "Epoch 603, Loss 6.402653\n",
      "grad:tensor([-0.1890,  1.0700])\n",
      "params:tensor([  4.2569, -11.0167])\n",
      "Epoch 604, Loss 6.390859\n",
      "grad:tensor([-0.1887,  1.0682])\n",
      "params:tensor([  4.2588, -11.0274])\n",
      "Epoch 605, Loss 6.379103\n",
      "grad:tensor([-0.1884,  1.0664])\n",
      "params:tensor([  4.2607, -11.0381])\n",
      "Epoch 606, Loss 6.367385\n",
      "grad:tensor([-0.1880,  1.0646])\n",
      "params:tensor([  4.2626, -11.0487])\n",
      "Epoch 607, Loss 6.355706\n",
      "grad:tensor([-0.1877,  1.0628])\n",
      "params:tensor([  4.2644, -11.0594])\n",
      "Epoch 608, Loss 6.344070\n",
      "grad:tensor([-0.1874,  1.0609])\n",
      "params:tensor([  4.2663, -11.0700])\n",
      "Epoch 609, Loss 6.332472\n",
      "grad:tensor([-0.1871,  1.0591])\n",
      "params:tensor([  4.2682, -11.0806])\n",
      "Epoch 610, Loss 6.320912\n",
      "grad:tensor([-0.1868,  1.0573])\n",
      "params:tensor([  4.2701, -11.0911])\n",
      "Epoch 611, Loss 6.309395\n",
      "grad:tensor([-0.1865,  1.0555])\n",
      "params:tensor([  4.2719, -11.1017])\n",
      "Epoch 612, Loss 6.297915\n",
      "grad:tensor([-0.1861,  1.0538])\n",
      "params:tensor([  4.2738, -11.1122])\n",
      "Epoch 613, Loss 6.286473\n",
      "grad:tensor([-0.1858,  1.0520])\n",
      "params:tensor([  4.2756, -11.1227])\n",
      "Epoch 614, Loss 6.275074\n",
      "grad:tensor([-0.1855,  1.0502])\n",
      "params:tensor([  4.2775, -11.1333])\n",
      "Epoch 615, Loss 6.263708\n",
      "grad:tensor([-0.1852,  1.0484])\n",
      "params:tensor([  4.2794, -11.1437])\n",
      "Epoch 616, Loss 6.252382\n",
      "grad:tensor([-0.1849,  1.0466])\n",
      "params:tensor([  4.2812, -11.1542])\n",
      "Epoch 617, Loss 6.241098\n",
      "grad:tensor([-0.1846,  1.0448])\n",
      "params:tensor([  4.2830, -11.1646])\n",
      "Epoch 618, Loss 6.229849\n",
      "grad:tensor([-0.1843,  1.0431])\n",
      "params:tensor([  4.2849, -11.1751])\n",
      "Epoch 619, Loss 6.218639\n",
      "grad:tensor([-0.1840,  1.0413])\n",
      "params:tensor([  4.2867, -11.1855])\n",
      "Epoch 620, Loss 6.207470\n",
      "grad:tensor([-0.1836,  1.0395])\n",
      "params:tensor([  4.2886, -11.1959])\n",
      "Epoch 621, Loss 6.196334\n",
      "grad:tensor([-0.1833,  1.0378])\n",
      "params:tensor([  4.2904, -11.2063])\n",
      "Epoch 622, Loss 6.185240\n",
      "grad:tensor([-0.1830,  1.0360])\n",
      "params:tensor([  4.2922, -11.2166])\n",
      "Epoch 623, Loss 6.174181\n",
      "grad:tensor([-0.1827,  1.0342])\n",
      "params:tensor([  4.2941, -11.2270])\n",
      "Epoch 624, Loss 6.163159\n",
      "grad:tensor([-0.1824,  1.0325])\n",
      "params:tensor([  4.2959, -11.2373])\n",
      "Epoch 625, Loss 6.152177\n",
      "grad:tensor([-0.1821,  1.0307])\n",
      "params:tensor([  4.2977, -11.2476])\n",
      "Epoch 626, Loss 6.141230\n",
      "grad:tensor([-0.1818,  1.0290])\n",
      "params:tensor([  4.2995, -11.2579])\n",
      "Epoch 627, Loss 6.130322\n",
      "grad:tensor([-0.1815,  1.0272])\n",
      "params:tensor([  4.3013, -11.2682])\n",
      "Epoch 628, Loss 6.119448\n",
      "grad:tensor([-0.1811,  1.0255])\n",
      "params:tensor([  4.3031, -11.2784])\n",
      "Epoch 629, Loss 6.108614\n",
      "grad:tensor([-0.1808,  1.0237])\n",
      "params:tensor([  4.3050, -11.2887])\n",
      "Epoch 630, Loss 6.097815\n",
      "grad:tensor([-0.1805,  1.0220])\n",
      "params:tensor([  4.3068, -11.2989])\n",
      "Epoch 631, Loss 6.087054\n",
      "grad:tensor([-0.1802,  1.0203])\n",
      "params:tensor([  4.3086, -11.3091])\n",
      "Epoch 632, Loss 6.076329\n",
      "grad:tensor([-0.1799,  1.0185])\n",
      "params:tensor([  4.3104, -11.3193])\n",
      "Epoch 633, Loss 6.065644\n",
      "grad:tensor([-0.1796,  1.0168])\n",
      "params:tensor([  4.3122, -11.3294])\n",
      "Epoch 634, Loss 6.054988\n",
      "grad:tensor([-0.1793,  1.0151])\n",
      "params:tensor([  4.3139, -11.3396])\n",
      "Epoch 635, Loss 6.044372\n",
      "grad:tensor([-0.1790,  1.0133])\n",
      "params:tensor([  4.3157, -11.3497])\n",
      "Epoch 636, Loss 6.033794\n",
      "grad:tensor([-0.1787,  1.0116])\n",
      "params:tensor([  4.3175, -11.3598])\n",
      "Epoch 637, Loss 6.023247\n",
      "grad:tensor([-0.1784,  1.0099])\n",
      "params:tensor([  4.3193, -11.3699])\n",
      "Epoch 638, Loss 6.012738\n",
      "grad:tensor([-0.1781,  1.0082])\n",
      "params:tensor([  4.3211, -11.3800])\n",
      "Epoch 639, Loss 6.002264\n",
      "grad:tensor([-0.1778,  1.0065])\n",
      "params:tensor([  4.3229, -11.3901])\n",
      "Epoch 640, Loss 5.991828\n",
      "grad:tensor([-0.1775,  1.0048])\n",
      "params:tensor([  4.3246, -11.4001])\n",
      "Epoch 641, Loss 5.981425\n",
      "grad:tensor([-0.1772,  1.0031])\n",
      "params:tensor([  4.3264, -11.4102])\n",
      "Epoch 642, Loss 5.971058\n",
      "grad:tensor([-0.1769,  1.0014])\n",
      "params:tensor([  4.3282, -11.4202])\n",
      "Epoch 643, Loss 5.960727\n",
      "grad:tensor([-0.1766,  0.9997])\n",
      "params:tensor([  4.3300, -11.4302])\n",
      "Epoch 644, Loss 5.950432\n",
      "grad:tensor([-0.1763,  0.9980])\n",
      "params:tensor([  4.3317, -11.4401])\n",
      "Epoch 645, Loss 5.940171\n",
      "grad:tensor([-0.1760,  0.9963])\n",
      "params:tensor([  4.3335, -11.4501])\n",
      "Epoch 646, Loss 5.929944\n",
      "grad:tensor([-0.1757,  0.9946])\n",
      "params:tensor([  4.3352, -11.4601])\n",
      "Epoch 647, Loss 5.919752\n",
      "grad:tensor([-0.1754,  0.9929])\n",
      "params:tensor([  4.3370, -11.4700])\n",
      "Epoch 648, Loss 5.909596\n",
      "grad:tensor([-0.1751,  0.9912])\n",
      "params:tensor([  4.3387, -11.4799])\n",
      "Epoch 649, Loss 5.899472\n",
      "grad:tensor([-0.1748,  0.9895])\n",
      "params:tensor([  4.3405, -11.4898])\n",
      "Epoch 650, Loss 5.889383\n",
      "grad:tensor([-0.1745,  0.9878])\n",
      "params:tensor([  4.3422, -11.4997])\n",
      "Epoch 651, Loss 5.879326\n",
      "grad:tensor([-0.1742,  0.9862])\n",
      "params:tensor([  4.3440, -11.5095])\n",
      "Epoch 652, Loss 5.869310\n",
      "grad:tensor([-0.1739,  0.9845])\n",
      "params:tensor([  4.3457, -11.5194])\n",
      "Epoch 653, Loss 5.859322\n",
      "grad:tensor([-0.1736,  0.9828])\n",
      "params:tensor([  4.3474, -11.5292])\n",
      "Epoch 654, Loss 5.849374\n",
      "grad:tensor([-0.1733,  0.9811])\n",
      "params:tensor([  4.3492, -11.5390])\n",
      "Epoch 655, Loss 5.839453\n",
      "grad:tensor([-0.1730,  0.9795])\n",
      "params:tensor([  4.3509, -11.5488])\n",
      "Epoch 656, Loss 5.829570\n",
      "grad:tensor([-0.1727,  0.9778])\n",
      "params:tensor([  4.3526, -11.5586])\n",
      "Epoch 657, Loss 5.819718\n",
      "grad:tensor([-0.1724,  0.9761])\n",
      "params:tensor([  4.3544, -11.5683])\n",
      "Epoch 658, Loss 5.809901\n",
      "grad:tensor([-0.1722,  0.9745])\n",
      "params:tensor([  4.3561, -11.5781])\n",
      "Epoch 659, Loss 5.800116\n",
      "grad:tensor([-0.1719,  0.9728])\n",
      "params:tensor([  4.3578, -11.5878])\n",
      "Epoch 660, Loss 5.790367\n",
      "grad:tensor([-0.1716,  0.9712])\n",
      "params:tensor([  4.3595, -11.5975])\n",
      "Epoch 661, Loss 5.780646\n",
      "grad:tensor([-0.1713,  0.9695])\n",
      "params:tensor([  4.3612, -11.6072])\n",
      "Epoch 662, Loss 5.770962\n",
      "grad:tensor([-0.1710,  0.9679])\n",
      "params:tensor([  4.3629, -11.6169])\n",
      "Epoch 663, Loss 5.761312\n",
      "grad:tensor([-0.1707,  0.9662])\n",
      "params:tensor([  4.3646, -11.6266])\n",
      "Epoch 664, Loss 5.751694\n",
      "grad:tensor([-0.1704,  0.9646])\n",
      "params:tensor([  4.3664, -11.6362])\n",
      "Epoch 665, Loss 5.742105\n",
      "grad:tensor([-0.1701,  0.9630])\n",
      "params:tensor([  4.3681, -11.6458])\n",
      "Epoch 666, Loss 5.732550\n",
      "grad:tensor([-0.1698,  0.9613])\n",
      "params:tensor([  4.3697, -11.6555])\n",
      "Epoch 667, Loss 5.723031\n",
      "grad:tensor([-0.1695,  0.9597])\n",
      "params:tensor([  4.3714, -11.6651])\n",
      "Epoch 668, Loss 5.713540\n",
      "grad:tensor([-0.1692,  0.9581])\n",
      "params:tensor([  4.3731, -11.6746])\n",
      "Epoch 669, Loss 5.704083\n",
      "grad:tensor([-0.1690,  0.9564])\n",
      "params:tensor([  4.3748, -11.6842])\n",
      "Epoch 670, Loss 5.694659\n",
      "grad:tensor([-0.1687,  0.9548])\n",
      "params:tensor([  4.3765, -11.6937])\n",
      "Epoch 671, Loss 5.685266\n",
      "grad:tensor([-0.1684,  0.9532])\n",
      "params:tensor([  4.3782, -11.7033])\n",
      "Epoch 672, Loss 5.675904\n",
      "grad:tensor([-0.1681,  0.9516])\n",
      "params:tensor([  4.3799, -11.7128])\n",
      "Epoch 673, Loss 5.666573\n",
      "grad:tensor([-0.1678,  0.9499])\n",
      "params:tensor([  4.3816, -11.7223])\n",
      "Epoch 674, Loss 5.657277\n",
      "grad:tensor([-0.1675,  0.9483])\n",
      "params:tensor([  4.3832, -11.7318])\n",
      "Epoch 675, Loss 5.648010\n",
      "grad:tensor([-0.1673,  0.9467])\n",
      "params:tensor([  4.3849, -11.7412])\n",
      "Epoch 676, Loss 5.638776\n",
      "grad:tensor([-0.1670,  0.9451])\n",
      "params:tensor([  4.3866, -11.7507])\n",
      "Epoch 677, Loss 5.629574\n",
      "grad:tensor([-0.1667,  0.9435])\n",
      "params:tensor([  4.3882, -11.7601])\n",
      "Epoch 678, Loss 5.620402\n",
      "grad:tensor([-0.1664,  0.9419])\n",
      "params:tensor([  4.3899, -11.7696])\n",
      "Epoch 679, Loss 5.611260\n",
      "grad:tensor([-0.1661,  0.9403])\n",
      "params:tensor([  4.3916, -11.7790])\n",
      "Epoch 680, Loss 5.602149\n",
      "grad:tensor([-0.1658,  0.9387])\n",
      "params:tensor([  4.3932, -11.7883])\n",
      "Epoch 681, Loss 5.593071\n",
      "grad:tensor([-0.1656,  0.9371])\n",
      "params:tensor([  4.3949, -11.7977])\n",
      "Epoch 682, Loss 5.584022\n",
      "grad:tensor([-0.1653,  0.9355])\n",
      "params:tensor([  4.3965, -11.8071])\n",
      "Epoch 683, Loss 5.575005\n",
      "grad:tensor([-0.1650,  0.9339])\n",
      "params:tensor([  4.3982, -11.8164])\n",
      "Epoch 684, Loss 5.566019\n",
      "grad:tensor([-0.1647,  0.9323])\n",
      "params:tensor([  4.3998, -11.8257])\n",
      "Epoch 685, Loss 5.557063\n",
      "grad:tensor([-0.1644,  0.9308])\n",
      "params:tensor([  4.4015, -11.8350])\n",
      "Epoch 686, Loss 5.548136\n",
      "grad:tensor([-0.1641,  0.9292])\n",
      "params:tensor([  4.4031, -11.8443])\n",
      "Epoch 687, Loss 5.539241\n",
      "grad:tensor([-0.1639,  0.9276])\n",
      "params:tensor([  4.4048, -11.8536])\n",
      "Epoch 688, Loss 5.530376\n",
      "grad:tensor([-0.1636,  0.9260])\n",
      "params:tensor([  4.4064, -11.8629])\n",
      "Epoch 689, Loss 5.521540\n",
      "grad:tensor([-0.1633,  0.9245])\n",
      "params:tensor([  4.4080, -11.8721])\n",
      "Epoch 690, Loss 5.512734\n",
      "grad:tensor([-0.1630,  0.9229])\n",
      "params:tensor([  4.4097, -11.8813])\n",
      "Epoch 691, Loss 5.503958\n",
      "grad:tensor([-0.1628,  0.9213])\n",
      "params:tensor([  4.4113, -11.8906])\n",
      "Epoch 692, Loss 5.495212\n",
      "grad:tensor([-0.1625,  0.9197])\n",
      "params:tensor([  4.4129, -11.8998])\n",
      "Epoch 693, Loss 5.486496\n",
      "grad:tensor([-0.1622,  0.9182])\n",
      "params:tensor([  4.4145, -11.9089])\n",
      "Epoch 694, Loss 5.477808\n",
      "grad:tensor([-0.1619,  0.9166])\n",
      "params:tensor([  4.4161, -11.9181])\n",
      "Epoch 695, Loss 5.469152\n",
      "grad:tensor([-0.1617,  0.9151])\n",
      "params:tensor([  4.4178, -11.9272])\n",
      "Epoch 696, Loss 5.460525\n",
      "grad:tensor([-0.1614,  0.9135])\n",
      "params:tensor([  4.4194, -11.9364])\n",
      "Epoch 697, Loss 5.451928\n",
      "grad:tensor([-0.1611,  0.9120])\n",
      "params:tensor([  4.4210, -11.9455])\n",
      "Epoch 698, Loss 5.443358\n",
      "grad:tensor([-0.1608,  0.9104])\n",
      "params:tensor([  4.4226, -11.9546])\n",
      "Epoch 699, Loss 5.434819\n",
      "grad:tensor([-0.1605,  0.9089])\n",
      "params:tensor([  4.4242, -11.9637])\n",
      "Epoch 700, Loss 5.426309\n",
      "grad:tensor([-0.1603,  0.9073])\n",
      "params:tensor([  4.4258, -11.9728])\n",
      "Epoch 701, Loss 5.417827\n",
      "grad:tensor([-0.1600,  0.9058])\n",
      "params:tensor([  4.4274, -11.9818])\n",
      "Epoch 702, Loss 5.409372\n",
      "grad:tensor([-0.1597,  0.9042])\n",
      "params:tensor([  4.4290, -11.9909])\n",
      "Epoch 703, Loss 5.400949\n",
      "grad:tensor([-0.1595,  0.9027])\n",
      "params:tensor([  4.4306, -11.9999])\n",
      "Epoch 704, Loss 5.392550\n",
      "grad:tensor([-0.1592,  0.9012])\n",
      "params:tensor([  4.4322, -12.0089])\n",
      "Epoch 705, Loss 5.384184\n",
      "grad:tensor([-0.1589,  0.8996])\n",
      "params:tensor([  4.4338, -12.0179])\n",
      "Epoch 706, Loss 5.375846\n",
      "grad:tensor([-0.1586,  0.8981])\n",
      "params:tensor([  4.4354, -12.0269])\n",
      "Epoch 707, Loss 5.367537\n",
      "grad:tensor([-0.1584,  0.8966])\n",
      "params:tensor([  4.4369, -12.0359])\n",
      "Epoch 708, Loss 5.359253\n",
      "grad:tensor([-0.1581,  0.8951])\n",
      "params:tensor([  4.4385, -12.0448])\n",
      "Epoch 709, Loss 5.350999\n",
      "grad:tensor([-0.1578,  0.8935])\n",
      "params:tensor([  4.4401, -12.0537])\n",
      "Epoch 710, Loss 5.342772\n",
      "grad:tensor([-0.1576,  0.8920])\n",
      "params:tensor([  4.4417, -12.0627])\n",
      "Epoch 711, Loss 5.334575\n",
      "grad:tensor([-0.1573,  0.8905])\n",
      "params:tensor([  4.4433, -12.0716])\n",
      "Epoch 712, Loss 5.326402\n",
      "grad:tensor([-0.1570,  0.8890])\n",
      "params:tensor([  4.4448, -12.0805])\n",
      "Epoch 713, Loss 5.318260\n",
      "grad:tensor([-0.1568,  0.8875])\n",
      "params:tensor([  4.4464, -12.0893])\n",
      "Epoch 714, Loss 5.310144\n",
      "grad:tensor([-0.1565,  0.8860])\n",
      "params:tensor([  4.4480, -12.0982])\n",
      "Epoch 715, Loss 5.302055\n",
      "grad:tensor([-0.1562,  0.8845])\n",
      "params:tensor([  4.4495, -12.1070])\n",
      "Epoch 716, Loss 5.293994\n",
      "grad:tensor([-0.1560,  0.8830])\n",
      "params:tensor([  4.4511, -12.1159])\n",
      "Epoch 717, Loss 5.285964\n",
      "grad:tensor([-0.1557,  0.8815])\n",
      "params:tensor([  4.4526, -12.1247])\n",
      "Epoch 718, Loss 5.277958\n",
      "grad:tensor([-0.1555,  0.8800])\n",
      "params:tensor([  4.4542, -12.1335])\n",
      "Epoch 719, Loss 5.269979\n",
      "grad:tensor([-0.1552,  0.8785])\n",
      "params:tensor([  4.4557, -12.1423])\n",
      "Epoch 720, Loss 5.262027\n",
      "grad:tensor([-0.1549,  0.8770])\n",
      "params:tensor([  4.4573, -12.1510])\n",
      "Epoch 721, Loss 5.254103\n",
      "grad:tensor([-0.1547,  0.8755])\n",
      "params:tensor([  4.4588, -12.1598])\n",
      "Epoch 722, Loss 5.246205\n",
      "grad:tensor([-0.1544,  0.8740])\n",
      "params:tensor([  4.4604, -12.1685])\n",
      "Epoch 723, Loss 5.238335\n",
      "grad:tensor([-0.1541,  0.8725])\n",
      "params:tensor([  4.4619, -12.1773])\n",
      "Epoch 724, Loss 5.230492\n",
      "grad:tensor([-0.1539,  0.8710])\n",
      "params:tensor([  4.4635, -12.1860])\n",
      "Epoch 725, Loss 5.222674\n",
      "grad:tensor([-0.1536,  0.8696])\n",
      "params:tensor([  4.4650, -12.1947])\n",
      "Epoch 726, Loss 5.214881\n",
      "grad:tensor([-0.1533,  0.8681])\n",
      "params:tensor([  4.4665, -12.2033])\n",
      "Epoch 727, Loss 5.207120\n",
      "grad:tensor([-0.1531,  0.8666])\n",
      "params:tensor([  4.4681, -12.2120])\n",
      "Epoch 728, Loss 5.199381\n",
      "grad:tensor([-0.1528,  0.8651])\n",
      "params:tensor([  4.4696, -12.2207])\n",
      "Epoch 729, Loss 5.191670\n",
      "grad:tensor([-0.1526,  0.8637])\n",
      "params:tensor([  4.4711, -12.2293])\n",
      "Epoch 730, Loss 5.183985\n",
      "grad:tensor([-0.1523,  0.8622])\n",
      "params:tensor([  4.4726, -12.2379])\n",
      "Epoch 731, Loss 5.176324\n",
      "grad:tensor([-0.1520,  0.8607])\n",
      "params:tensor([  4.4742, -12.2465])\n",
      "Epoch 732, Loss 5.168688\n",
      "grad:tensor([-0.1518,  0.8593])\n",
      "params:tensor([  4.4757, -12.2551])\n",
      "Epoch 733, Loss 5.161084\n",
      "grad:tensor([-0.1515,  0.8578])\n",
      "params:tensor([  4.4772, -12.2637])\n",
      "Epoch 734, Loss 5.153500\n",
      "grad:tensor([-0.1513,  0.8564])\n",
      "params:tensor([  4.4787, -12.2723])\n",
      "Epoch 735, Loss 5.145944\n",
      "grad:tensor([-0.1510,  0.8549])\n",
      "params:tensor([  4.4802, -12.2808])\n",
      "Epoch 736, Loss 5.138413\n",
      "grad:tensor([-0.1508,  0.8535])\n",
      "params:tensor([  4.4817, -12.2893])\n",
      "Epoch 737, Loss 5.130910\n",
      "grad:tensor([-0.1505,  0.8520])\n",
      "params:tensor([  4.4832, -12.2979])\n",
      "Epoch 738, Loss 5.123428\n",
      "grad:tensor([-0.1502,  0.8506])\n",
      "params:tensor([  4.4847, -12.3064])\n",
      "Epoch 739, Loss 5.115978\n",
      "grad:tensor([-0.1500,  0.8491])\n",
      "params:tensor([  4.4862, -12.3149])\n",
      "Epoch 740, Loss 5.108547\n",
      "grad:tensor([-0.1497,  0.8477])\n",
      "params:tensor([  4.4877, -12.3233])\n",
      "Epoch 741, Loss 5.101143\n",
      "grad:tensor([-0.1495,  0.8462])\n",
      "params:tensor([  4.4892, -12.3318])\n",
      "Epoch 742, Loss 5.093765\n",
      "grad:tensor([-0.1492,  0.8448])\n",
      "params:tensor([  4.4907, -12.3402])\n",
      "Epoch 743, Loss 5.086414\n",
      "grad:tensor([-0.1490,  0.8434])\n",
      "params:tensor([  4.4922, -12.3487])\n",
      "Epoch 744, Loss 5.079086\n",
      "grad:tensor([-0.1487,  0.8419])\n",
      "params:tensor([  4.4937, -12.3571])\n",
      "Epoch 745, Loss 5.071781\n",
      "grad:tensor([-0.1485,  0.8405])\n",
      "params:tensor([  4.4952, -12.3655])\n",
      "Epoch 746, Loss 5.064505\n",
      "grad:tensor([-0.1482,  0.8391])\n",
      "params:tensor([  4.4967, -12.3739])\n",
      "Epoch 747, Loss 5.057247\n",
      "grad:tensor([-0.1480,  0.8376])\n",
      "params:tensor([  4.4981, -12.3823])\n",
      "Epoch 748, Loss 5.050021\n",
      "grad:tensor([-0.1477,  0.8362])\n",
      "params:tensor([  4.4996, -12.3906])\n",
      "Epoch 749, Loss 5.042817\n",
      "grad:tensor([-0.1475,  0.8348])\n",
      "params:tensor([  4.5011, -12.3990])\n",
      "Epoch 750, Loss 5.035636\n",
      "grad:tensor([-0.1472,  0.8334])\n",
      "params:tensor([  4.5026, -12.4073])\n",
      "Epoch 751, Loss 5.028476\n",
      "grad:tensor([-0.1470,  0.8320])\n",
      "params:tensor([  4.5040, -12.4156])\n",
      "Epoch 752, Loss 5.021346\n",
      "grad:tensor([-0.1467,  0.8305])\n",
      "params:tensor([  4.5055, -12.4239])\n",
      "Epoch 753, Loss 5.014240\n",
      "grad:tensor([-0.1465,  0.8291])\n",
      "params:tensor([  4.5070, -12.4322])\n",
      "Epoch 754, Loss 5.007157\n",
      "grad:tensor([-0.1462,  0.8277])\n",
      "params:tensor([  4.5084, -12.4405])\n",
      "Epoch 755, Loss 5.000099\n",
      "grad:tensor([-0.1460,  0.8263])\n",
      "params:tensor([  4.5099, -12.4488])\n",
      "Epoch 756, Loss 4.993064\n",
      "grad:tensor([-0.1457,  0.8249])\n",
      "params:tensor([  4.5113, -12.4570])\n",
      "Epoch 757, Loss 4.986051\n",
      "grad:tensor([-0.1455,  0.8235])\n",
      "params:tensor([  4.5128, -12.4653])\n",
      "Epoch 758, Loss 4.979064\n",
      "grad:tensor([-0.1452,  0.8221])\n",
      "params:tensor([  4.5143, -12.4735])\n",
      "Epoch 759, Loss 4.972100\n",
      "grad:tensor([-0.1450,  0.8207])\n",
      "params:tensor([  4.5157, -12.4817])\n",
      "Epoch 760, Loss 4.965159\n",
      "grad:tensor([-0.1447,  0.8193])\n",
      "params:tensor([  4.5172, -12.4899])\n",
      "Epoch 761, Loss 4.958245\n",
      "grad:tensor([-0.1445,  0.8179])\n",
      "params:tensor([  4.5186, -12.4981])\n",
      "Epoch 762, Loss 4.951351\n",
      "grad:tensor([-0.1443,  0.8165])\n",
      "params:tensor([  4.5200, -12.5062])\n",
      "Epoch 763, Loss 4.944479\n",
      "grad:tensor([-0.1440,  0.8152])\n",
      "params:tensor([  4.5215, -12.5144])\n",
      "Epoch 764, Loss 4.937633\n",
      "grad:tensor([-0.1438,  0.8138])\n",
      "params:tensor([  4.5229, -12.5225])\n",
      "Epoch 765, Loss 4.930812\n",
      "grad:tensor([-0.1435,  0.8124])\n",
      "params:tensor([  4.5244, -12.5306])\n",
      "Epoch 766, Loss 4.924009\n",
      "grad:tensor([-0.1433,  0.8110])\n",
      "params:tensor([  4.5258, -12.5387])\n",
      "Epoch 767, Loss 4.917234\n",
      "grad:tensor([-0.1430,  0.8096])\n",
      "params:tensor([  4.5272, -12.5468])\n",
      "Epoch 768, Loss 4.910480\n",
      "grad:tensor([-0.1428,  0.8083])\n",
      "params:tensor([  4.5286, -12.5549])\n",
      "Epoch 769, Loss 4.903749\n",
      "grad:tensor([-0.1426,  0.8069])\n",
      "params:tensor([  4.5301, -12.5630])\n",
      "Epoch 770, Loss 4.897040\n",
      "grad:tensor([-0.1423,  0.8055])\n",
      "params:tensor([  4.5315, -12.5711])\n",
      "Epoch 771, Loss 4.890356\n",
      "grad:tensor([-0.1420,  0.8042])\n",
      "params:tensor([  4.5329, -12.5791])\n",
      "Epoch 772, Loss 4.883692\n",
      "grad:tensor([-0.1418,  0.8028])\n",
      "params:tensor([  4.5343, -12.5871])\n",
      "Epoch 773, Loss 4.877052\n",
      "grad:tensor([-0.1416,  0.8014])\n",
      "params:tensor([  4.5357, -12.5951])\n",
      "Epoch 774, Loss 4.870436\n",
      "grad:tensor([-0.1413,  0.8001])\n",
      "params:tensor([  4.5372, -12.6031])\n",
      "Epoch 775, Loss 4.863839\n",
      "grad:tensor([-0.1411,  0.7987])\n",
      "params:tensor([  4.5386, -12.6111])\n",
      "Epoch 776, Loss 4.857268\n",
      "grad:tensor([-0.1408,  0.7973])\n",
      "params:tensor([  4.5400, -12.6191])\n",
      "Epoch 777, Loss 4.850718\n",
      "grad:tensor([-0.1406,  0.7960])\n",
      "params:tensor([  4.5414, -12.6271])\n",
      "Epoch 778, Loss 4.844189\n",
      "grad:tensor([-0.1404,  0.7946])\n",
      "params:tensor([  4.5428, -12.6350])\n",
      "Epoch 779, Loss 4.837683\n",
      "grad:tensor([-0.1401,  0.7933])\n",
      "params:tensor([  4.5442, -12.6429])\n",
      "Epoch 780, Loss 4.831196\n",
      "grad:tensor([-0.1399,  0.7919])\n",
      "params:tensor([  4.5456, -12.6509])\n",
      "Epoch 781, Loss 4.824737\n",
      "grad:tensor([-0.1397,  0.7906])\n",
      "params:tensor([  4.5470, -12.6588])\n",
      "Epoch 782, Loss 4.818298\n",
      "grad:tensor([-0.1394,  0.7893])\n",
      "params:tensor([  4.5484, -12.6667])\n",
      "Epoch 783, Loss 4.811879\n",
      "grad:tensor([-0.1392,  0.7879])\n",
      "params:tensor([  4.5498, -12.6745])\n",
      "Epoch 784, Loss 4.805481\n",
      "grad:tensor([-0.1389,  0.7866])\n",
      "params:tensor([  4.5512, -12.6824])\n",
      "Epoch 785, Loss 4.799106\n",
      "grad:tensor([-0.1387,  0.7852])\n",
      "params:tensor([  4.5525, -12.6902])\n",
      "Epoch 786, Loss 4.792755\n",
      "grad:tensor([-0.1385,  0.7839])\n",
      "params:tensor([  4.5539, -12.6981])\n",
      "Epoch 787, Loss 4.786422\n",
      "grad:tensor([-0.1383,  0.7826])\n",
      "params:tensor([  4.5553, -12.7059])\n",
      "Epoch 788, Loss 4.780112\n",
      "grad:tensor([-0.1380,  0.7812])\n",
      "params:tensor([  4.5567, -12.7137])\n",
      "Epoch 789, Loss 4.773824\n",
      "grad:tensor([-0.1378,  0.7799])\n",
      "params:tensor([  4.5581, -12.7215])\n",
      "Epoch 790, Loss 4.767558\n",
      "grad:tensor([-0.1375,  0.7786])\n",
      "params:tensor([  4.5594, -12.7293])\n",
      "Epoch 791, Loss 4.761312\n",
      "grad:tensor([-0.1373,  0.7773])\n",
      "params:tensor([  4.5608, -12.7371])\n",
      "Epoch 792, Loss 4.755087\n",
      "grad:tensor([-0.1371,  0.7759])\n",
      "params:tensor([  4.5622, -12.7448])\n",
      "Epoch 793, Loss 4.748885\n",
      "grad:tensor([-0.1368,  0.7746])\n",
      "params:tensor([  4.5636, -12.7526])\n",
      "Epoch 794, Loss 4.742700\n",
      "grad:tensor([-0.1366,  0.7733])\n",
      "params:tensor([  4.5649, -12.7603])\n",
      "Epoch 795, Loss 4.736537\n",
      "grad:tensor([-0.1364,  0.7720])\n",
      "params:tensor([  4.5663, -12.7680])\n",
      "Epoch 796, Loss 4.730397\n",
      "grad:tensor([-0.1361,  0.7707])\n",
      "params:tensor([  4.5677, -12.7758])\n",
      "Epoch 797, Loss 4.724279\n",
      "grad:tensor([-0.1359,  0.7694])\n",
      "params:tensor([  4.5690, -12.7834])\n",
      "Epoch 798, Loss 4.718181\n",
      "grad:tensor([-0.1357,  0.7681])\n",
      "params:tensor([  4.5704, -12.7911])\n",
      "Epoch 799, Loss 4.712101\n",
      "grad:tensor([-0.1354,  0.7668])\n",
      "params:tensor([  4.5717, -12.7988])\n",
      "Epoch 800, Loss 4.706046\n",
      "grad:tensor([-0.1352,  0.7655])\n",
      "params:tensor([  4.5731, -12.8064])\n",
      "Epoch 801, Loss 4.700009\n",
      "grad:tensor([-0.1350,  0.7642])\n",
      "params:tensor([  4.5744, -12.8141])\n",
      "Epoch 802, Loss 4.693990\n",
      "grad:tensor([-0.1347,  0.7629])\n",
      "params:tensor([  4.5758, -12.8217])\n",
      "Epoch 803, Loss 4.687995\n",
      "grad:tensor([-0.1345,  0.7616])\n",
      "params:tensor([  4.5771, -12.8293])\n",
      "Epoch 804, Loss 4.682020\n",
      "grad:tensor([-0.1343,  0.7603])\n",
      "params:tensor([  4.5785, -12.8369])\n",
      "Epoch 805, Loss 4.676063\n",
      "grad:tensor([-0.1341,  0.7590])\n",
      "params:tensor([  4.5798, -12.8445])\n",
      "Epoch 806, Loss 4.670130\n",
      "grad:tensor([-0.1338,  0.7577])\n",
      "params:tensor([  4.5811, -12.8521])\n",
      "Epoch 807, Loss 4.664214\n",
      "grad:tensor([-0.1336,  0.7564])\n",
      "params:tensor([  4.5825, -12.8597])\n",
      "Epoch 808, Loss 4.658319\n",
      "grad:tensor([-0.1334,  0.7551])\n",
      "params:tensor([  4.5838, -12.8672])\n",
      "Epoch 809, Loss 4.652445\n",
      "grad:tensor([-0.1332,  0.7538])\n",
      "params:tensor([  4.5851, -12.8748])\n",
      "Epoch 810, Loss 4.646592\n",
      "grad:tensor([-0.1330,  0.7526])\n",
      "params:tensor([  4.5865, -12.8823])\n",
      "Epoch 811, Loss 4.640754\n",
      "grad:tensor([-0.1327,  0.7513])\n",
      "params:tensor([  4.5878, -12.8898])\n",
      "Epoch 812, Loss 4.634938\n",
      "grad:tensor([-0.1325,  0.7500])\n",
      "params:tensor([  4.5891, -12.8973])\n",
      "Epoch 813, Loss 4.629142\n",
      "grad:tensor([-0.1323,  0.7487])\n",
      "params:tensor([  4.5904, -12.9048])\n",
      "Epoch 814, Loss 4.623367\n",
      "grad:tensor([-0.1320,  0.7475])\n",
      "params:tensor([  4.5918, -12.9123])\n",
      "Epoch 815, Loss 4.617611\n",
      "grad:tensor([-0.1318,  0.7462])\n",
      "params:tensor([  4.5931, -12.9197])\n",
      "Epoch 816, Loss 4.611872\n",
      "grad:tensor([-0.1316,  0.7449])\n",
      "params:tensor([  4.5944, -12.9272])\n",
      "Epoch 817, Loss 4.606156\n",
      "grad:tensor([-0.1314,  0.7437])\n",
      "params:tensor([  4.5957, -12.9346])\n",
      "Epoch 818, Loss 4.600458\n",
      "grad:tensor([-0.1311,  0.7424])\n",
      "params:tensor([  4.5970, -12.9420])\n",
      "Epoch 819, Loss 4.594780\n",
      "grad:tensor([-0.1309,  0.7411])\n",
      "params:tensor([  4.5983, -12.9494])\n",
      "Epoch 820, Loss 4.589119\n",
      "grad:tensor([-0.1307,  0.7399])\n",
      "params:tensor([  4.5996, -12.9568])\n",
      "Epoch 821, Loss 4.583479\n",
      "grad:tensor([-0.1305,  0.7386])\n",
      "params:tensor([  4.6009, -12.9642])\n",
      "Epoch 822, Loss 4.577857\n",
      "grad:tensor([-0.1303,  0.7374])\n",
      "params:tensor([  4.6022, -12.9716])\n",
      "Epoch 823, Loss 4.572256\n",
      "grad:tensor([-0.1300,  0.7361])\n",
      "params:tensor([  4.6035, -12.9790])\n",
      "Epoch 824, Loss 4.566675\n",
      "grad:tensor([-0.1298,  0.7349])\n",
      "params:tensor([  4.6048, -12.9863])\n",
      "Epoch 825, Loss 4.561108\n",
      "grad:tensor([-0.1296,  0.7336])\n",
      "params:tensor([  4.6061, -12.9936])\n",
      "Epoch 826, Loss 4.555565\n",
      "grad:tensor([-0.1294,  0.7324])\n",
      "params:tensor([  4.6074, -13.0010])\n",
      "Epoch 827, Loss 4.550039\n",
      "grad:tensor([-0.1292,  0.7311])\n",
      "params:tensor([  4.6087, -13.0083])\n",
      "Epoch 828, Loss 4.544534\n",
      "grad:tensor([-0.1289,  0.7299])\n",
      "params:tensor([  4.6100, -13.0156])\n",
      "Epoch 829, Loss 4.539044\n",
      "grad:tensor([-0.1287,  0.7286])\n",
      "params:tensor([  4.6113, -13.0229])\n",
      "Epoch 830, Loss 4.533575\n",
      "grad:tensor([-0.1285,  0.7274])\n",
      "params:tensor([  4.6126, -13.0301])\n",
      "Epoch 831, Loss 4.528122\n",
      "grad:tensor([-0.1283,  0.7262])\n",
      "params:tensor([  4.6139, -13.0374])\n",
      "Epoch 832, Loss 4.522691\n",
      "grad:tensor([-0.1280,  0.7249])\n",
      "params:tensor([  4.6152, -13.0446])\n",
      "Epoch 833, Loss 4.517276\n",
      "grad:tensor([-0.1278,  0.7237])\n",
      "params:tensor([  4.6164, -13.0519])\n",
      "Epoch 834, Loss 4.511879\n",
      "grad:tensor([-0.1276,  0.7225])\n",
      "params:tensor([  4.6177, -13.0591])\n",
      "Epoch 835, Loss 4.506505\n",
      "grad:tensor([-0.1274,  0.7212])\n",
      "params:tensor([  4.6190, -13.0663])\n",
      "Epoch 836, Loss 4.501141\n",
      "grad:tensor([-0.1272,  0.7200])\n",
      "params:tensor([  4.6203, -13.0735])\n",
      "Epoch 837, Loss 4.495801\n",
      "grad:tensor([-0.1270,  0.7188])\n",
      "params:tensor([  4.6215, -13.0807])\n",
      "Epoch 838, Loss 4.490475\n",
      "grad:tensor([-0.1268,  0.7176])\n",
      "params:tensor([  4.6228, -13.0879])\n",
      "Epoch 839, Loss 4.485169\n",
      "grad:tensor([-0.1266,  0.7163])\n",
      "params:tensor([  4.6241, -13.0950])\n",
      "Epoch 840, Loss 4.479884\n",
      "grad:tensor([-0.1263,  0.7151])\n",
      "params:tensor([  4.6253, -13.1022])\n",
      "Epoch 841, Loss 4.474613\n",
      "grad:tensor([-0.1261,  0.7139])\n",
      "params:tensor([  4.6266, -13.1093])\n",
      "Epoch 842, Loss 4.469364\n",
      "grad:tensor([-0.1259,  0.7127])\n",
      "params:tensor([  4.6278, -13.1165])\n",
      "Epoch 843, Loss 4.464130\n",
      "grad:tensor([-0.1257,  0.7115])\n",
      "params:tensor([  4.6291, -13.1236])\n",
      "Epoch 844, Loss 4.458913\n",
      "grad:tensor([-0.1255,  0.7103])\n",
      "params:tensor([  4.6304, -13.1307])\n",
      "Epoch 845, Loss 4.453716\n",
      "grad:tensor([-0.1253,  0.7091])\n",
      "params:tensor([  4.6316, -13.1378])\n",
      "Epoch 846, Loss 4.448535\n",
      "grad:tensor([-0.1250,  0.7079])\n",
      "params:tensor([  4.6329, -13.1449])\n",
      "Epoch 847, Loss 4.443372\n",
      "grad:tensor([-0.1249,  0.7067])\n",
      "params:tensor([  4.6341, -13.1519])\n",
      "Epoch 848, Loss 4.438226\n",
      "grad:tensor([-0.1246,  0.7055])\n",
      "params:tensor([  4.6353, -13.1590])\n",
      "Epoch 849, Loss 4.433099\n",
      "grad:tensor([-0.1244,  0.7043])\n",
      "params:tensor([  4.6366, -13.1660])\n",
      "Epoch 850, Loss 4.427990\n",
      "grad:tensor([-0.1242,  0.7031])\n",
      "params:tensor([  4.6378, -13.1730])\n",
      "Epoch 851, Loss 4.422897\n",
      "grad:tensor([-0.1240,  0.7019])\n",
      "params:tensor([  4.6391, -13.1801])\n",
      "Epoch 852, Loss 4.417819\n",
      "grad:tensor([-0.1238,  0.7007])\n",
      "params:tensor([  4.6403, -13.1871])\n",
      "Epoch 853, Loss 4.412762\n",
      "grad:tensor([-0.1236,  0.6995])\n",
      "params:tensor([  4.6415, -13.1941])\n",
      "Epoch 854, Loss 4.407721\n",
      "grad:tensor([-0.1234,  0.6983])\n",
      "params:tensor([  4.6428, -13.2010])\n",
      "Epoch 855, Loss 4.402698\n",
      "grad:tensor([-0.1232,  0.6971])\n",
      "params:tensor([  4.6440, -13.2080])\n",
      "Epoch 856, Loss 4.397688\n",
      "grad:tensor([-0.1229,  0.6959])\n",
      "params:tensor([  4.6452, -13.2150])\n",
      "Epoch 857, Loss 4.392697\n",
      "grad:tensor([-0.1227,  0.6948])\n",
      "params:tensor([  4.6465, -13.2219])\n",
      "Epoch 858, Loss 4.387725\n",
      "grad:tensor([-0.1225,  0.6936])\n",
      "params:tensor([  4.6477, -13.2289])\n",
      "Epoch 859, Loss 4.382770\n",
      "grad:tensor([-0.1223,  0.6924])\n",
      "params:tensor([  4.6489, -13.2358])\n",
      "Epoch 860, Loss 4.377828\n",
      "grad:tensor([-0.1221,  0.6912])\n",
      "params:tensor([  4.6501, -13.2427])\n",
      "Epoch 861, Loss 4.372905\n",
      "grad:tensor([-0.1219,  0.6901])\n",
      "params:tensor([  4.6514, -13.2496])\n",
      "Epoch 862, Loss 4.368000\n",
      "grad:tensor([-0.1217,  0.6889])\n",
      "params:tensor([  4.6526, -13.2565])\n",
      "Epoch 863, Loss 4.363111\n",
      "grad:tensor([-0.1215,  0.6877])\n",
      "params:tensor([  4.6538, -13.2634])\n",
      "Epoch 864, Loss 4.358238\n",
      "grad:tensor([-0.1213,  0.6865])\n",
      "params:tensor([  4.6550, -13.2702])\n",
      "Epoch 865, Loss 4.353383\n",
      "grad:tensor([-0.1211,  0.6854])\n",
      "params:tensor([  4.6562, -13.2771])\n",
      "Epoch 866, Loss 4.348542\n",
      "grad:tensor([-0.1209,  0.6842])\n",
      "params:tensor([  4.6574, -13.2839])\n",
      "Epoch 867, Loss 4.343716\n",
      "grad:tensor([-0.1207,  0.6830])\n",
      "params:tensor([  4.6586, -13.2908])\n",
      "Epoch 868, Loss 4.338911\n",
      "grad:tensor([-0.1205,  0.6819])\n",
      "params:tensor([  4.6598, -13.2976])\n",
      "Epoch 869, Loss 4.334120\n",
      "grad:tensor([-0.1203,  0.6807])\n",
      "params:tensor([  4.6610, -13.3044])\n",
      "Epoch 870, Loss 4.329345\n",
      "grad:tensor([-0.1201,  0.6796])\n",
      "params:tensor([  4.6622, -13.3112])\n",
      "Epoch 871, Loss 4.324588\n",
      "grad:tensor([-0.1198,  0.6784])\n",
      "params:tensor([  4.6634, -13.3180])\n",
      "Epoch 872, Loss 4.319846\n",
      "grad:tensor([-0.1196,  0.6773])\n",
      "params:tensor([  4.6646, -13.3247])\n",
      "Epoch 873, Loss 4.315117\n",
      "grad:tensor([-0.1195,  0.6761])\n",
      "params:tensor([  4.6658, -13.3315])\n",
      "Epoch 874, Loss 4.310409\n",
      "grad:tensor([-0.1192,  0.6750])\n",
      "params:tensor([  4.6670, -13.3382])\n",
      "Epoch 875, Loss 4.305714\n",
      "grad:tensor([-0.1190,  0.6738])\n",
      "params:tensor([  4.6682, -13.3450])\n",
      "Epoch 876, Loss 4.301036\n",
      "grad:tensor([-0.1188,  0.6727])\n",
      "params:tensor([  4.6694, -13.3517])\n",
      "Epoch 877, Loss 4.296376\n",
      "grad:tensor([-0.1186,  0.6715])\n",
      "params:tensor([  4.6706, -13.3584])\n",
      "Epoch 878, Loss 4.291727\n",
      "grad:tensor([-0.1184,  0.6704])\n",
      "params:tensor([  4.6718, -13.3651])\n",
      "Epoch 879, Loss 4.287098\n",
      "grad:tensor([-0.1182,  0.6693])\n",
      "params:tensor([  4.6730, -13.3718])\n",
      "Epoch 880, Loss 4.282482\n",
      "grad:tensor([-0.1180,  0.6681])\n",
      "params:tensor([  4.6741, -13.3785])\n",
      "Epoch 881, Loss 4.277882\n",
      "grad:tensor([-0.1178,  0.6670])\n",
      "params:tensor([  4.6753, -13.3852])\n",
      "Epoch 882, Loss 4.273299\n",
      "grad:tensor([-0.1176,  0.6658])\n",
      "params:tensor([  4.6765, -13.3918])\n",
      "Epoch 883, Loss 4.268732\n",
      "grad:tensor([-0.1174,  0.6647])\n",
      "params:tensor([  4.6777, -13.3985])\n",
      "Epoch 884, Loss 4.264178\n",
      "grad:tensor([-0.1172,  0.6636])\n",
      "params:tensor([  4.6788, -13.4051])\n",
      "Epoch 885, Loss 4.259643\n",
      "grad:tensor([-0.1170,  0.6625])\n",
      "params:tensor([  4.6800, -13.4117])\n",
      "Epoch 886, Loss 4.255120\n",
      "grad:tensor([-0.1168,  0.6613])\n",
      "params:tensor([  4.6812, -13.4184])\n",
      "Epoch 887, Loss 4.250614\n",
      "grad:tensor([-0.1166,  0.6602])\n",
      "params:tensor([  4.6823, -13.4250])\n",
      "Epoch 888, Loss 4.246124\n",
      "grad:tensor([-0.1164,  0.6591])\n",
      "params:tensor([  4.6835, -13.4316])\n",
      "Epoch 889, Loss 4.241648\n",
      "grad:tensor([-0.1162,  0.6580])\n",
      "params:tensor([  4.6847, -13.4381])\n",
      "Epoch 890, Loss 4.237185\n",
      "grad:tensor([-0.1160,  0.6569])\n",
      "params:tensor([  4.6858, -13.4447])\n",
      "Epoch 891, Loss 4.232740\n",
      "grad:tensor([-0.1158,  0.6557])\n",
      "params:tensor([  4.6870, -13.4513])\n",
      "Epoch 892, Loss 4.228308\n",
      "grad:tensor([-0.1157,  0.6546])\n",
      "params:tensor([  4.6881, -13.4578])\n",
      "Epoch 893, Loss 4.223895\n",
      "grad:tensor([-0.1154,  0.6535])\n",
      "params:tensor([  4.6893, -13.4643])\n",
      "Epoch 894, Loss 4.219494\n",
      "grad:tensor([-0.1153,  0.6524])\n",
      "params:tensor([  4.6904, -13.4709])\n",
      "Epoch 895, Loss 4.215109\n",
      "grad:tensor([-0.1151,  0.6513])\n",
      "params:tensor([  4.6916, -13.4774])\n",
      "Epoch 896, Loss 4.210737\n",
      "grad:tensor([-0.1148,  0.6502])\n",
      "params:tensor([  4.6927, -13.4839])\n",
      "Epoch 897, Loss 4.206383\n",
      "grad:tensor([-0.1147,  0.6491])\n",
      "params:tensor([  4.6939, -13.4904])\n",
      "Epoch 898, Loss 4.202043\n",
      "grad:tensor([-0.1145,  0.6480])\n",
      "params:tensor([  4.6950, -13.4968])\n",
      "Epoch 899, Loss 4.197715\n",
      "grad:tensor([-0.1143,  0.6469])\n",
      "params:tensor([  4.6962, -13.5033])\n",
      "Epoch 900, Loss 4.193405\n",
      "grad:tensor([-0.1141,  0.6458])\n",
      "params:tensor([  4.6973, -13.5098])\n",
      "Epoch 901, Loss 4.189108\n",
      "grad:tensor([-0.1139,  0.6447])\n",
      "params:tensor([  4.6985, -13.5162])\n",
      "Epoch 902, Loss 4.184825\n",
      "grad:tensor([-0.1137,  0.6436])\n",
      "params:tensor([  4.6996, -13.5227])\n",
      "Epoch 903, Loss 4.180559\n",
      "grad:tensor([-0.1135,  0.6425])\n",
      "params:tensor([  4.7007, -13.5291])\n",
      "Epoch 904, Loss 4.176305\n",
      "grad:tensor([-0.1133,  0.6414])\n",
      "params:tensor([  4.7019, -13.5355])\n",
      "Epoch 905, Loss 4.172065\n",
      "grad:tensor([-0.1131,  0.6403])\n",
      "params:tensor([  4.7030, -13.5419])\n",
      "Epoch 906, Loss 4.167842\n",
      "grad:tensor([-0.1129,  0.6392])\n",
      "params:tensor([  4.7041, -13.5483])\n",
      "Epoch 907, Loss 4.163630\n",
      "grad:tensor([-0.1127,  0.6381])\n",
      "params:tensor([  4.7053, -13.5547])\n",
      "Epoch 908, Loss 4.159436\n",
      "grad:tensor([-0.1125,  0.6371])\n",
      "params:tensor([  4.7064, -13.5610])\n",
      "Epoch 909, Loss 4.155253\n",
      "grad:tensor([-0.1124,  0.6360])\n",
      "params:tensor([  4.7075, -13.5674])\n",
      "Epoch 910, Loss 4.151086\n",
      "grad:tensor([-0.1122,  0.6349])\n",
      "params:tensor([  4.7086, -13.5738])\n",
      "Epoch 911, Loss 4.146934\n",
      "grad:tensor([-0.1120,  0.6338])\n",
      "params:tensor([  4.7097, -13.5801])\n",
      "Epoch 912, Loss 4.142794\n",
      "grad:tensor([-0.1118,  0.6327])\n",
      "params:tensor([  4.7109, -13.5864])\n",
      "Epoch 913, Loss 4.138669\n",
      "grad:tensor([-0.1116,  0.6317])\n",
      "params:tensor([  4.7120, -13.5927])\n",
      "Epoch 914, Loss 4.134559\n",
      "grad:tensor([-0.1114,  0.6306])\n",
      "params:tensor([  4.7131, -13.5990])\n",
      "Epoch 915, Loss 4.130465\n",
      "grad:tensor([-0.1112,  0.6295])\n",
      "params:tensor([  4.7142, -13.6053])\n",
      "Epoch 916, Loss 4.126378\n",
      "grad:tensor([-0.1110,  0.6284])\n",
      "params:tensor([  4.7153, -13.6116])\n",
      "Epoch 917, Loss 4.122310\n",
      "grad:tensor([-0.1108,  0.6274])\n",
      "params:tensor([  4.7164, -13.6179])\n",
      "Epoch 918, Loss 4.118253\n",
      "grad:tensor([-0.1107,  0.6263])\n",
      "params:tensor([  4.7175, -13.6242])\n",
      "Epoch 919, Loss 4.114213\n",
      "grad:tensor([-0.1104,  0.6253])\n",
      "params:tensor([  4.7186, -13.6304])\n",
      "Epoch 920, Loss 4.110184\n",
      "grad:tensor([-0.1103,  0.6242])\n",
      "params:tensor([  4.7197, -13.6367])\n",
      "Epoch 921, Loss 4.106170\n",
      "grad:tensor([-0.1101,  0.6231])\n",
      "params:tensor([  4.7208, -13.6429])\n",
      "Epoch 922, Loss 4.102171\n",
      "grad:tensor([-0.1099,  0.6221])\n",
      "params:tensor([  4.7219, -13.6491])\n",
      "Epoch 923, Loss 4.098181\n",
      "grad:tensor([-0.1097,  0.6210])\n",
      "params:tensor([  4.7230, -13.6553])\n",
      "Epoch 924, Loss 4.094209\n",
      "grad:tensor([-0.1095,  0.6200])\n",
      "params:tensor([  4.7241, -13.6615])\n",
      "Epoch 925, Loss 4.090250\n",
      "grad:tensor([-0.1093,  0.6189])\n",
      "params:tensor([  4.7252, -13.6677])\n",
      "Epoch 926, Loss 4.086300\n",
      "grad:tensor([-0.1091,  0.6179])\n",
      "params:tensor([  4.7263, -13.6739])\n",
      "Epoch 927, Loss 4.082366\n",
      "grad:tensor([-0.1090,  0.6168])\n",
      "params:tensor([  4.7274, -13.6800])\n",
      "Epoch 928, Loss 4.078448\n",
      "grad:tensor([-0.1088,  0.6158])\n",
      "params:tensor([  4.7285, -13.6862])\n",
      "Epoch 929, Loss 4.074540\n",
      "grad:tensor([-0.1086,  0.6147])\n",
      "params:tensor([  4.7296, -13.6924])\n",
      "Epoch 930, Loss 4.070650\n",
      "grad:tensor([-0.1084,  0.6137])\n",
      "params:tensor([  4.7307, -13.6985])\n",
      "Epoch 931, Loss 4.066769\n",
      "grad:tensor([-0.1082,  0.6126])\n",
      "params:tensor([  4.7317, -13.7046])\n",
      "Epoch 932, Loss 4.062900\n",
      "grad:tensor([-0.1080,  0.6116])\n",
      "params:tensor([  4.7328, -13.7107])\n",
      "Epoch 933, Loss 4.059047\n",
      "grad:tensor([-0.1079,  0.6105])\n",
      "params:tensor([  4.7339, -13.7168])\n",
      "Epoch 934, Loss 4.055204\n",
      "grad:tensor([-0.1077,  0.6095])\n",
      "params:tensor([  4.7350, -13.7229])\n",
      "Epoch 935, Loss 4.051378\n",
      "grad:tensor([-0.1075,  0.6085])\n",
      "params:tensor([  4.7360, -13.7290])\n",
      "Epoch 936, Loss 4.047564\n",
      "grad:tensor([-0.1073,  0.6074])\n",
      "params:tensor([  4.7371, -13.7351])\n",
      "Epoch 937, Loss 4.043762\n",
      "grad:tensor([-0.1071,  0.6064])\n",
      "params:tensor([  4.7382, -13.7412])\n",
      "Epoch 938, Loss 4.039972\n",
      "grad:tensor([-0.1069,  0.6054])\n",
      "params:tensor([  4.7393, -13.7472])\n",
      "Epoch 939, Loss 4.036197\n",
      "grad:tensor([-0.1068,  0.6043])\n",
      "params:tensor([  4.7403, -13.7533])\n",
      "Epoch 940, Loss 4.032433\n",
      "grad:tensor([-0.1066,  0.6033])\n",
      "params:tensor([  4.7414, -13.7593])\n",
      "Epoch 941, Loss 4.028685\n",
      "grad:tensor([-0.1064,  0.6023])\n",
      "params:tensor([  4.7425, -13.7653])\n",
      "Epoch 942, Loss 4.024947\n",
      "grad:tensor([-0.1062,  0.6013])\n",
      "params:tensor([  4.7435, -13.7713])\n",
      "Epoch 943, Loss 4.021221\n",
      "grad:tensor([-0.1060,  0.6003])\n",
      "params:tensor([  4.7446, -13.7773])\n",
      "Epoch 944, Loss 4.017508\n",
      "grad:tensor([-0.1058,  0.5992])\n",
      "params:tensor([  4.7456, -13.7833])\n",
      "Epoch 945, Loss 4.013809\n",
      "grad:tensor([-0.1057,  0.5982])\n",
      "params:tensor([  4.7467, -13.7893])\n",
      "Epoch 946, Loss 4.010123\n",
      "grad:tensor([-0.1055,  0.5972])\n",
      "params:tensor([  4.7478, -13.7953])\n",
      "Epoch 947, Loss 4.006446\n",
      "grad:tensor([-0.1053,  0.5962])\n",
      "params:tensor([  4.7488, -13.8012])\n",
      "Epoch 948, Loss 4.002786\n",
      "grad:tensor([-0.1051,  0.5952])\n",
      "params:tensor([  4.7499, -13.8072])\n",
      "Epoch 949, Loss 3.999135\n",
      "grad:tensor([-0.1050,  0.5942])\n",
      "params:tensor([  4.7509, -13.8131])\n",
      "Epoch 950, Loss 3.995498\n",
      "grad:tensor([-0.1048,  0.5931])\n",
      "params:tensor([  4.7520, -13.8191])\n",
      "Epoch 951, Loss 3.991874\n",
      "grad:tensor([-0.1046,  0.5921])\n",
      "params:tensor([  4.7530, -13.8250])\n",
      "Epoch 952, Loss 3.988261\n",
      "grad:tensor([-0.1044,  0.5911])\n",
      "params:tensor([  4.7540, -13.8309])\n",
      "Epoch 953, Loss 3.984660\n",
      "grad:tensor([-0.1042,  0.5901])\n",
      "params:tensor([  4.7551, -13.8368])\n",
      "Epoch 954, Loss 3.981073\n",
      "grad:tensor([-0.1041,  0.5891])\n",
      "params:tensor([  4.7561, -13.8427])\n",
      "Epoch 955, Loss 3.977496\n",
      "grad:tensor([-0.1039,  0.5881])\n",
      "params:tensor([  4.7572, -13.8486])\n",
      "Epoch 956, Loss 3.973931\n",
      "grad:tensor([-0.1037,  0.5871])\n",
      "params:tensor([  4.7582, -13.8544])\n",
      "Epoch 957, Loss 3.970381\n",
      "grad:tensor([-0.1035,  0.5861])\n",
      "params:tensor([  4.7592, -13.8603])\n",
      "Epoch 958, Loss 3.966841\n",
      "grad:tensor([-0.1034,  0.5851])\n",
      "params:tensor([  4.7603, -13.8661])\n",
      "Epoch 959, Loss 3.963313\n",
      "grad:tensor([-0.1032,  0.5841])\n",
      "params:tensor([  4.7613, -13.8720])\n",
      "Epoch 960, Loss 3.959796\n",
      "grad:tensor([-0.1030,  0.5831])\n",
      "params:tensor([  4.7623, -13.8778])\n",
      "Epoch 961, Loss 3.956295\n",
      "grad:tensor([-0.1028,  0.5822])\n",
      "params:tensor([  4.7634, -13.8836])\n",
      "Epoch 962, Loss 3.952801\n",
      "grad:tensor([-0.1026,  0.5812])\n",
      "params:tensor([  4.7644, -13.8895])\n",
      "Epoch 963, Loss 3.949323\n",
      "grad:tensor([-0.1025,  0.5802])\n",
      "params:tensor([  4.7654, -13.8953])\n",
      "Epoch 964, Loss 3.945855\n",
      "grad:tensor([-0.1023,  0.5792])\n",
      "params:tensor([  4.7664, -13.9010])\n",
      "Epoch 965, Loss 3.942398\n",
      "grad:tensor([-0.1021,  0.5782])\n",
      "params:tensor([  4.7675, -13.9068])\n",
      "Epoch 966, Loss 3.938954\n",
      "grad:tensor([-0.1020,  0.5772])\n",
      "params:tensor([  4.7685, -13.9126])\n",
      "Epoch 967, Loss 3.935520\n",
      "grad:tensor([-0.1018,  0.5762])\n",
      "params:tensor([  4.7695, -13.9184])\n",
      "Epoch 968, Loss 3.932096\n",
      "grad:tensor([-0.1016,  0.5753])\n",
      "params:tensor([  4.7705, -13.9241])\n",
      "Epoch 969, Loss 3.928688\n",
      "grad:tensor([-0.1015,  0.5743])\n",
      "params:tensor([  4.7715, -13.9299])\n",
      "Epoch 970, Loss 3.925292\n",
      "grad:tensor([-0.1013,  0.5733])\n",
      "params:tensor([  4.7725, -13.9356])\n",
      "Epoch 971, Loss 3.921906\n",
      "grad:tensor([-0.1011,  0.5723])\n",
      "params:tensor([  4.7736, -13.9413])\n",
      "Epoch 972, Loss 3.918527\n",
      "grad:tensor([-0.1009,  0.5714])\n",
      "params:tensor([  4.7746, -13.9470])\n",
      "Epoch 973, Loss 3.915166\n",
      "grad:tensor([-0.1008,  0.5704])\n",
      "params:tensor([  4.7756, -13.9527])\n",
      "Epoch 974, Loss 3.911815\n",
      "grad:tensor([-0.1006,  0.5694])\n",
      "params:tensor([  4.7766, -13.9584])\n",
      "Epoch 975, Loss 3.908474\n",
      "grad:tensor([-0.1004,  0.5685])\n",
      "params:tensor([  4.7776, -13.9641])\n",
      "Epoch 976, Loss 3.905143\n",
      "grad:tensor([-0.1003,  0.5675])\n",
      "params:tensor([  4.7786, -13.9698])\n",
      "Epoch 977, Loss 3.901825\n",
      "grad:tensor([-0.1001,  0.5665])\n",
      "params:tensor([  4.7796, -13.9755])\n",
      "Epoch 978, Loss 3.898517\n",
      "grad:tensor([-0.0999,  0.5656])\n",
      "params:tensor([  4.7806, -13.9811])\n",
      "Epoch 979, Loss 3.895222\n",
      "grad:tensor([-0.0997,  0.5646])\n",
      "params:tensor([  4.7816, -13.9868])\n",
      "Epoch 980, Loss 3.891935\n",
      "grad:tensor([-0.0996,  0.5637])\n",
      "params:tensor([  4.7826, -13.9924])\n",
      "Epoch 981, Loss 3.888664\n",
      "grad:tensor([-0.0994,  0.5627])\n",
      "params:tensor([  4.7836, -13.9980])\n",
      "Epoch 982, Loss 3.885401\n",
      "grad:tensor([-0.0992,  0.5617])\n",
      "params:tensor([  4.7846, -14.0036])\n",
      "Epoch 983, Loss 3.882150\n",
      "grad:tensor([-0.0991,  0.5608])\n",
      "params:tensor([  4.7856, -14.0092])\n",
      "Epoch 984, Loss 3.878910\n",
      "grad:tensor([-0.0989,  0.5598])\n",
      "params:tensor([  4.7865, -14.0148])\n",
      "Epoch 985, Loss 3.875680\n",
      "grad:tensor([-0.0987,  0.5589])\n",
      "params:tensor([  4.7875, -14.0204])\n",
      "Epoch 986, Loss 3.872463\n",
      "grad:tensor([-0.0986,  0.5579])\n",
      "params:tensor([  4.7885, -14.0260])\n",
      "Epoch 987, Loss 3.869256\n",
      "grad:tensor([-0.0984,  0.5570])\n",
      "params:tensor([  4.7895, -14.0316])\n",
      "Epoch 988, Loss 3.866060\n",
      "grad:tensor([-0.0982,  0.5560])\n",
      "params:tensor([  4.7905, -14.0371])\n",
      "Epoch 989, Loss 3.862872\n",
      "grad:tensor([-0.0981,  0.5551])\n",
      "params:tensor([  4.7915, -14.0427])\n",
      "Epoch 990, Loss 3.859699\n",
      "grad:tensor([-0.0979,  0.5541])\n",
      "params:tensor([  4.7924, -14.0482])\n",
      "Epoch 991, Loss 3.856535\n",
      "grad:tensor([-0.0978,  0.5532])\n",
      "params:tensor([  4.7934, -14.0538])\n",
      "Epoch 992, Loss 3.853381\n",
      "grad:tensor([-0.0976,  0.5523])\n",
      "params:tensor([  4.7944, -14.0593])\n",
      "Epoch 993, Loss 3.850237\n",
      "grad:tensor([-0.0974,  0.5513])\n",
      "params:tensor([  4.7954, -14.0648])\n",
      "Epoch 994, Loss 3.847109\n",
      "grad:tensor([-0.0973,  0.5504])\n",
      "params:tensor([  4.7963, -14.0703])\n",
      "Epoch 995, Loss 3.843984\n",
      "grad:tensor([-0.0971,  0.5495])\n",
      "params:tensor([  4.7973, -14.0758])\n",
      "Epoch 996, Loss 3.840876\n",
      "grad:tensor([-0.0969,  0.5485])\n",
      "params:tensor([  4.7983, -14.0813])\n",
      "Epoch 997, Loss 3.837775\n",
      "grad:tensor([-0.0967,  0.5476])\n",
      "params:tensor([  4.7992, -14.0868])\n",
      "Epoch 998, Loss 3.834686\n",
      "grad:tensor([-0.0966,  0.5467])\n",
      "params:tensor([  4.8002, -14.0922])\n",
      "Epoch 999, Loss 3.831606\n",
      "grad:tensor([-0.0964,  0.5457])\n",
      "params:tensor([  4.8012, -14.0977])\n",
      "Epoch 1000, Loss 3.828538\n",
      "grad:tensor([-0.0962,  0.5448])\n",
      "params:tensor([  4.8021, -14.1031])\n",
      "Epoch 1001, Loss 3.825483\n",
      "grad:tensor([-0.0961,  0.5439])\n",
      "params:tensor([  4.8031, -14.1086])\n",
      "Epoch 1002, Loss 3.822433\n",
      "grad:tensor([-0.0959,  0.5430])\n",
      "params:tensor([  4.8041, -14.1140])\n",
      "Epoch 1003, Loss 3.819398\n",
      "grad:tensor([-0.0957,  0.5420])\n",
      "params:tensor([  4.8050, -14.1194])\n",
      "Epoch 1004, Loss 3.816369\n",
      "grad:tensor([-0.0956,  0.5411])\n",
      "params:tensor([  4.8060, -14.1248])\n",
      "Epoch 1005, Loss 3.813350\n",
      "grad:tensor([-0.0954,  0.5402])\n",
      "params:tensor([  4.8069, -14.1302])\n",
      "Epoch 1006, Loss 3.810344\n",
      "grad:tensor([-0.0953,  0.5393])\n",
      "params:tensor([  4.8079, -14.1356])\n",
      "Epoch 1007, Loss 3.807348\n",
      "grad:tensor([-0.0951,  0.5384])\n",
      "params:tensor([  4.8088, -14.1410])\n",
      "Epoch 1008, Loss 3.804360\n",
      "grad:tensor([-0.0949,  0.5375])\n",
      "params:tensor([  4.8098, -14.1464])\n",
      "Epoch 1009, Loss 3.801384\n",
      "grad:tensor([-0.0948,  0.5365])\n",
      "params:tensor([  4.8107, -14.1518])\n",
      "Epoch 1010, Loss 3.798421\n",
      "grad:tensor([-0.0946,  0.5356])\n",
      "params:tensor([  4.8117, -14.1571])\n",
      "Epoch 1011, Loss 3.795465\n",
      "grad:tensor([-0.0945,  0.5347])\n",
      "params:tensor([  4.8126, -14.1625])\n",
      "Epoch 1012, Loss 3.792518\n",
      "grad:tensor([-0.0943,  0.5338])\n",
      "params:tensor([  4.8136, -14.1678])\n",
      "Epoch 1013, Loss 3.789584\n",
      "grad:tensor([-0.0942,  0.5329])\n",
      "params:tensor([  4.8145, -14.1731])\n",
      "Epoch 1014, Loss 3.786658\n",
      "grad:tensor([-0.0940,  0.5320])\n",
      "params:tensor([  4.8154, -14.1784])\n",
      "Epoch 1015, Loss 3.783740\n",
      "grad:tensor([-0.0938,  0.5311])\n",
      "params:tensor([  4.8164, -14.1838])\n",
      "Epoch 1016, Loss 3.780832\n",
      "grad:tensor([-0.0937,  0.5302])\n",
      "params:tensor([  4.8173, -14.1891])\n",
      "Epoch 1017, Loss 3.777939\n",
      "grad:tensor([-0.0935,  0.5293])\n",
      "params:tensor([  4.8183, -14.1943])\n",
      "Epoch 1018, Loss 3.775053\n",
      "grad:tensor([-0.0933,  0.5284])\n",
      "params:tensor([  4.8192, -14.1996])\n",
      "Epoch 1019, Loss 3.772173\n",
      "grad:tensor([-0.0932,  0.5275])\n",
      "params:tensor([  4.8201, -14.2049])\n",
      "Epoch 1020, Loss 3.769311\n",
      "grad:tensor([-0.0930,  0.5266])\n",
      "params:tensor([  4.8210, -14.2102])\n",
      "Epoch 1021, Loss 3.766450\n",
      "grad:tensor([-0.0929,  0.5257])\n",
      "params:tensor([  4.8220, -14.2154])\n",
      "Epoch 1022, Loss 3.763602\n",
      "grad:tensor([-0.0927,  0.5248])\n",
      "params:tensor([  4.8229, -14.2207])\n",
      "Epoch 1023, Loss 3.760766\n",
      "grad:tensor([-0.0926,  0.5239])\n",
      "params:tensor([  4.8238, -14.2259])\n",
      "Epoch 1024, Loss 3.757936\n",
      "grad:tensor([-0.0924,  0.5230])\n",
      "params:tensor([  4.8248, -14.2311])\n",
      "Epoch 1025, Loss 3.755118\n",
      "grad:tensor([-0.0922,  0.5221])\n",
      "params:tensor([  4.8257, -14.2364])\n",
      "Epoch 1026, Loss 3.752309\n",
      "grad:tensor([-0.0921,  0.5213])\n",
      "params:tensor([  4.8266, -14.2416])\n",
      "Epoch 1027, Loss 3.749511\n",
      "grad:tensor([-0.0919,  0.5204])\n",
      "params:tensor([  4.8275, -14.2468])\n",
      "Epoch 1028, Loss 3.746722\n",
      "grad:tensor([-0.0918,  0.5195])\n",
      "params:tensor([  4.8284, -14.2520])\n",
      "Epoch 1029, Loss 3.743940\n",
      "grad:tensor([-0.0916,  0.5186])\n",
      "params:tensor([  4.8293, -14.2572])\n",
      "Epoch 1030, Loss 3.741169\n",
      "grad:tensor([-0.0915,  0.5177])\n",
      "params:tensor([  4.8303, -14.2623])\n",
      "Epoch 1031, Loss 3.738407\n",
      "grad:tensor([-0.0913,  0.5168])\n",
      "params:tensor([  4.8312, -14.2675])\n",
      "Epoch 1032, Loss 3.735656\n",
      "grad:tensor([-0.0912,  0.5160])\n",
      "params:tensor([  4.8321, -14.2727])\n",
      "Epoch 1033, Loss 3.732914\n",
      "grad:tensor([-0.0910,  0.5151])\n",
      "params:tensor([  4.8330, -14.2778])\n",
      "Epoch 1034, Loss 3.730181\n",
      "grad:tensor([-0.0908,  0.5142])\n",
      "params:tensor([  4.8339, -14.2830])\n",
      "Epoch 1035, Loss 3.727456\n",
      "grad:tensor([-0.0907,  0.5133])\n",
      "params:tensor([  4.8348, -14.2881])\n",
      "Epoch 1036, Loss 3.724740\n",
      "grad:tensor([-0.0905,  0.5125])\n",
      "params:tensor([  4.8357, -14.2932])\n",
      "Epoch 1037, Loss 3.722034\n",
      "grad:tensor([-0.0904,  0.5116])\n",
      "params:tensor([  4.8366, -14.2983])\n",
      "Epoch 1038, Loss 3.719337\n",
      "grad:tensor([-0.0902,  0.5107])\n",
      "params:tensor([  4.8375, -14.3034])\n",
      "Epoch 1039, Loss 3.716651\n",
      "grad:tensor([-0.0901,  0.5099])\n",
      "params:tensor([  4.8384, -14.3085])\n",
      "Epoch 1040, Loss 3.713972\n",
      "grad:tensor([-0.0899,  0.5090])\n",
      "params:tensor([  4.8393, -14.3136])\n",
      "Epoch 1041, Loss 3.711302\n",
      "grad:tensor([-0.0898,  0.5081])\n",
      "params:tensor([  4.8402, -14.3187])\n",
      "Epoch 1042, Loss 3.708644\n",
      "grad:tensor([-0.0896,  0.5073])\n",
      "params:tensor([  4.8411, -14.3238])\n",
      "Epoch 1043, Loss 3.705991\n",
      "grad:tensor([-0.0895,  0.5064])\n",
      "params:tensor([  4.8420, -14.3288])\n",
      "Epoch 1044, Loss 3.703351\n",
      "grad:tensor([-0.0893,  0.5055])\n",
      "params:tensor([  4.8429, -14.3339])\n",
      "Epoch 1045, Loss 3.700716\n",
      "grad:tensor([-0.0892,  0.5047])\n",
      "params:tensor([  4.8438, -14.3390])\n",
      "Epoch 1046, Loss 3.698091\n",
      "grad:tensor([-0.0890,  0.5038])\n",
      "params:tensor([  4.8447, -14.3440])\n",
      "Epoch 1047, Loss 3.695476\n",
      "grad:tensor([-0.0888,  0.5030])\n",
      "params:tensor([  4.8456, -14.3490])\n",
      "Epoch 1048, Loss 3.692869\n",
      "grad:tensor([-0.0887,  0.5021])\n",
      "params:tensor([  4.8465, -14.3540])\n",
      "Epoch 1049, Loss 3.690273\n",
      "grad:tensor([-0.0886,  0.5013])\n",
      "params:tensor([  4.8473, -14.3591])\n",
      "Epoch 1050, Loss 3.687683\n",
      "grad:tensor([-0.0884,  0.5004])\n",
      "params:tensor([  4.8482, -14.3641])\n",
      "Epoch 1051, Loss 3.685104\n",
      "grad:tensor([-0.0882,  0.4996])\n",
      "params:tensor([  4.8491, -14.3691])\n",
      "Epoch 1052, Loss 3.682532\n",
      "grad:tensor([-0.0881,  0.4987])\n",
      "params:tensor([  4.8500, -14.3740])\n",
      "Epoch 1053, Loss 3.679969\n",
      "grad:tensor([-0.0879,  0.4979])\n",
      "params:tensor([  4.8509, -14.3790])\n",
      "Epoch 1054, Loss 3.677417\n",
      "grad:tensor([-0.0878,  0.4970])\n",
      "params:tensor([  4.8518, -14.3840])\n",
      "Epoch 1055, Loss 3.674871\n",
      "grad:tensor([-0.0877,  0.4962])\n",
      "params:tensor([  4.8526, -14.3889])\n",
      "Epoch 1056, Loss 3.672335\n",
      "grad:tensor([-0.0875,  0.4953])\n",
      "params:tensor([  4.8535, -14.3939])\n",
      "Epoch 1057, Loss 3.669804\n",
      "grad:tensor([-0.0873,  0.4945])\n",
      "params:tensor([  4.8544, -14.3988])\n",
      "Epoch 1058, Loss 3.667287\n",
      "grad:tensor([-0.0872,  0.4936])\n",
      "params:tensor([  4.8552, -14.4038])\n",
      "Epoch 1059, Loss 3.664775\n",
      "grad:tensor([-0.0870,  0.4928])\n",
      "params:tensor([  4.8561, -14.4087])\n",
      "Epoch 1060, Loss 3.662273\n",
      "grad:tensor([-0.0869,  0.4920])\n",
      "params:tensor([  4.8570, -14.4136])\n",
      "Epoch 1061, Loss 3.659778\n",
      "grad:tensor([-0.0868,  0.4911])\n",
      "params:tensor([  4.8579, -14.4185])\n",
      "Epoch 1062, Loss 3.657295\n",
      "grad:tensor([-0.0866,  0.4903])\n",
      "params:tensor([  4.8587, -14.4234])\n",
      "Epoch 1063, Loss 3.654816\n",
      "grad:tensor([-0.0865,  0.4895])\n",
      "params:tensor([  4.8596, -14.4283])\n",
      "Epoch 1064, Loss 3.652349\n",
      "grad:tensor([-0.0863,  0.4886])\n",
      "params:tensor([  4.8604, -14.4332])\n",
      "Epoch 1065, Loss 3.649889\n",
      "grad:tensor([-0.0862,  0.4878])\n",
      "params:tensor([  4.8613, -14.4381])\n",
      "Epoch 1066, Loss 3.647437\n",
      "grad:tensor([-0.0860,  0.4870])\n",
      "params:tensor([  4.8622, -14.4430])\n",
      "Epoch 1067, Loss 3.644991\n",
      "grad:tensor([-0.0859,  0.4862])\n",
      "params:tensor([  4.8630, -14.4478])\n",
      "Epoch 1068, Loss 3.642559\n",
      "grad:tensor([-0.0857,  0.4853])\n",
      "params:tensor([  4.8639, -14.4527])\n",
      "Epoch 1069, Loss 3.640132\n",
      "grad:tensor([-0.0856,  0.4845])\n",
      "params:tensor([  4.8647, -14.4575])\n",
      "Epoch 1070, Loss 3.637711\n",
      "grad:tensor([-0.0854,  0.4837])\n",
      "params:tensor([  4.8656, -14.4624])\n",
      "Epoch 1071, Loss 3.635302\n",
      "grad:tensor([-0.0853,  0.4829])\n",
      "params:tensor([  4.8665, -14.4672])\n",
      "Epoch 1072, Loss 3.632902\n",
      "grad:tensor([-0.0851,  0.4820])\n",
      "params:tensor([  4.8673, -14.4720])\n",
      "Epoch 1073, Loss 3.630508\n",
      "grad:tensor([-0.0850,  0.4812])\n",
      "params:tensor([  4.8682, -14.4768])\n",
      "Epoch 1074, Loss 3.628119\n",
      "grad:tensor([-0.0849,  0.4804])\n",
      "params:tensor([  4.8690, -14.4816])\n",
      "Epoch 1075, Loss 3.625741\n",
      "grad:tensor([-0.0847,  0.4796])\n",
      "params:tensor([  4.8698, -14.4864])\n",
      "Epoch 1076, Loss 3.623374\n",
      "grad:tensor([-0.0846,  0.4788])\n",
      "params:tensor([  4.8707, -14.4912])\n",
      "Epoch 1077, Loss 3.621010\n",
      "grad:tensor([-0.0844,  0.4780])\n",
      "params:tensor([  4.8715, -14.4960])\n",
      "Epoch 1078, Loss 3.618659\n",
      "grad:tensor([-0.0843,  0.4771])\n",
      "params:tensor([  4.8724, -14.5008])\n",
      "Epoch 1079, Loss 3.616311\n",
      "grad:tensor([-0.0841,  0.4763])\n",
      "params:tensor([  4.8732, -14.5055])\n",
      "Epoch 1080, Loss 3.613973\n",
      "grad:tensor([-0.0840,  0.4755])\n",
      "params:tensor([  4.8741, -14.5103])\n",
      "Epoch 1081, Loss 3.611643\n",
      "grad:tensor([-0.0839,  0.4747])\n",
      "params:tensor([  4.8749, -14.5150])\n",
      "Epoch 1082, Loss 3.609321\n",
      "grad:tensor([-0.0837,  0.4739])\n",
      "params:tensor([  4.8757, -14.5198])\n",
      "Epoch 1083, Loss 3.607008\n",
      "grad:tensor([-0.0836,  0.4731])\n",
      "params:tensor([  4.8766, -14.5245])\n",
      "Epoch 1084, Loss 3.604701\n",
      "grad:tensor([-0.0834,  0.4723])\n",
      "params:tensor([  4.8774, -14.5292])\n",
      "Epoch 1085, Loss 3.602403\n",
      "grad:tensor([-0.0833,  0.4715])\n",
      "params:tensor([  4.8782, -14.5339])\n",
      "Epoch 1086, Loss 3.600114\n",
      "grad:tensor([-0.0832,  0.4707])\n",
      "params:tensor([  4.8791, -14.5387])\n",
      "Epoch 1087, Loss 3.597831\n",
      "grad:tensor([-0.0830,  0.4699])\n",
      "params:tensor([  4.8799, -14.5434])\n",
      "Epoch 1088, Loss 3.595553\n",
      "grad:tensor([-0.0829,  0.4691])\n",
      "params:tensor([  4.8807, -14.5480])\n",
      "Epoch 1089, Loss 3.593287\n",
      "grad:tensor([-0.0827,  0.4683])\n",
      "params:tensor([  4.8816, -14.5527])\n",
      "Epoch 1090, Loss 3.591030\n",
      "grad:tensor([-0.0826,  0.4675])\n",
      "params:tensor([  4.8824, -14.5574])\n",
      "Epoch 1091, Loss 3.588776\n",
      "grad:tensor([-0.0824,  0.4667])\n",
      "params:tensor([  4.8832, -14.5621])\n",
      "Epoch 1092, Loss 3.586534\n",
      "grad:tensor([-0.0823,  0.4659])\n",
      "params:tensor([  4.8840, -14.5667])\n",
      "Epoch 1093, Loss 3.584294\n",
      "grad:tensor([-0.0822,  0.4651])\n",
      "params:tensor([  4.8849, -14.5714])\n",
      "Epoch 1094, Loss 3.582067\n",
      "grad:tensor([-0.0820,  0.4643])\n",
      "params:tensor([  4.8857, -14.5760])\n",
      "Epoch 1095, Loss 3.579845\n",
      "grad:tensor([-0.0819,  0.4636])\n",
      "params:tensor([  4.8865, -14.5807])\n",
      "Epoch 1096, Loss 3.577631\n",
      "grad:tensor([-0.0818,  0.4628])\n",
      "params:tensor([  4.8873, -14.5853])\n",
      "Epoch 1097, Loss 3.575424\n",
      "grad:tensor([-0.0816,  0.4620])\n",
      "params:tensor([  4.8881, -14.5899])\n",
      "Epoch 1098, Loss 3.573225\n",
      "grad:tensor([-0.0815,  0.4612])\n",
      "params:tensor([  4.8889, -14.5945])\n",
      "Epoch 1099, Loss 3.571035\n",
      "grad:tensor([-0.0813,  0.4604])\n",
      "params:tensor([  4.8898, -14.5991])\n",
      "Epoch 1100, Loss 3.568848\n",
      "grad:tensor([-0.0812,  0.4596])\n",
      "params:tensor([  4.8906, -14.6037])\n",
      "Epoch 1101, Loss 3.566673\n",
      "grad:tensor([-0.0810,  0.4588])\n",
      "params:tensor([  4.8914, -14.6083])\n",
      "Epoch 1102, Loss 3.564506\n",
      "grad:tensor([-0.0809,  0.4581])\n",
      "params:tensor([  4.8922, -14.6129])\n",
      "Epoch 1103, Loss 3.562341\n",
      "grad:tensor([-0.0808,  0.4573])\n",
      "params:tensor([  4.8930, -14.6175])\n",
      "Epoch 1104, Loss 3.560185\n",
      "grad:tensor([-0.0806,  0.4565])\n",
      "params:tensor([  4.8938, -14.6220])\n",
      "Epoch 1105, Loss 3.558040\n",
      "grad:tensor([-0.0805,  0.4557])\n",
      "params:tensor([  4.8946, -14.6266])\n",
      "Epoch 1106, Loss 3.555901\n",
      "grad:tensor([-0.0804,  0.4550])\n",
      "params:tensor([  4.8954, -14.6311])\n",
      "Epoch 1107, Loss 3.553767\n",
      "grad:tensor([-0.0802,  0.4542])\n",
      "params:tensor([  4.8962, -14.6357])\n",
      "Epoch 1108, Loss 3.551641\n",
      "grad:tensor([-0.0801,  0.4534])\n",
      "params:tensor([  4.8970, -14.6402])\n",
      "Epoch 1109, Loss 3.549524\n",
      "grad:tensor([-0.0799,  0.4527])\n",
      "params:tensor([  4.8978, -14.6447])\n",
      "Epoch 1110, Loss 3.547411\n",
      "grad:tensor([-0.0798,  0.4519])\n",
      "params:tensor([  4.8986, -14.6493])\n",
      "Epoch 1111, Loss 3.545309\n",
      "grad:tensor([-0.0797,  0.4511])\n",
      "params:tensor([  4.8994, -14.6538])\n",
      "Epoch 1112, Loss 3.543211\n",
      "grad:tensor([-0.0796,  0.4503])\n",
      "params:tensor([  4.9002, -14.6583])\n",
      "Epoch 1113, Loss 3.541124\n",
      "grad:tensor([-0.0794,  0.4496])\n",
      "params:tensor([  4.9010, -14.6628])\n",
      "Epoch 1114, Loss 3.539041\n",
      "grad:tensor([-0.0793,  0.4488])\n",
      "params:tensor([  4.9018, -14.6673])\n",
      "Epoch 1115, Loss 3.536967\n",
      "grad:tensor([-0.0791,  0.4481])\n",
      "params:tensor([  4.9026, -14.6717])\n",
      "Epoch 1116, Loss 3.534896\n",
      "grad:tensor([-0.0790,  0.4473])\n",
      "params:tensor([  4.9034, -14.6762])\n",
      "Epoch 1117, Loss 3.532835\n",
      "grad:tensor([-0.0789,  0.4465])\n",
      "params:tensor([  4.9042, -14.6807])\n",
      "Epoch 1118, Loss 3.530781\n",
      "grad:tensor([-0.0787,  0.4458])\n",
      "params:tensor([  4.9049, -14.6851])\n",
      "Epoch 1119, Loss 3.528734\n",
      "grad:tensor([-0.0786,  0.4450])\n",
      "params:tensor([  4.9057, -14.6896])\n",
      "Epoch 1120, Loss 3.526694\n",
      "grad:tensor([-0.0785,  0.4443])\n",
      "params:tensor([  4.9065, -14.6940])\n",
      "Epoch 1121, Loss 3.524662\n",
      "grad:tensor([-0.0784,  0.4435])\n",
      "params:tensor([  4.9073, -14.6985])\n",
      "Epoch 1122, Loss 3.522633\n",
      "grad:tensor([-0.0782,  0.4428])\n",
      "params:tensor([  4.9081, -14.7029])\n",
      "Epoch 1123, Loss 3.520614\n",
      "grad:tensor([-0.0781,  0.4420])\n",
      "params:tensor([  4.9089, -14.7073])\n",
      "Epoch 1124, Loss 3.518601\n",
      "grad:tensor([-0.0779,  0.4413])\n",
      "params:tensor([  4.9096, -14.7117])\n",
      "Epoch 1125, Loss 3.516594\n",
      "grad:tensor([-0.0778,  0.4405])\n",
      "params:tensor([  4.9104, -14.7161])\n",
      "Epoch 1126, Loss 3.514594\n",
      "grad:tensor([-0.0777,  0.4398])\n",
      "params:tensor([  4.9112, -14.7205])\n",
      "Epoch 1127, Loss 3.512602\n",
      "grad:tensor([-0.0775,  0.4390])\n",
      "params:tensor([  4.9120, -14.7249])\n",
      "Epoch 1128, Loss 3.510619\n",
      "grad:tensor([-0.0774,  0.4383])\n",
      "params:tensor([  4.9128, -14.7293])\n",
      "Epoch 1129, Loss 3.508637\n",
      "grad:tensor([-0.0773,  0.4375])\n",
      "params:tensor([  4.9135, -14.7337])\n",
      "Epoch 1130, Loss 3.506665\n",
      "grad:tensor([-0.0772,  0.4368])\n",
      "params:tensor([  4.9143, -14.7380])\n",
      "Epoch 1131, Loss 3.504699\n",
      "grad:tensor([-0.0770,  0.4360])\n",
      "params:tensor([  4.9151, -14.7424])\n",
      "Epoch 1132, Loss 3.502741\n",
      "grad:tensor([-0.0769,  0.4353])\n",
      "params:tensor([  4.9158, -14.7467])\n",
      "Epoch 1133, Loss 3.500789\n",
      "grad:tensor([-0.0767,  0.4346])\n",
      "params:tensor([  4.9166, -14.7511])\n",
      "Epoch 1134, Loss 3.498843\n",
      "grad:tensor([-0.0766,  0.4338])\n",
      "params:tensor([  4.9174, -14.7554])\n",
      "Epoch 1135, Loss 3.496905\n",
      "grad:tensor([-0.0765,  0.4331])\n",
      "params:tensor([  4.9181, -14.7598])\n",
      "Epoch 1136, Loss 3.494972\n",
      "grad:tensor([-0.0764,  0.4323])\n",
      "params:tensor([  4.9189, -14.7641])\n",
      "Epoch 1137, Loss 3.493046\n",
      "grad:tensor([-0.0763,  0.4316])\n",
      "params:tensor([  4.9197, -14.7684])\n",
      "Epoch 1138, Loss 3.491127\n",
      "grad:tensor([-0.0761,  0.4309])\n",
      "params:tensor([  4.9204, -14.7727])\n",
      "Epoch 1139, Loss 3.489214\n",
      "grad:tensor([-0.0760,  0.4301])\n",
      "params:tensor([  4.9212, -14.7770])\n",
      "Epoch 1140, Loss 3.487308\n",
      "grad:tensor([-0.0759,  0.4294])\n",
      "params:tensor([  4.9219, -14.7813])\n",
      "Epoch 1141, Loss 3.485410\n",
      "grad:tensor([-0.0757,  0.4287])\n",
      "params:tensor([  4.9227, -14.7856])\n",
      "Epoch 1142, Loss 3.483515\n",
      "grad:tensor([-0.0756,  0.4280])\n",
      "params:tensor([  4.9235, -14.7899])\n",
      "Epoch 1143, Loss 3.481627\n",
      "grad:tensor([-0.0755,  0.4272])\n",
      "params:tensor([  4.9242, -14.7941])\n",
      "Epoch 1144, Loss 3.479746\n",
      "grad:tensor([-0.0753,  0.4265])\n",
      "params:tensor([  4.9250, -14.7984])\n",
      "Epoch 1145, Loss 3.477872\n",
      "grad:tensor([-0.0752,  0.4258])\n",
      "params:tensor([  4.9257, -14.8027])\n",
      "Epoch 1146, Loss 3.476005\n",
      "grad:tensor([-0.0751,  0.4250])\n",
      "params:tensor([  4.9265, -14.8069])\n",
      "Epoch 1147, Loss 3.474143\n",
      "grad:tensor([-0.0750,  0.4243])\n",
      "params:tensor([  4.9272, -14.8112])\n",
      "Epoch 1148, Loss 3.472288\n",
      "grad:tensor([-0.0748,  0.4236])\n",
      "params:tensor([  4.9280, -14.8154])\n",
      "Epoch 1149, Loss 3.470441\n",
      "grad:tensor([-0.0747,  0.4229])\n",
      "params:tensor([  4.9287, -14.8196])\n",
      "Epoch 1150, Loss 3.468597\n",
      "grad:tensor([-0.0746,  0.4222])\n",
      "params:tensor([  4.9295, -14.8238])\n",
      "Epoch 1151, Loss 3.466762\n",
      "grad:tensor([-0.0745,  0.4215])\n",
      "params:tensor([  4.9302, -14.8281])\n",
      "Epoch 1152, Loss 3.464930\n",
      "grad:tensor([-0.0743,  0.4207])\n",
      "params:tensor([  4.9309, -14.8323])\n",
      "Epoch 1153, Loss 3.463105\n",
      "grad:tensor([-0.0742,  0.4200])\n",
      "params:tensor([  4.9317, -14.8365])\n",
      "Epoch 1154, Loss 3.461290\n",
      "grad:tensor([-0.0741,  0.4193])\n",
      "params:tensor([  4.9324, -14.8407])\n",
      "Epoch 1155, Loss 3.459477\n",
      "grad:tensor([-0.0739,  0.4186])\n",
      "params:tensor([  4.9332, -14.8448])\n",
      "Epoch 1156, Loss 3.457671\n",
      "grad:tensor([-0.0738,  0.4179])\n",
      "params:tensor([  4.9339, -14.8490])\n",
      "Epoch 1157, Loss 3.455873\n",
      "grad:tensor([-0.0737,  0.4172])\n",
      "params:tensor([  4.9346, -14.8532])\n",
      "Epoch 1158, Loss 3.454080\n",
      "grad:tensor([-0.0736,  0.4165])\n",
      "params:tensor([  4.9354, -14.8574])\n",
      "Epoch 1159, Loss 3.452293\n",
      "grad:tensor([-0.0734,  0.4158])\n",
      "params:tensor([  4.9361, -14.8615])\n",
      "Epoch 1160, Loss 3.450513\n",
      "grad:tensor([-0.0733,  0.4151])\n",
      "params:tensor([  4.9368, -14.8657])\n",
      "Epoch 1161, Loss 3.448736\n",
      "grad:tensor([-0.0732,  0.4143])\n",
      "params:tensor([  4.9376, -14.8698])\n",
      "Epoch 1162, Loss 3.446968\n",
      "grad:tensor([-0.0731,  0.4136])\n",
      "params:tensor([  4.9383, -14.8739])\n",
      "Epoch 1163, Loss 3.445203\n",
      "grad:tensor([-0.0730,  0.4129])\n",
      "params:tensor([  4.9390, -14.8781])\n",
      "Epoch 1164, Loss 3.443449\n",
      "grad:tensor([-0.0728,  0.4122])\n",
      "params:tensor([  4.9398, -14.8822])\n",
      "Epoch 1165, Loss 3.441697\n",
      "grad:tensor([-0.0727,  0.4115])\n",
      "params:tensor([  4.9405, -14.8863])\n",
      "Epoch 1166, Loss 3.439952\n",
      "grad:tensor([-0.0726,  0.4108])\n",
      "params:tensor([  4.9412, -14.8904])\n",
      "Epoch 1167, Loss 3.438210\n",
      "grad:tensor([-0.0725,  0.4101])\n",
      "params:tensor([  4.9419, -14.8945])\n",
      "Epoch 1168, Loss 3.436479\n",
      "grad:tensor([-0.0723,  0.4094])\n",
      "params:tensor([  4.9427, -14.8986])\n",
      "Epoch 1169, Loss 3.434753\n",
      "grad:tensor([-0.0722,  0.4087])\n",
      "params:tensor([  4.9434, -14.9027])\n",
      "Epoch 1170, Loss 3.433030\n",
      "grad:tensor([-0.0721,  0.4081])\n",
      "params:tensor([  4.9441, -14.9068])\n",
      "Epoch 1171, Loss 3.431314\n",
      "grad:tensor([-0.0720,  0.4074])\n",
      "params:tensor([  4.9448, -14.9109])\n",
      "Epoch 1172, Loss 3.429607\n",
      "grad:tensor([-0.0719,  0.4067])\n",
      "params:tensor([  4.9455, -14.9149])\n",
      "Epoch 1173, Loss 3.427903\n",
      "grad:tensor([-0.0717,  0.4060])\n",
      "params:tensor([  4.9463, -14.9190])\n",
      "Epoch 1174, Loss 3.426204\n",
      "grad:tensor([-0.0716,  0.4053])\n",
      "params:tensor([  4.9470, -14.9230])\n",
      "Epoch 1175, Loss 3.424510\n",
      "grad:tensor([-0.0715,  0.4046])\n",
      "params:tensor([  4.9477, -14.9271])\n",
      "Epoch 1176, Loss 3.422823\n",
      "grad:tensor([-0.0714,  0.4039])\n",
      "params:tensor([  4.9484, -14.9311])\n",
      "Epoch 1177, Loss 3.421144\n",
      "grad:tensor([-0.0712,  0.4032])\n",
      "params:tensor([  4.9491, -14.9352])\n",
      "Epoch 1178, Loss 3.419468\n",
      "grad:tensor([-0.0711,  0.4025])\n",
      "params:tensor([  4.9498, -14.9392])\n",
      "Epoch 1179, Loss 3.417798\n",
      "grad:tensor([-0.0710,  0.4019])\n",
      "params:tensor([  4.9505, -14.9432])\n",
      "Epoch 1180, Loss 3.416134\n",
      "grad:tensor([-0.0709,  0.4012])\n",
      "params:tensor([  4.9512, -14.9472])\n",
      "Epoch 1181, Loss 3.414476\n",
      "grad:tensor([-0.0708,  0.4005])\n",
      "params:tensor([  4.9520, -14.9512])\n",
      "Epoch 1182, Loss 3.412824\n",
      "grad:tensor([-0.0706,  0.3998])\n",
      "params:tensor([  4.9527, -14.9552])\n",
      "Epoch 1183, Loss 3.411176\n",
      "grad:tensor([-0.0705,  0.3991])\n",
      "params:tensor([  4.9534, -14.9592])\n",
      "Epoch 1184, Loss 3.409534\n",
      "grad:tensor([-0.0704,  0.3985])\n",
      "params:tensor([  4.9541, -14.9632])\n",
      "Epoch 1185, Loss 3.407900\n",
      "grad:tensor([-0.0703,  0.3978])\n",
      "params:tensor([  4.9548, -14.9672])\n",
      "Epoch 1186, Loss 3.406271\n",
      "grad:tensor([-0.0701,  0.3971])\n",
      "params:tensor([  4.9555, -14.9711])\n",
      "Epoch 1187, Loss 3.404645\n",
      "grad:tensor([-0.0700,  0.3964])\n",
      "params:tensor([  4.9562, -14.9751])\n",
      "Epoch 1188, Loss 3.403024\n",
      "grad:tensor([-0.0699,  0.3958])\n",
      "params:tensor([  4.9569, -14.9791])\n",
      "Epoch 1189, Loss 3.401413\n",
      "grad:tensor([-0.0698,  0.3951])\n",
      "params:tensor([  4.9576, -14.9830])\n",
      "Epoch 1190, Loss 3.399802\n",
      "grad:tensor([-0.0697,  0.3944])\n",
      "params:tensor([  4.9583, -14.9870])\n",
      "Epoch 1191, Loss 3.398200\n",
      "grad:tensor([-0.0696,  0.3937])\n",
      "params:tensor([  4.9590, -14.9909])\n",
      "Epoch 1192, Loss 3.396602\n",
      "grad:tensor([-0.0694,  0.3931])\n",
      "params:tensor([  4.9597, -14.9948])\n",
      "Epoch 1193, Loss 3.395011\n",
      "grad:tensor([-0.0693,  0.3924])\n",
      "params:tensor([  4.9604, -14.9988])\n",
      "Epoch 1194, Loss 3.393425\n",
      "grad:tensor([-0.0692,  0.3917])\n",
      "params:tensor([  4.9610, -15.0027])\n",
      "Epoch 1195, Loss 3.391844\n",
      "grad:tensor([-0.0691,  0.3911])\n",
      "params:tensor([  4.9617, -15.0066])\n",
      "Epoch 1196, Loss 3.390266\n",
      "grad:tensor([-0.0690,  0.3904])\n",
      "params:tensor([  4.9624, -15.0105])\n",
      "Epoch 1197, Loss 3.388697\n",
      "grad:tensor([-0.0689,  0.3897])\n",
      "params:tensor([  4.9631, -15.0144])\n",
      "Epoch 1198, Loss 3.387131\n",
      "grad:tensor([-0.0687,  0.3891])\n",
      "params:tensor([  4.9638, -15.0183])\n",
      "Epoch 1199, Loss 3.385571\n",
      "grad:tensor([-0.0686,  0.3884])\n",
      "params:tensor([  4.9645, -15.0222])\n",
      "Epoch 1200, Loss 3.384018\n",
      "grad:tensor([-0.0685,  0.3878])\n",
      "params:tensor([  4.9652, -15.0260])\n",
      "Epoch 1201, Loss 3.382467\n",
      "grad:tensor([-0.0684,  0.3871])\n",
      "params:tensor([  4.9659, -15.0299])\n",
      "Epoch 1202, Loss 3.380925\n",
      "grad:tensor([-0.0683,  0.3864])\n",
      "params:tensor([  4.9665, -15.0338])\n",
      "Epoch 1203, Loss 3.379385\n",
      "grad:tensor([-0.0681,  0.3858])\n",
      "params:tensor([  4.9672, -15.0376])\n",
      "Epoch 1204, Loss 3.377851\n",
      "grad:tensor([-0.0680,  0.3851])\n",
      "params:tensor([  4.9679, -15.0415])\n",
      "Epoch 1205, Loss 3.376323\n",
      "grad:tensor([-0.0679,  0.3845])\n",
      "params:tensor([  4.9686, -15.0453])\n",
      "Epoch 1206, Loss 3.374800\n",
      "grad:tensor([-0.0678,  0.3838])\n",
      "params:tensor([  4.9693, -15.0492])\n",
      "Epoch 1207, Loss 3.373284\n",
      "grad:tensor([-0.0677,  0.3832])\n",
      "params:tensor([  4.9699, -15.0530])\n",
      "Epoch 1208, Loss 3.371769\n",
      "grad:tensor([-0.0676,  0.3825])\n",
      "params:tensor([  4.9706, -15.0568])\n",
      "Epoch 1209, Loss 3.370261\n",
      "grad:tensor([-0.0675,  0.3819])\n",
      "params:tensor([  4.9713, -15.0606])\n",
      "Epoch 1210, Loss 3.368760\n",
      "grad:tensor([-0.0673,  0.3812])\n",
      "params:tensor([  4.9720, -15.0645])\n",
      "Epoch 1211, Loss 3.367262\n",
      "grad:tensor([-0.0672,  0.3806])\n",
      "params:tensor([  4.9726, -15.0683])\n",
      "Epoch 1212, Loss 3.365771\n",
      "grad:tensor([-0.0671,  0.3799])\n",
      "params:tensor([  4.9733, -15.0721])\n",
      "Epoch 1213, Loss 3.364282\n",
      "grad:tensor([-0.0670,  0.3793])\n",
      "params:tensor([  4.9740, -15.0758])\n",
      "Epoch 1214, Loss 3.362800\n",
      "grad:tensor([-0.0669,  0.3786])\n",
      "params:tensor([  4.9746, -15.0796])\n",
      "Epoch 1215, Loss 3.361324\n",
      "grad:tensor([-0.0668,  0.3780])\n",
      "params:tensor([  4.9753, -15.0834])\n",
      "Epoch 1216, Loss 3.359850\n",
      "grad:tensor([-0.0667,  0.3774])\n",
      "params:tensor([  4.9760, -15.0872])\n",
      "Epoch 1217, Loss 3.358384\n",
      "grad:tensor([-0.0665,  0.3767])\n",
      "params:tensor([  4.9766, -15.0910])\n",
      "Epoch 1218, Loss 3.356921\n",
      "grad:tensor([-0.0664,  0.3761])\n",
      "params:tensor([  4.9773, -15.0947])\n",
      "Epoch 1219, Loss 3.355464\n",
      "grad:tensor([-0.0663,  0.3754])\n",
      "params:tensor([  4.9780, -15.0985])\n",
      "Epoch 1220, Loss 3.354012\n",
      "grad:tensor([-0.0662,  0.3748])\n",
      "params:tensor([  4.9786, -15.1022])\n",
      "Epoch 1221, Loss 3.352564\n",
      "grad:tensor([-0.0661,  0.3742])\n",
      "params:tensor([  4.9793, -15.1060])\n",
      "Epoch 1222, Loss 3.351122\n",
      "grad:tensor([-0.0660,  0.3735])\n",
      "params:tensor([  4.9799, -15.1097])\n",
      "Epoch 1223, Loss 3.349685\n",
      "grad:tensor([-0.0659,  0.3729])\n",
      "params:tensor([  4.9806, -15.1134])\n",
      "Epoch 1224, Loss 3.348251\n",
      "grad:tensor([-0.0657,  0.3723])\n",
      "params:tensor([  4.9813, -15.1171])\n",
      "Epoch 1225, Loss 3.346824\n",
      "grad:tensor([-0.0656,  0.3716])\n",
      "params:tensor([  4.9819, -15.1209])\n",
      "Epoch 1226, Loss 3.345403\n",
      "grad:tensor([-0.0655,  0.3710])\n",
      "params:tensor([  4.9826, -15.1246])\n",
      "Epoch 1227, Loss 3.343982\n",
      "grad:tensor([-0.0654,  0.3704])\n",
      "params:tensor([  4.9832, -15.1283])\n",
      "Epoch 1228, Loss 3.342571\n",
      "grad:tensor([-0.0653,  0.3697])\n",
      "params:tensor([  4.9839, -15.1320])\n",
      "Epoch 1229, Loss 3.341160\n",
      "grad:tensor([-0.0652,  0.3691])\n",
      "params:tensor([  4.9845, -15.1357])\n",
      "Epoch 1230, Loss 3.339758\n",
      "grad:tensor([-0.0651,  0.3685])\n",
      "params:tensor([  4.9852, -15.1393])\n",
      "Epoch 1231, Loss 3.338359\n",
      "grad:tensor([-0.0650,  0.3679])\n",
      "params:tensor([  4.9858, -15.1430])\n",
      "Epoch 1232, Loss 3.336965\n",
      "grad:tensor([-0.0649,  0.3672])\n",
      "params:tensor([  4.9865, -15.1467])\n",
      "Epoch 1233, Loss 3.335577\n",
      "grad:tensor([-0.0648,  0.3666])\n",
      "params:tensor([  4.9871, -15.1504])\n",
      "Epoch 1234, Loss 3.334192\n",
      "grad:tensor([-0.0646,  0.3660])\n",
      "params:tensor([  4.9878, -15.1540])\n",
      "Epoch 1235, Loss 3.332811\n",
      "grad:tensor([-0.0645,  0.3654])\n",
      "params:tensor([  4.9884, -15.1577])\n",
      "Epoch 1236, Loss 3.331436\n",
      "grad:tensor([-0.0644,  0.3647])\n",
      "params:tensor([  4.9891, -15.1613])\n",
      "Epoch 1237, Loss 3.330065\n",
      "grad:tensor([-0.0643,  0.3641])\n",
      "params:tensor([  4.9897, -15.1650])\n",
      "Epoch 1238, Loss 3.328699\n",
      "grad:tensor([-0.0642,  0.3635])\n",
      "params:tensor([  4.9904, -15.1686])\n",
      "Epoch 1239, Loss 3.327339\n",
      "grad:tensor([-0.0641,  0.3629])\n",
      "params:tensor([  4.9910, -15.1722])\n",
      "Epoch 1240, Loss 3.325980\n",
      "grad:tensor([-0.0640,  0.3623])\n",
      "params:tensor([  4.9916, -15.1759])\n",
      "Epoch 1241, Loss 3.324628\n",
      "grad:tensor([-0.0639,  0.3617])\n",
      "params:tensor([  4.9923, -15.1795])\n",
      "Epoch 1242, Loss 3.323279\n",
      "grad:tensor([-0.0638,  0.3610])\n",
      "params:tensor([  4.9929, -15.1831])\n",
      "Epoch 1243, Loss 3.321935\n",
      "grad:tensor([-0.0637,  0.3604])\n",
      "params:tensor([  4.9936, -15.1867])\n",
      "Epoch 1244, Loss 3.320600\n",
      "grad:tensor([-0.0636,  0.3598])\n",
      "params:tensor([  4.9942, -15.1903])\n",
      "Epoch 1245, Loss 3.319264\n",
      "grad:tensor([-0.0635,  0.3592])\n",
      "params:tensor([  4.9948, -15.1939])\n",
      "Epoch 1246, Loss 3.317935\n",
      "grad:tensor([-0.0633,  0.3586])\n",
      "params:tensor([  4.9955, -15.1975])\n",
      "Epoch 1247, Loss 3.316611\n",
      "grad:tensor([-0.0633,  0.3580])\n",
      "params:tensor([  4.9961, -15.2010])\n",
      "Epoch 1248, Loss 3.315289\n",
      "grad:tensor([-0.0631,  0.3574])\n",
      "params:tensor([  4.9967, -15.2046])\n",
      "Epoch 1249, Loss 3.313973\n",
      "grad:tensor([-0.0630,  0.3568])\n",
      "params:tensor([  4.9973, -15.2082])\n",
      "Epoch 1250, Loss 3.312663\n",
      "grad:tensor([-0.0629,  0.3562])\n",
      "params:tensor([  4.9980, -15.2117])\n",
      "Epoch 1251, Loss 3.311353\n",
      "grad:tensor([-0.0628,  0.3556])\n",
      "params:tensor([  4.9986, -15.2153])\n",
      "Epoch 1252, Loss 3.310053\n",
      "grad:tensor([-0.0627,  0.3550])\n",
      "params:tensor([  4.9992, -15.2189])\n",
      "Epoch 1253, Loss 3.308756\n",
      "grad:tensor([-0.0626,  0.3543])\n",
      "params:tensor([  4.9999, -15.2224])\n",
      "Epoch 1254, Loss 3.307463\n",
      "grad:tensor([-0.0625,  0.3537])\n",
      "params:tensor([  5.0005, -15.2259])\n",
      "Epoch 1255, Loss 3.306170\n",
      "grad:tensor([-0.0624,  0.3531])\n",
      "params:tensor([  5.0011, -15.2295])\n",
      "Epoch 1256, Loss 3.304887\n",
      "grad:tensor([-0.0623,  0.3525])\n",
      "params:tensor([  5.0017, -15.2330])\n",
      "Epoch 1257, Loss 3.303605\n",
      "grad:tensor([-0.0622,  0.3519])\n",
      "params:tensor([  5.0024, -15.2365])\n",
      "Epoch 1258, Loss 3.302329\n",
      "grad:tensor([-0.0620,  0.3514])\n",
      "params:tensor([  5.0030, -15.2400])\n",
      "Epoch 1259, Loss 3.301057\n",
      "grad:tensor([-0.0620,  0.3508])\n",
      "params:tensor([  5.0036, -15.2435])\n",
      "Epoch 1260, Loss 3.299791\n",
      "grad:tensor([-0.0619,  0.3502])\n",
      "params:tensor([  5.0042, -15.2470])\n",
      "Epoch 1261, Loss 3.298528\n",
      "grad:tensor([-0.0618,  0.3496])\n",
      "params:tensor([  5.0048, -15.2505])\n",
      "Epoch 1262, Loss 3.297267\n",
      "grad:tensor([-0.0616,  0.3490])\n",
      "params:tensor([  5.0054, -15.2540])\n",
      "Epoch 1263, Loss 3.296014\n",
      "grad:tensor([-0.0615,  0.3484])\n",
      "params:tensor([  5.0061, -15.2575])\n",
      "Epoch 1264, Loss 3.294762\n",
      "grad:tensor([-0.0614,  0.3478])\n",
      "params:tensor([  5.0067, -15.2610])\n",
      "Epoch 1265, Loss 3.293517\n",
      "grad:tensor([-0.0613,  0.3472])\n",
      "params:tensor([  5.0073, -15.2645])\n",
      "Epoch 1266, Loss 3.292276\n",
      "grad:tensor([-0.0612,  0.3466])\n",
      "params:tensor([  5.0079, -15.2679])\n",
      "Epoch 1267, Loss 3.291036\n",
      "grad:tensor([-0.0611,  0.3460])\n",
      "params:tensor([  5.0085, -15.2714])\n",
      "Epoch 1268, Loss 3.289804\n",
      "grad:tensor([-0.0610,  0.3454])\n",
      "params:tensor([  5.0091, -15.2748])\n",
      "Epoch 1269, Loss 3.288573\n",
      "grad:tensor([-0.0609,  0.3448])\n",
      "params:tensor([  5.0097, -15.2783])\n",
      "Epoch 1270, Loss 3.287347\n",
      "grad:tensor([-0.0608,  0.3443])\n",
      "params:tensor([  5.0103, -15.2817])\n",
      "Epoch 1271, Loss 3.286129\n",
      "grad:tensor([-0.0607,  0.3437])\n",
      "params:tensor([  5.0109, -15.2852])\n",
      "Epoch 1272, Loss 3.284911\n",
      "grad:tensor([-0.0606,  0.3431])\n",
      "params:tensor([  5.0116, -15.2886])\n",
      "Epoch 1273, Loss 3.283698\n",
      "grad:tensor([-0.0605,  0.3425])\n",
      "params:tensor([  5.0122, -15.2920])\n",
      "Epoch 1274, Loss 3.282488\n",
      "grad:tensor([-0.0604,  0.3419])\n",
      "params:tensor([  5.0128, -15.2954])\n",
      "Epoch 1275, Loss 3.281284\n",
      "grad:tensor([-0.0603,  0.3413])\n",
      "params:tensor([  5.0134, -15.2988])\n",
      "Epoch 1276, Loss 3.280085\n",
      "grad:tensor([-0.0602,  0.3408])\n",
      "params:tensor([  5.0140, -15.3023])\n",
      "Epoch 1277, Loss 3.278888\n",
      "grad:tensor([-0.0601,  0.3402])\n",
      "params:tensor([  5.0146, -15.3057])\n",
      "Epoch 1278, Loss 3.277696\n",
      "grad:tensor([-0.0600,  0.3396])\n",
      "params:tensor([  5.0152, -15.3091])\n",
      "Epoch 1279, Loss 3.276506\n",
      "grad:tensor([-0.0599,  0.3390])\n",
      "params:tensor([  5.0158, -15.3124])\n",
      "Epoch 1280, Loss 3.275322\n",
      "grad:tensor([-0.0598,  0.3384])\n",
      "params:tensor([  5.0164, -15.3158])\n",
      "Epoch 1281, Loss 3.274142\n",
      "grad:tensor([-0.0597,  0.3379])\n",
      "params:tensor([  5.0170, -15.3192])\n",
      "Epoch 1282, Loss 3.272968\n",
      "grad:tensor([-0.0596,  0.3373])\n",
      "params:tensor([  5.0176, -15.3226])\n",
      "Epoch 1283, Loss 3.271793\n",
      "grad:tensor([-0.0595,  0.3367])\n",
      "params:tensor([  5.0182, -15.3259])\n",
      "Epoch 1284, Loss 3.270625\n",
      "grad:tensor([-0.0594,  0.3362])\n",
      "params:tensor([  5.0187, -15.3293])\n",
      "Epoch 1285, Loss 3.269460\n",
      "grad:tensor([-0.0593,  0.3356])\n",
      "params:tensor([  5.0193, -15.3327])\n",
      "Epoch 1286, Loss 3.268301\n",
      "grad:tensor([-0.0592,  0.3350])\n",
      "params:tensor([  5.0199, -15.3360])\n",
      "Epoch 1287, Loss 3.267143\n",
      "grad:tensor([-0.0591,  0.3344])\n",
      "params:tensor([  5.0205, -15.3394])\n",
      "Epoch 1288, Loss 3.265991\n",
      "grad:tensor([-0.0590,  0.3339])\n",
      "params:tensor([  5.0211, -15.3427])\n",
      "Epoch 1289, Loss 3.264842\n",
      "grad:tensor([-0.0589,  0.3333])\n",
      "params:tensor([  5.0217, -15.3460])\n",
      "Epoch 1290, Loss 3.263700\n",
      "grad:tensor([-0.0588,  0.3327])\n",
      "params:tensor([  5.0223, -15.3494])\n",
      "Epoch 1291, Loss 3.262556\n",
      "grad:tensor([-0.0587,  0.3322])\n",
      "params:tensor([  5.0229, -15.3527])\n",
      "Epoch 1292, Loss 3.261421\n",
      "grad:tensor([-0.0586,  0.3316])\n",
      "params:tensor([  5.0235, -15.3560])\n",
      "Epoch 1293, Loss 3.260287\n",
      "grad:tensor([-0.0585,  0.3311])\n",
      "params:tensor([  5.0240, -15.3593])\n",
      "Epoch 1294, Loss 3.259160\n",
      "grad:tensor([-0.0584,  0.3305])\n",
      "params:tensor([  5.0246, -15.3626])\n",
      "Epoch 1295, Loss 3.258033\n",
      "grad:tensor([-0.0583,  0.3299])\n",
      "params:tensor([  5.0252, -15.3659])\n",
      "Epoch 1296, Loss 3.256912\n",
      "grad:tensor([-0.0582,  0.3294])\n",
      "params:tensor([  5.0258, -15.3692])\n",
      "Epoch 1297, Loss 3.255795\n",
      "grad:tensor([-0.0581,  0.3288])\n",
      "params:tensor([  5.0264, -15.3725])\n",
      "Epoch 1298, Loss 3.254681\n",
      "grad:tensor([-0.0580,  0.3282])\n",
      "params:tensor([  5.0270, -15.3758])\n",
      "Epoch 1299, Loss 3.253569\n",
      "grad:tensor([-0.0579,  0.3277])\n",
      "params:tensor([  5.0275, -15.3791])\n",
      "Epoch 1300, Loss 3.252462\n",
      "grad:tensor([-0.0578,  0.3271])\n",
      "params:tensor([  5.0281, -15.3823])\n",
      "Epoch 1301, Loss 3.251362\n",
      "grad:tensor([-0.0577,  0.3266])\n",
      "params:tensor([  5.0287, -15.3856])\n",
      "Epoch 1302, Loss 3.250263\n",
      "grad:tensor([-0.0576,  0.3260])\n",
      "params:tensor([  5.0293, -15.3888])\n",
      "Epoch 1303, Loss 3.249168\n",
      "grad:tensor([-0.0575,  0.3255])\n",
      "params:tensor([  5.0298, -15.3921])\n",
      "Epoch 1304, Loss 3.248077\n",
      "grad:tensor([-0.0574,  0.3249])\n",
      "params:tensor([  5.0304, -15.3954])\n",
      "Epoch 1305, Loss 3.246988\n",
      "grad:tensor([-0.0573,  0.3244])\n",
      "params:tensor([  5.0310, -15.3986])\n",
      "Epoch 1306, Loss 3.245904\n",
      "grad:tensor([-0.0572,  0.3238])\n",
      "params:tensor([  5.0316, -15.4018])\n",
      "Epoch 1307, Loss 3.244824\n",
      "grad:tensor([-0.0571,  0.3233])\n",
      "params:tensor([  5.0321, -15.4051])\n",
      "Epoch 1308, Loss 3.243747\n",
      "grad:tensor([-0.0570,  0.3227])\n",
      "params:tensor([  5.0327, -15.4083])\n",
      "Epoch 1309, Loss 3.242674\n",
      "grad:tensor([-0.0569,  0.3222])\n",
      "params:tensor([  5.0333, -15.4115])\n",
      "Epoch 1310, Loss 3.241606\n",
      "grad:tensor([-0.0568,  0.3216])\n",
      "params:tensor([  5.0338, -15.4147])\n",
      "Epoch 1311, Loss 3.240538\n",
      "grad:tensor([-0.0567,  0.3211])\n",
      "params:tensor([  5.0344, -15.4179])\n",
      "Epoch 1312, Loss 3.239475\n",
      "grad:tensor([-0.0566,  0.3205])\n",
      "params:tensor([  5.0350, -15.4211])\n",
      "Epoch 1313, Loss 3.238419\n",
      "grad:tensor([-0.0565,  0.3200])\n",
      "params:tensor([  5.0355, -15.4243])\n",
      "Epoch 1314, Loss 3.237363\n",
      "grad:tensor([-0.0564,  0.3194])\n",
      "params:tensor([  5.0361, -15.4275])\n",
      "Epoch 1315, Loss 3.236314\n",
      "grad:tensor([-0.0563,  0.3189])\n",
      "params:tensor([  5.0367, -15.4307])\n",
      "Epoch 1316, Loss 3.235265\n",
      "grad:tensor([-0.0562,  0.3184])\n",
      "params:tensor([  5.0372, -15.4339])\n",
      "Epoch 1317, Loss 3.234218\n",
      "grad:tensor([-0.0561,  0.3178])\n",
      "params:tensor([  5.0378, -15.4371])\n",
      "Epoch 1318, Loss 3.233179\n",
      "grad:tensor([-0.0561,  0.3173])\n",
      "params:tensor([  5.0383, -15.4403])\n",
      "Epoch 1319, Loss 3.232143\n",
      "grad:tensor([-0.0560,  0.3167])\n",
      "params:tensor([  5.0389, -15.4434])\n",
      "Epoch 1320, Loss 3.231109\n",
      "grad:tensor([-0.0558,  0.3162])\n",
      "params:tensor([  5.0395, -15.4466])\n",
      "Epoch 1321, Loss 3.230078\n",
      "grad:tensor([-0.0558,  0.3157])\n",
      "params:tensor([  5.0400, -15.4498])\n",
      "Epoch 1322, Loss 3.229051\n",
      "grad:tensor([-0.0557,  0.3151])\n",
      "params:tensor([  5.0406, -15.4529])\n",
      "Epoch 1323, Loss 3.228027\n",
      "grad:tensor([-0.0556,  0.3146])\n",
      "params:tensor([  5.0411, -15.4560])\n",
      "Epoch 1324, Loss 3.227010\n",
      "grad:tensor([-0.0555,  0.3141])\n",
      "params:tensor([  5.0417, -15.4592])\n",
      "Epoch 1325, Loss 3.225992\n",
      "grad:tensor([-0.0554,  0.3135])\n",
      "params:tensor([  5.0422, -15.4623])\n",
      "Epoch 1326, Loss 3.224979\n",
      "grad:tensor([-0.0553,  0.3130])\n",
      "params:tensor([  5.0428, -15.4655])\n",
      "Epoch 1327, Loss 3.223971\n",
      "grad:tensor([-0.0552,  0.3125])\n",
      "params:tensor([  5.0433, -15.4686])\n",
      "Epoch 1328, Loss 3.222965\n",
      "grad:tensor([-0.0551,  0.3119])\n",
      "params:tensor([  5.0439, -15.4717])\n",
      "Epoch 1329, Loss 3.221960\n",
      "grad:tensor([-0.0550,  0.3114])\n",
      "params:tensor([  5.0444, -15.4748])\n",
      "Epoch 1330, Loss 3.220962\n",
      "grad:tensor([-0.0549,  0.3109])\n",
      "params:tensor([  5.0450, -15.4779])\n",
      "Epoch 1331, Loss 3.219967\n",
      "grad:tensor([-0.0548,  0.3103])\n",
      "params:tensor([  5.0455, -15.4810])\n",
      "Epoch 1332, Loss 3.218975\n",
      "grad:tensor([-0.0547,  0.3098])\n",
      "params:tensor([  5.0461, -15.4841])\n",
      "Epoch 1333, Loss 3.217986\n",
      "grad:tensor([-0.0546,  0.3093])\n",
      "params:tensor([  5.0466, -15.4872])\n",
      "Epoch 1334, Loss 3.217000\n",
      "grad:tensor([-0.0545,  0.3088])\n",
      "params:tensor([  5.0472, -15.4903])\n",
      "Epoch 1335, Loss 3.216017\n",
      "grad:tensor([-0.0544,  0.3082])\n",
      "params:tensor([  5.0477, -15.4934])\n",
      "Epoch 1336, Loss 3.215038\n",
      "grad:tensor([-0.0543,  0.3077])\n",
      "params:tensor([  5.0483, -15.4965])\n",
      "Epoch 1337, Loss 3.214062\n",
      "grad:tensor([-0.0543,  0.3072])\n",
      "params:tensor([  5.0488, -15.4995])\n",
      "Epoch 1338, Loss 3.213092\n",
      "grad:tensor([-0.0542,  0.3067])\n",
      "params:tensor([  5.0494, -15.5026])\n",
      "Epoch 1339, Loss 3.212122\n",
      "grad:tensor([-0.0541,  0.3061])\n",
      "params:tensor([  5.0499, -15.5057])\n",
      "Epoch 1340, Loss 3.211157\n",
      "grad:tensor([-0.0540,  0.3056])\n",
      "params:tensor([  5.0504, -15.5087])\n",
      "Epoch 1341, Loss 3.210192\n",
      "grad:tensor([-0.0539,  0.3051])\n",
      "params:tensor([  5.0510, -15.5118])\n",
      "Epoch 1342, Loss 3.209235\n",
      "grad:tensor([-0.0538,  0.3046])\n",
      "params:tensor([  5.0515, -15.5148])\n",
      "Epoch 1343, Loss 3.208279\n",
      "grad:tensor([-0.0537,  0.3041])\n",
      "params:tensor([  5.0521, -15.5179])\n",
      "Epoch 1344, Loss 3.207326\n",
      "grad:tensor([-0.0536,  0.3036])\n",
      "params:tensor([  5.0526, -15.5209])\n",
      "Epoch 1345, Loss 3.206376\n",
      "grad:tensor([-0.0535,  0.3030])\n",
      "params:tensor([  5.0531, -15.5239])\n",
      "Epoch 1346, Loss 3.205430\n",
      "grad:tensor([-0.0534,  0.3025])\n",
      "params:tensor([  5.0537, -15.5269])\n",
      "Epoch 1347, Loss 3.204488\n",
      "grad:tensor([-0.0533,  0.3020])\n",
      "params:tensor([  5.0542, -15.5300])\n",
      "Epoch 1348, Loss 3.203547\n",
      "grad:tensor([-0.0532,  0.3015])\n",
      "params:tensor([  5.0547, -15.5330])\n",
      "Epoch 1349, Loss 3.202610\n",
      "grad:tensor([-0.0532,  0.3010])\n",
      "params:tensor([  5.0553, -15.5360])\n",
      "Epoch 1350, Loss 3.201678\n",
      "grad:tensor([-0.0531,  0.3005])\n",
      "params:tensor([  5.0558, -15.5390])\n",
      "Epoch 1351, Loss 3.200747\n",
      "grad:tensor([-0.0530,  0.3000])\n",
      "params:tensor([  5.0563, -15.5420])\n",
      "Epoch 1352, Loss 3.199820\n",
      "grad:tensor([-0.0529,  0.2995])\n",
      "params:tensor([  5.0568, -15.5450])\n",
      "Epoch 1353, Loss 3.198897\n",
      "grad:tensor([-0.0528,  0.2989])\n",
      "params:tensor([  5.0574, -15.5480])\n",
      "Epoch 1354, Loss 3.197976\n",
      "grad:tensor([-0.0527,  0.2984])\n",
      "params:tensor([  5.0579, -15.5510])\n",
      "Epoch 1355, Loss 3.197060\n",
      "grad:tensor([-0.0526,  0.2979])\n",
      "params:tensor([  5.0584, -15.5539])\n",
      "Epoch 1356, Loss 3.196143\n",
      "grad:tensor([-0.0525,  0.2974])\n",
      "params:tensor([  5.0590, -15.5569])\n",
      "Epoch 1357, Loss 3.195231\n",
      "grad:tensor([-0.0524,  0.2969])\n",
      "params:tensor([  5.0595, -15.5599])\n",
      "Epoch 1358, Loss 3.194324\n",
      "grad:tensor([-0.0524,  0.2964])\n",
      "params:tensor([  5.0600, -15.5629])\n",
      "Epoch 1359, Loss 3.193420\n",
      "grad:tensor([-0.0523,  0.2959])\n",
      "params:tensor([  5.0605, -15.5658])\n",
      "Epoch 1360, Loss 3.192517\n",
      "grad:tensor([-0.0522,  0.2954])\n",
      "params:tensor([  5.0610, -15.5688])\n",
      "Epoch 1361, Loss 3.191616\n",
      "grad:tensor([-0.0521,  0.2949])\n",
      "params:tensor([  5.0616, -15.5717])\n",
      "Epoch 1362, Loss 3.190720\n",
      "grad:tensor([-0.0520,  0.2944])\n",
      "params:tensor([  5.0621, -15.5747])\n",
      "Epoch 1363, Loss 3.189829\n",
      "grad:tensor([-0.0519,  0.2939])\n",
      "params:tensor([  5.0626, -15.5776])\n",
      "Epoch 1364, Loss 3.188938\n",
      "grad:tensor([-0.0518,  0.2934])\n",
      "params:tensor([  5.0631, -15.5805])\n",
      "Epoch 1365, Loss 3.188051\n",
      "grad:tensor([-0.0517,  0.2929])\n",
      "params:tensor([  5.0636, -15.5835])\n",
      "Epoch 1366, Loss 3.187166\n",
      "grad:tensor([-0.0516,  0.2924])\n",
      "params:tensor([  5.0642, -15.5864])\n",
      "Epoch 1367, Loss 3.186287\n",
      "grad:tensor([-0.0516,  0.2919])\n",
      "params:tensor([  5.0647, -15.5893])\n",
      "Epoch 1368, Loss 3.185409\n",
      "grad:tensor([-0.0515,  0.2914])\n",
      "params:tensor([  5.0652, -15.5922])\n",
      "Epoch 1369, Loss 3.184534\n",
      "grad:tensor([-0.0514,  0.2909])\n",
      "params:tensor([  5.0657, -15.5951])\n",
      "Epoch 1370, Loss 3.183662\n",
      "grad:tensor([-0.0513,  0.2904])\n",
      "params:tensor([  5.0662, -15.5980])\n",
      "Epoch 1371, Loss 3.182792\n",
      "grad:tensor([-0.0512,  0.2899])\n",
      "params:tensor([  5.0667, -15.6009])\n",
      "Epoch 1372, Loss 3.181925\n",
      "grad:tensor([-0.0511,  0.2894])\n",
      "params:tensor([  5.0672, -15.6038])\n",
      "Epoch 1373, Loss 3.181063\n",
      "grad:tensor([-0.0510,  0.2890])\n",
      "params:tensor([  5.0678, -15.6067])\n",
      "Epoch 1374, Loss 3.180201\n",
      "grad:tensor([-0.0509,  0.2885])\n",
      "params:tensor([  5.0683, -15.6096])\n",
      "Epoch 1375, Loss 3.179347\n",
      "grad:tensor([-0.0509,  0.2880])\n",
      "params:tensor([  5.0688, -15.6125])\n",
      "Epoch 1376, Loss 3.178490\n",
      "grad:tensor([-0.0508,  0.2875])\n",
      "params:tensor([  5.0693, -15.6154])\n",
      "Epoch 1377, Loss 3.177638\n",
      "grad:tensor([-0.0507,  0.2870])\n",
      "params:tensor([  5.0698, -15.6182])\n",
      "Epoch 1378, Loss 3.176789\n",
      "grad:tensor([-0.0506,  0.2865])\n",
      "params:tensor([  5.0703, -15.6211])\n",
      "Epoch 1379, Loss 3.175945\n",
      "grad:tensor([-0.0505,  0.2860])\n",
      "params:tensor([  5.0708, -15.6240])\n",
      "Epoch 1380, Loss 3.175101\n",
      "grad:tensor([-0.0504,  0.2855])\n",
      "params:tensor([  5.0713, -15.6268])\n",
      "Epoch 1381, Loss 3.174262\n",
      "grad:tensor([-0.0504,  0.2850])\n",
      "params:tensor([  5.0718, -15.6297])\n",
      "Epoch 1382, Loss 3.173425\n",
      "grad:tensor([-0.0503,  0.2846])\n",
      "params:tensor([  5.0723, -15.6325])\n",
      "Epoch 1383, Loss 3.172590\n",
      "grad:tensor([-0.0502,  0.2841])\n",
      "params:tensor([  5.0728, -15.6353])\n",
      "Epoch 1384, Loss 3.171759\n",
      "grad:tensor([-0.0501,  0.2836])\n",
      "params:tensor([  5.0733, -15.6382])\n",
      "Epoch 1385, Loss 3.170929\n",
      "grad:tensor([-0.0500,  0.2831])\n",
      "params:tensor([  5.0738, -15.6410])\n",
      "Epoch 1386, Loss 3.170103\n",
      "grad:tensor([-0.0499,  0.2826])\n",
      "params:tensor([  5.0743, -15.6438])\n",
      "Epoch 1387, Loss 3.169280\n",
      "grad:tensor([-0.0498,  0.2822])\n",
      "params:tensor([  5.0748, -15.6467])\n",
      "Epoch 1388, Loss 3.168462\n",
      "grad:tensor([-0.0498,  0.2817])\n",
      "params:tensor([  5.0753, -15.6495])\n",
      "Epoch 1389, Loss 3.167644\n",
      "grad:tensor([-0.0497,  0.2812])\n",
      "params:tensor([  5.0758, -15.6523])\n",
      "Epoch 1390, Loss 3.166827\n",
      "grad:tensor([-0.0496,  0.2807])\n",
      "params:tensor([  5.0763, -15.6551])\n",
      "Epoch 1391, Loss 3.166017\n",
      "grad:tensor([-0.0495,  0.2802])\n",
      "params:tensor([  5.0768, -15.6579])\n",
      "Epoch 1392, Loss 3.165207\n",
      "grad:tensor([-0.0494,  0.2798])\n",
      "params:tensor([  5.0773, -15.6607])\n",
      "Epoch 1393, Loss 3.164401\n",
      "grad:tensor([-0.0493,  0.2793])\n",
      "params:tensor([  5.0778, -15.6635])\n",
      "Epoch 1394, Loss 3.163594\n",
      "grad:tensor([-0.0492,  0.2788])\n",
      "params:tensor([  5.0783, -15.6663])\n",
      "Epoch 1395, Loss 3.162795\n",
      "grad:tensor([-0.0492,  0.2783])\n",
      "params:tensor([  5.0788, -15.6691])\n",
      "Epoch 1396, Loss 3.161996\n",
      "grad:tensor([-0.0491,  0.2779])\n",
      "params:tensor([  5.0793, -15.6718])\n",
      "Epoch 1397, Loss 3.161201\n",
      "grad:tensor([-0.0490,  0.2774])\n",
      "params:tensor([  5.0797, -15.6746])\n",
      "Epoch 1398, Loss 3.160410\n",
      "grad:tensor([-0.0489,  0.2769])\n",
      "params:tensor([  5.0802, -15.6774])\n",
      "Epoch 1399, Loss 3.159618\n",
      "grad:tensor([-0.0488,  0.2765])\n",
      "params:tensor([  5.0807, -15.6802])\n",
      "Epoch 1400, Loss 3.158830\n",
      "grad:tensor([-0.0488,  0.2760])\n",
      "params:tensor([  5.0812, -15.6829])\n",
      "Epoch 1401, Loss 3.158046\n",
      "grad:tensor([-0.0487,  0.2755])\n",
      "params:tensor([  5.0817, -15.6857])\n",
      "Epoch 1402, Loss 3.157263\n",
      "grad:tensor([-0.0486,  0.2751])\n",
      "params:tensor([  5.0822, -15.6884])\n",
      "Epoch 1403, Loss 3.156484\n",
      "grad:tensor([-0.0485,  0.2746])\n",
      "params:tensor([  5.0827, -15.6912])\n",
      "Epoch 1404, Loss 3.155708\n",
      "grad:tensor([-0.0484,  0.2741])\n",
      "params:tensor([  5.0832, -15.6939])\n",
      "Epoch 1405, Loss 3.154933\n",
      "grad:tensor([-0.0483,  0.2736])\n",
      "params:tensor([  5.0836, -15.6966])\n",
      "Epoch 1406, Loss 3.154162\n",
      "grad:tensor([-0.0483,  0.2732])\n",
      "params:tensor([  5.0841, -15.6994])\n",
      "Epoch 1407, Loss 3.153393\n",
      "grad:tensor([-0.0482,  0.2727])\n",
      "params:tensor([  5.0846, -15.7021])\n",
      "Epoch 1408, Loss 3.152628\n",
      "grad:tensor([-0.0481,  0.2723])\n",
      "params:tensor([  5.0851, -15.7048])\n",
      "Epoch 1409, Loss 3.151865\n",
      "grad:tensor([-0.0480,  0.2718])\n",
      "params:tensor([  5.0856, -15.7075])\n",
      "Epoch 1410, Loss 3.151101\n",
      "grad:tensor([-0.0479,  0.2713])\n",
      "params:tensor([  5.0860, -15.7103])\n",
      "Epoch 1411, Loss 3.150343\n",
      "grad:tensor([-0.0479,  0.2709])\n",
      "params:tensor([  5.0865, -15.7130])\n",
      "Epoch 1412, Loss 3.149587\n",
      "grad:tensor([-0.0478,  0.2704])\n",
      "params:tensor([  5.0870, -15.7157])\n",
      "Epoch 1413, Loss 3.148833\n",
      "grad:tensor([-0.0477,  0.2700])\n",
      "params:tensor([  5.0875, -15.7184])\n",
      "Epoch 1414, Loss 3.148083\n",
      "grad:tensor([-0.0476,  0.2695])\n",
      "params:tensor([  5.0879, -15.7211])\n",
      "Epoch 1415, Loss 3.147335\n",
      "grad:tensor([-0.0475,  0.2690])\n",
      "params:tensor([  5.0884, -15.7238])\n",
      "Epoch 1416, Loss 3.146588\n",
      "grad:tensor([-0.0474,  0.2686])\n",
      "params:tensor([  5.0889, -15.7264])\n",
      "Epoch 1417, Loss 3.145845\n",
      "grad:tensor([-0.0474,  0.2681])\n",
      "params:tensor([  5.0894, -15.7291])\n",
      "Epoch 1418, Loss 3.145105\n",
      "grad:tensor([-0.0473,  0.2677])\n",
      "params:tensor([  5.0898, -15.7318])\n",
      "Epoch 1419, Loss 3.144367\n",
      "grad:tensor([-0.0472,  0.2672])\n",
      "params:tensor([  5.0903, -15.7345])\n",
      "Epoch 1420, Loss 3.143630\n",
      "grad:tensor([-0.0471,  0.2668])\n",
      "params:tensor([  5.0908, -15.7371])\n",
      "Epoch 1421, Loss 3.142899\n",
      "grad:tensor([-0.0470,  0.2663])\n",
      "params:tensor([  5.0913, -15.7398])\n",
      "Epoch 1422, Loss 3.142166\n",
      "grad:tensor([-0.0469,  0.2659])\n",
      "params:tensor([  5.0917, -15.7425])\n",
      "Epoch 1423, Loss 3.141439\n",
      "grad:tensor([-0.0469,  0.2654])\n",
      "params:tensor([  5.0922, -15.7451])\n",
      "Epoch 1424, Loss 3.140712\n",
      "grad:tensor([-0.0468,  0.2649])\n",
      "params:tensor([  5.0927, -15.7478])\n",
      "Epoch 1425, Loss 3.139989\n",
      "grad:tensor([-0.0467,  0.2645])\n",
      "params:tensor([  5.0931, -15.7504])\n",
      "Epoch 1426, Loss 3.139271\n",
      "grad:tensor([-0.0466,  0.2641])\n",
      "params:tensor([  5.0936, -15.7530])\n",
      "Epoch 1427, Loss 3.138551\n",
      "grad:tensor([-0.0466,  0.2636])\n",
      "params:tensor([  5.0941, -15.7557])\n",
      "Epoch 1428, Loss 3.137835\n",
      "grad:tensor([-0.0465,  0.2632])\n",
      "params:tensor([  5.0945, -15.7583])\n",
      "Epoch 1429, Loss 3.137121\n",
      "grad:tensor([-0.0464,  0.2627])\n",
      "params:tensor([  5.0950, -15.7609])\n",
      "Epoch 1430, Loss 3.136409\n",
      "grad:tensor([-0.0463,  0.2623])\n",
      "params:tensor([  5.0955, -15.7636])\n",
      "Epoch 1431, Loss 3.135702\n",
      "grad:tensor([-0.0462,  0.2618])\n",
      "params:tensor([  5.0959, -15.7662])\n",
      "Epoch 1432, Loss 3.134994\n",
      "grad:tensor([-0.0461,  0.2614])\n",
      "params:tensor([  5.0964, -15.7688])\n",
      "Epoch 1433, Loss 3.134292\n",
      "grad:tensor([-0.0461,  0.2609])\n",
      "params:tensor([  5.0968, -15.7714])\n",
      "Epoch 1434, Loss 3.133590\n",
      "grad:tensor([-0.0460,  0.2605])\n",
      "params:tensor([  5.0973, -15.7740])\n",
      "Epoch 1435, Loss 3.132889\n",
      "grad:tensor([-0.0459,  0.2600])\n",
      "params:tensor([  5.0978, -15.7766])\n",
      "Epoch 1436, Loss 3.132194\n",
      "grad:tensor([-0.0459,  0.2596])\n",
      "params:tensor([  5.0982, -15.7792])\n",
      "Epoch 1437, Loss 3.131500\n",
      "grad:tensor([-0.0458,  0.2592])\n",
      "params:tensor([  5.0987, -15.7818])\n",
      "Epoch 1438, Loss 3.130810\n",
      "grad:tensor([-0.0457,  0.2587])\n",
      "params:tensor([  5.0991, -15.7844])\n",
      "Epoch 1439, Loss 3.130119\n",
      "grad:tensor([-0.0456,  0.2583])\n",
      "params:tensor([  5.0996, -15.7870])\n",
      "Epoch 1440, Loss 3.129432\n",
      "grad:tensor([-0.0455,  0.2578])\n",
      "params:tensor([  5.1000, -15.7895])\n",
      "Epoch 1441, Loss 3.128746\n",
      "grad:tensor([-0.0455,  0.2574])\n",
      "params:tensor([  5.1005, -15.7921])\n",
      "Epoch 1442, Loss 3.128064\n",
      "grad:tensor([-0.0454,  0.2570])\n",
      "params:tensor([  5.1010, -15.7947])\n",
      "Epoch 1443, Loss 3.127382\n",
      "grad:tensor([-0.0453,  0.2565])\n",
      "params:tensor([  5.1014, -15.7973])\n",
      "Epoch 1444, Loss 3.126705\n",
      "grad:tensor([-0.0453,  0.2561])\n",
      "params:tensor([  5.1019, -15.7998])\n",
      "Epoch 1445, Loss 3.126030\n",
      "grad:tensor([-0.0452,  0.2557])\n",
      "params:tensor([  5.1023, -15.8024])\n",
      "Epoch 1446, Loss 3.125356\n",
      "grad:tensor([-0.0451,  0.2552])\n",
      "params:tensor([  5.1028, -15.8049])\n",
      "Epoch 1447, Loss 3.124683\n",
      "grad:tensor([-0.0450,  0.2548])\n",
      "params:tensor([  5.1032, -15.8075])\n",
      "Epoch 1448, Loss 3.124016\n",
      "grad:tensor([-0.0449,  0.2544])\n",
      "params:tensor([  5.1037, -15.8100])\n",
      "Epoch 1449, Loss 3.123349\n",
      "grad:tensor([-0.0449,  0.2539])\n",
      "params:tensor([  5.1041, -15.8126])\n",
      "Epoch 1450, Loss 3.122686\n",
      "grad:tensor([-0.0448,  0.2535])\n",
      "params:tensor([  5.1046, -15.8151])\n",
      "Epoch 1451, Loss 3.122022\n",
      "grad:tensor([-0.0447,  0.2531])\n",
      "params:tensor([  5.1050, -15.8176])\n",
      "Epoch 1452, Loss 3.121362\n",
      "grad:tensor([-0.0446,  0.2526])\n",
      "params:tensor([  5.1055, -15.8201])\n",
      "Epoch 1453, Loss 3.120707\n",
      "grad:tensor([-0.0445,  0.2522])\n",
      "params:tensor([  5.1059, -15.8227])\n",
      "Epoch 1454, Loss 3.120049\n",
      "grad:tensor([-0.0445,  0.2518])\n",
      "params:tensor([  5.1063, -15.8252])\n",
      "Epoch 1455, Loss 3.119397\n",
      "grad:tensor([-0.0444,  0.2513])\n",
      "params:tensor([  5.1068, -15.8277])\n",
      "Epoch 1456, Loss 3.118746\n",
      "grad:tensor([-0.0443,  0.2509])\n",
      "params:tensor([  5.1072, -15.8302])\n",
      "Epoch 1457, Loss 3.118098\n",
      "grad:tensor([-0.0442,  0.2505])\n",
      "params:tensor([  5.1077, -15.8327])\n",
      "Epoch 1458, Loss 3.117451\n",
      "grad:tensor([-0.0442,  0.2501])\n",
      "params:tensor([  5.1081, -15.8352])\n",
      "Epoch 1459, Loss 3.116805\n",
      "grad:tensor([-0.0441,  0.2496])\n",
      "params:tensor([  5.1086, -15.8377])\n",
      "Epoch 1460, Loss 3.116164\n",
      "grad:tensor([-0.0440,  0.2492])\n",
      "params:tensor([  5.1090, -15.8402])\n",
      "Epoch 1461, Loss 3.115525\n",
      "grad:tensor([-0.0439,  0.2488])\n",
      "params:tensor([  5.1094, -15.8427])\n",
      "Epoch 1462, Loss 3.114886\n",
      "grad:tensor([-0.0439,  0.2484])\n",
      "params:tensor([  5.1099, -15.8452])\n",
      "Epoch 1463, Loss 3.114251\n",
      "grad:tensor([-0.0438,  0.2480])\n",
      "params:tensor([  5.1103, -15.8477])\n",
      "Epoch 1464, Loss 3.113617\n",
      "grad:tensor([-0.0437,  0.2475])\n",
      "params:tensor([  5.1107, -15.8501])\n",
      "Epoch 1465, Loss 3.112985\n",
      "grad:tensor([-0.0437,  0.2471])\n",
      "params:tensor([  5.1112, -15.8526])\n",
      "Epoch 1466, Loss 3.112358\n",
      "grad:tensor([-0.0436,  0.2467])\n",
      "params:tensor([  5.1116, -15.8551])\n",
      "Epoch 1467, Loss 3.111731\n",
      "grad:tensor([-0.0435,  0.2463])\n",
      "params:tensor([  5.1121, -15.8575])\n",
      "Epoch 1468, Loss 3.111103\n",
      "grad:tensor([-0.0434,  0.2459])\n",
      "params:tensor([  5.1125, -15.8600])\n",
      "Epoch 1469, Loss 3.110484\n",
      "grad:tensor([-0.0433,  0.2454])\n",
      "params:tensor([  5.1129, -15.8624])\n",
      "Epoch 1470, Loss 3.109859\n",
      "grad:tensor([-0.0433,  0.2450])\n",
      "params:tensor([  5.1134, -15.8649])\n",
      "Epoch 1471, Loss 3.109243\n",
      "grad:tensor([-0.0432,  0.2446])\n",
      "params:tensor([  5.1138, -15.8673])\n",
      "Epoch 1472, Loss 3.108627\n",
      "grad:tensor([-0.0431,  0.2442])\n",
      "params:tensor([  5.1142, -15.8698])\n",
      "Epoch 1473, Loss 3.108011\n",
      "grad:tensor([-0.0430,  0.2438])\n",
      "params:tensor([  5.1147, -15.8722])\n",
      "Epoch 1474, Loss 3.107401\n",
      "grad:tensor([-0.0430,  0.2434])\n",
      "params:tensor([  5.1151, -15.8747])\n",
      "Epoch 1475, Loss 3.106791\n",
      "grad:tensor([-0.0429,  0.2429])\n",
      "params:tensor([  5.1155, -15.8771])\n",
      "Epoch 1476, Loss 3.106180\n",
      "grad:tensor([-0.0428,  0.2425])\n",
      "params:tensor([  5.1159, -15.8795])\n",
      "Epoch 1477, Loss 3.105575\n",
      "grad:tensor([-0.0428,  0.2421])\n",
      "params:tensor([  5.1164, -15.8819])\n",
      "Epoch 1478, Loss 3.104972\n",
      "grad:tensor([-0.0427,  0.2417])\n",
      "params:tensor([  5.1168, -15.8843])\n",
      "Epoch 1479, Loss 3.104370\n",
      "grad:tensor([-0.0426,  0.2413])\n",
      "params:tensor([  5.1172, -15.8868])\n",
      "Epoch 1480, Loss 3.103770\n",
      "grad:tensor([-0.0425,  0.2409])\n",
      "params:tensor([  5.1176, -15.8892])\n",
      "Epoch 1481, Loss 3.103172\n",
      "grad:tensor([-0.0425,  0.2405])\n",
      "params:tensor([  5.1181, -15.8916])\n",
      "Epoch 1482, Loss 3.102576\n",
      "grad:tensor([-0.0424,  0.2401])\n",
      "params:tensor([  5.1185, -15.8940])\n",
      "Epoch 1483, Loss 3.101982\n",
      "grad:tensor([-0.0423,  0.2397])\n",
      "params:tensor([  5.1189, -15.8964])\n",
      "Epoch 1484, Loss 3.101390\n",
      "grad:tensor([-0.0423,  0.2393])\n",
      "params:tensor([  5.1193, -15.8988])\n",
      "Epoch 1485, Loss 3.100802\n",
      "grad:tensor([-0.0422,  0.2388])\n",
      "params:tensor([  5.1198, -15.9011])\n",
      "Epoch 1486, Loss 3.100213\n",
      "grad:tensor([-0.0421,  0.2384])\n",
      "params:tensor([  5.1202, -15.9035])\n",
      "Epoch 1487, Loss 3.099627\n",
      "grad:tensor([-0.0421,  0.2380])\n",
      "params:tensor([  5.1206, -15.9059])\n",
      "Epoch 1488, Loss 3.099044\n",
      "grad:tensor([-0.0420,  0.2376])\n",
      "params:tensor([  5.1210, -15.9083])\n",
      "Epoch 1489, Loss 3.098463\n",
      "grad:tensor([-0.0419,  0.2372])\n",
      "params:tensor([  5.1214, -15.9107])\n",
      "Epoch 1490, Loss 3.097883\n",
      "grad:tensor([-0.0418,  0.2368])\n",
      "params:tensor([  5.1219, -15.9130])\n",
      "Epoch 1491, Loss 3.097302\n",
      "grad:tensor([-0.0418,  0.2364])\n",
      "params:tensor([  5.1223, -15.9154])\n",
      "Epoch 1492, Loss 3.096727\n",
      "grad:tensor([-0.0417,  0.2360])\n",
      "params:tensor([  5.1227, -15.9178])\n",
      "Epoch 1493, Loss 3.096153\n",
      "grad:tensor([-0.0416,  0.2356])\n",
      "params:tensor([  5.1231, -15.9201])\n",
      "Epoch 1494, Loss 3.095583\n",
      "grad:tensor([-0.0416,  0.2352])\n",
      "params:tensor([  5.1235, -15.9225])\n",
      "Epoch 1495, Loss 3.095011\n",
      "grad:tensor([-0.0415,  0.2348])\n",
      "params:tensor([  5.1239, -15.9248])\n",
      "Epoch 1496, Loss 3.094444\n",
      "grad:tensor([-0.0414,  0.2344])\n",
      "params:tensor([  5.1244, -15.9272])\n",
      "Epoch 1497, Loss 3.093877\n",
      "grad:tensor([-0.0413,  0.2340])\n",
      "params:tensor([  5.1248, -15.9295])\n",
      "Epoch 1498, Loss 3.093314\n",
      "grad:tensor([-0.0413,  0.2336])\n",
      "params:tensor([  5.1252, -15.9318])\n",
      "Epoch 1499, Loss 3.092751\n",
      "grad:tensor([-0.0412,  0.2332])\n",
      "params:tensor([  5.1256, -15.9342])\n",
      "Epoch 1500, Loss 3.092191\n",
      "grad:tensor([-0.0411,  0.2328])\n",
      "params:tensor([  5.1260, -15.9365])\n",
      "Epoch 1501, Loss 3.091630\n",
      "grad:tensor([-0.0411,  0.2324])\n",
      "params:tensor([  5.1264, -15.9388])\n",
      "Epoch 1502, Loss 3.091074\n",
      "grad:tensor([-0.0410,  0.2320])\n",
      "params:tensor([  5.1268, -15.9411])\n",
      "Epoch 1503, Loss 3.090520\n",
      "grad:tensor([-0.0409,  0.2317])\n",
      "params:tensor([  5.1272, -15.9435])\n",
      "Epoch 1504, Loss 3.089969\n",
      "grad:tensor([-0.0408,  0.2313])\n",
      "params:tensor([  5.1276, -15.9458])\n",
      "Epoch 1505, Loss 3.089417\n",
      "grad:tensor([-0.0408,  0.2309])\n",
      "params:tensor([  5.1281, -15.9481])\n",
      "Epoch 1506, Loss 3.088867\n",
      "grad:tensor([-0.0407,  0.2305])\n",
      "params:tensor([  5.1285, -15.9504])\n",
      "Epoch 1507, Loss 3.088320\n",
      "grad:tensor([-0.0406,  0.2301])\n",
      "params:tensor([  5.1289, -15.9527])\n",
      "Epoch 1508, Loss 3.087775\n",
      "grad:tensor([-0.0406,  0.2297])\n",
      "params:tensor([  5.1293, -15.9550])\n",
      "Epoch 1509, Loss 3.087232\n",
      "grad:tensor([-0.0405,  0.2293])\n",
      "params:tensor([  5.1297, -15.9573])\n",
      "Epoch 1510, Loss 3.086690\n",
      "grad:tensor([-0.0404,  0.2289])\n",
      "params:tensor([  5.1301, -15.9596])\n",
      "Epoch 1511, Loss 3.086150\n",
      "grad:tensor([-0.0404,  0.2285])\n",
      "params:tensor([  5.1305, -15.9618])\n",
      "Epoch 1512, Loss 3.085612\n",
      "grad:tensor([-0.0403,  0.2281])\n",
      "params:tensor([  5.1309, -15.9641])\n",
      "Epoch 1513, Loss 3.085075\n",
      "grad:tensor([-0.0402,  0.2277])\n",
      "params:tensor([  5.1313, -15.9664])\n",
      "Epoch 1514, Loss 3.084542\n",
      "grad:tensor([-0.0402,  0.2274])\n",
      "params:tensor([  5.1317, -15.9687])\n",
      "Epoch 1515, Loss 3.084009\n",
      "grad:tensor([-0.0401,  0.2270])\n",
      "params:tensor([  5.1321, -15.9709])\n",
      "Epoch 1516, Loss 3.083478\n",
      "grad:tensor([-0.0400,  0.2266])\n",
      "params:tensor([  5.1325, -15.9732])\n",
      "Epoch 1517, Loss 3.082948\n",
      "grad:tensor([-0.0400,  0.2262])\n",
      "params:tensor([  5.1329, -15.9755])\n",
      "Epoch 1518, Loss 3.082422\n",
      "grad:tensor([-0.0399,  0.2258])\n",
      "params:tensor([  5.1333, -15.9777])\n",
      "Epoch 1519, Loss 3.081897\n",
      "grad:tensor([-0.0398,  0.2254])\n",
      "params:tensor([  5.1337, -15.9800])\n",
      "Epoch 1520, Loss 3.081373\n",
      "grad:tensor([-0.0398,  0.2250])\n",
      "params:tensor([  5.1341, -15.9822])\n",
      "Epoch 1521, Loss 3.080850\n",
      "grad:tensor([-0.0397,  0.2247])\n",
      "params:tensor([  5.1345, -15.9845])\n",
      "Epoch 1522, Loss 3.080331\n",
      "grad:tensor([-0.0396,  0.2243])\n",
      "params:tensor([  5.1349, -15.9867])\n",
      "Epoch 1523, Loss 3.079811\n",
      "grad:tensor([-0.0396,  0.2239])\n",
      "params:tensor([  5.1353, -15.9890])\n",
      "Epoch 1524, Loss 3.079296\n",
      "grad:tensor([-0.0395,  0.2235])\n",
      "params:tensor([  5.1357, -15.9912])\n",
      "Epoch 1525, Loss 3.078781\n",
      "grad:tensor([-0.0394,  0.2231])\n",
      "params:tensor([  5.1361, -15.9934])\n",
      "Epoch 1526, Loss 3.078268\n",
      "grad:tensor([-0.0394,  0.2228])\n",
      "params:tensor([  5.1365, -15.9957])\n",
      "Epoch 1527, Loss 3.077758\n",
      "grad:tensor([-0.0393,  0.2224])\n",
      "params:tensor([  5.1369, -15.9979])\n",
      "Epoch 1528, Loss 3.077248\n",
      "grad:tensor([-0.0392,  0.2220])\n",
      "params:tensor([  5.1372, -16.0001])\n",
      "Epoch 1529, Loss 3.076739\n",
      "grad:tensor([-0.0391,  0.2216])\n",
      "params:tensor([  5.1376, -16.0023])\n",
      "Epoch 1530, Loss 3.076232\n",
      "grad:tensor([-0.0391,  0.2213])\n",
      "params:tensor([  5.1380, -16.0045])\n",
      "Epoch 1531, Loss 3.075729\n",
      "grad:tensor([-0.0390,  0.2209])\n",
      "params:tensor([  5.1384, -16.0067])\n",
      "Epoch 1532, Loss 3.075225\n",
      "grad:tensor([-0.0390,  0.2205])\n",
      "params:tensor([  5.1388, -16.0089])\n",
      "Epoch 1533, Loss 3.074724\n",
      "grad:tensor([-0.0389,  0.2201])\n",
      "params:tensor([  5.1392, -16.0111])\n",
      "Epoch 1534, Loss 3.074227\n",
      "grad:tensor([-0.0388,  0.2198])\n",
      "params:tensor([  5.1396, -16.0133])\n",
      "Epoch 1535, Loss 3.073726\n",
      "grad:tensor([-0.0387,  0.2194])\n",
      "params:tensor([  5.1400, -16.0155])\n",
      "Epoch 1536, Loss 3.073232\n",
      "grad:tensor([-0.0387,  0.2190])\n",
      "params:tensor([  5.1404, -16.0177])\n",
      "Epoch 1537, Loss 3.072739\n",
      "grad:tensor([-0.0386,  0.2186])\n",
      "params:tensor([  5.1407, -16.0199])\n",
      "Epoch 1538, Loss 3.072245\n",
      "grad:tensor([-0.0385,  0.2183])\n",
      "params:tensor([  5.1411, -16.0221])\n",
      "Epoch 1539, Loss 3.071753\n",
      "grad:tensor([-0.0385,  0.2179])\n",
      "params:tensor([  5.1415, -16.0243])\n",
      "Epoch 1540, Loss 3.071265\n",
      "grad:tensor([-0.0384,  0.2175])\n",
      "params:tensor([  5.1419, -16.0264])\n",
      "Epoch 1541, Loss 3.070778\n",
      "grad:tensor([-0.0383,  0.2172])\n",
      "params:tensor([  5.1423, -16.0286])\n",
      "Epoch 1542, Loss 3.070293\n",
      "grad:tensor([-0.0383,  0.2168])\n",
      "params:tensor([  5.1427, -16.0308])\n",
      "Epoch 1543, Loss 3.069808\n",
      "grad:tensor([-0.0382,  0.2164])\n",
      "params:tensor([  5.1430, -16.0330])\n",
      "Epoch 1544, Loss 3.069326\n",
      "grad:tensor([-0.0382,  0.2161])\n",
      "params:tensor([  5.1434, -16.0351])\n",
      "Epoch 1545, Loss 3.068845\n",
      "grad:tensor([-0.0381,  0.2157])\n",
      "params:tensor([  5.1438, -16.0373])\n",
      "Epoch 1546, Loss 3.068366\n",
      "grad:tensor([-0.0380,  0.2153])\n",
      "params:tensor([  5.1442, -16.0394])\n",
      "Epoch 1547, Loss 3.067887\n",
      "grad:tensor([-0.0380,  0.2150])\n",
      "params:tensor([  5.1446, -16.0416])\n",
      "Epoch 1548, Loss 3.067412\n",
      "grad:tensor([-0.0379,  0.2146])\n",
      "params:tensor([  5.1449, -16.0437])\n",
      "Epoch 1549, Loss 3.066937\n",
      "grad:tensor([-0.0378,  0.2142])\n",
      "params:tensor([  5.1453, -16.0459])\n",
      "Epoch 1550, Loss 3.066463\n",
      "grad:tensor([-0.0378,  0.2139])\n",
      "params:tensor([  5.1457, -16.0480])\n",
      "Epoch 1551, Loss 3.065993\n",
      "grad:tensor([-0.0377,  0.2135])\n",
      "params:tensor([  5.1461, -16.0501])\n",
      "Epoch 1552, Loss 3.065524\n",
      "grad:tensor([-0.0376,  0.2131])\n",
      "params:tensor([  5.1465, -16.0523])\n",
      "Epoch 1553, Loss 3.065055\n",
      "grad:tensor([-0.0376,  0.2128])\n",
      "params:tensor([  5.1468, -16.0544])\n",
      "Epoch 1554, Loss 3.064588\n",
      "grad:tensor([-0.0375,  0.2124])\n",
      "params:tensor([  5.1472, -16.0565])\n",
      "Epoch 1555, Loss 3.064123\n",
      "grad:tensor([-0.0375,  0.2120])\n",
      "params:tensor([  5.1476, -16.0586])\n",
      "Epoch 1556, Loss 3.063660\n",
      "grad:tensor([-0.0374,  0.2117])\n",
      "params:tensor([  5.1480, -16.0608])\n",
      "Epoch 1557, Loss 3.063199\n",
      "grad:tensor([-0.0373,  0.2113])\n",
      "params:tensor([  5.1483, -16.0629])\n",
      "Epoch 1558, Loss 3.062738\n",
      "grad:tensor([-0.0373,  0.2110])\n",
      "params:tensor([  5.1487, -16.0650])\n",
      "Epoch 1559, Loss 3.062280\n",
      "grad:tensor([-0.0372,  0.2106])\n",
      "params:tensor([  5.1491, -16.0671])\n",
      "Epoch 1560, Loss 3.061822\n",
      "grad:tensor([-0.0371,  0.2103])\n",
      "params:tensor([  5.1494, -16.0692])\n",
      "Epoch 1561, Loss 3.061367\n",
      "grad:tensor([-0.0371,  0.2099])\n",
      "params:tensor([  5.1498, -16.0713])\n",
      "Epoch 1562, Loss 3.060913\n",
      "grad:tensor([-0.0370,  0.2095])\n",
      "params:tensor([  5.1502, -16.0734])\n",
      "Epoch 1563, Loss 3.060462\n",
      "grad:tensor([-0.0370,  0.2092])\n",
      "params:tensor([  5.1506, -16.0755])\n",
      "Epoch 1564, Loss 3.060011\n",
      "grad:tensor([-0.0369,  0.2088])\n",
      "params:tensor([  5.1509, -16.0776])\n",
      "Epoch 1565, Loss 3.059561\n",
      "grad:tensor([-0.0368,  0.2085])\n",
      "params:tensor([  5.1513, -16.0796])\n",
      "Epoch 1566, Loss 3.059114\n",
      "grad:tensor([-0.0368,  0.2081])\n",
      "params:tensor([  5.1517, -16.0817])\n",
      "Epoch 1567, Loss 3.058668\n",
      "grad:tensor([-0.0367,  0.2078])\n",
      "params:tensor([  5.1520, -16.0838])\n",
      "Epoch 1568, Loss 3.058221\n",
      "grad:tensor([-0.0366,  0.2074])\n",
      "params:tensor([  5.1524, -16.0859])\n",
      "Epoch 1569, Loss 3.057781\n",
      "grad:tensor([-0.0366,  0.2071])\n",
      "params:tensor([  5.1528, -16.0880])\n",
      "Epoch 1570, Loss 3.057338\n",
      "grad:tensor([-0.0365,  0.2067])\n",
      "params:tensor([  5.1531, -16.0900])\n",
      "Epoch 1571, Loss 3.056898\n",
      "grad:tensor([-0.0364,  0.2064])\n",
      "params:tensor([  5.1535, -16.0921])\n",
      "Epoch 1572, Loss 3.056458\n",
      "grad:tensor([-0.0364,  0.2060])\n",
      "params:tensor([  5.1539, -16.0941])\n",
      "Epoch 1573, Loss 3.056019\n",
      "grad:tensor([-0.0363,  0.2057])\n",
      "params:tensor([  5.1542, -16.0962])\n",
      "Epoch 1574, Loss 3.055585\n",
      "grad:tensor([-0.0363,  0.2053])\n",
      "params:tensor([  5.1546, -16.0983])\n",
      "Epoch 1575, Loss 3.055151\n",
      "grad:tensor([-0.0362,  0.2050])\n",
      "params:tensor([  5.1549, -16.1003])\n",
      "Epoch 1576, Loss 3.054717\n",
      "grad:tensor([-0.0361,  0.2046])\n",
      "params:tensor([  5.1553, -16.1023])\n",
      "Epoch 1577, Loss 3.054286\n",
      "grad:tensor([-0.0361,  0.2043])\n",
      "params:tensor([  5.1557, -16.1044])\n",
      "Epoch 1578, Loss 3.053857\n",
      "grad:tensor([-0.0360,  0.2039])\n",
      "params:tensor([  5.1560, -16.1064])\n",
      "Epoch 1579, Loss 3.053427\n",
      "grad:tensor([-0.0360,  0.2036])\n",
      "params:tensor([  5.1564, -16.1085])\n",
      "Epoch 1580, Loss 3.053000\n",
      "grad:tensor([-0.0359,  0.2032])\n",
      "params:tensor([  5.1567, -16.1105])\n",
      "Epoch 1581, Loss 3.052576\n",
      "grad:tensor([-0.0358,  0.2029])\n",
      "params:tensor([  5.1571, -16.1125])\n",
      "Epoch 1582, Loss 3.052152\n",
      "grad:tensor([-0.0358,  0.2025])\n",
      "params:tensor([  5.1575, -16.1146])\n",
      "Epoch 1583, Loss 3.051730\n",
      "grad:tensor([-0.0357,  0.2022])\n",
      "params:tensor([  5.1578, -16.1166])\n",
      "Epoch 1584, Loss 3.051306\n",
      "grad:tensor([-0.0357,  0.2018])\n",
      "params:tensor([  5.1582, -16.1186])\n",
      "Epoch 1585, Loss 3.050888\n",
      "grad:tensor([-0.0356,  0.2015])\n",
      "params:tensor([  5.1585, -16.1206])\n",
      "Epoch 1586, Loss 3.050471\n",
      "grad:tensor([-0.0355,  0.2012])\n",
      "params:tensor([  5.1589, -16.1226])\n",
      "Epoch 1587, Loss 3.050052\n",
      "grad:tensor([-0.0355,  0.2008])\n",
      "params:tensor([  5.1592, -16.1246])\n",
      "Epoch 1588, Loss 3.049639\n",
      "grad:tensor([-0.0354,  0.2005])\n",
      "params:tensor([  5.1596, -16.1266])\n",
      "Epoch 1589, Loss 3.049223\n",
      "grad:tensor([-0.0354,  0.2001])\n",
      "params:tensor([  5.1599, -16.1286])\n",
      "Epoch 1590, Loss 3.048811\n",
      "grad:tensor([-0.0353,  0.1998])\n",
      "params:tensor([  5.1603, -16.1306])\n",
      "Epoch 1591, Loss 3.048398\n",
      "grad:tensor([-0.0353,  0.1995])\n",
      "params:tensor([  5.1607, -16.1326])\n",
      "Epoch 1592, Loss 3.047991\n",
      "grad:tensor([-0.0352,  0.1991])\n",
      "params:tensor([  5.1610, -16.1346])\n",
      "Epoch 1593, Loss 3.047581\n",
      "grad:tensor([-0.0351,  0.1988])\n",
      "params:tensor([  5.1614, -16.1366])\n",
      "Epoch 1594, Loss 3.047173\n",
      "grad:tensor([-0.0351,  0.1984])\n",
      "params:tensor([  5.1617, -16.1386])\n",
      "Epoch 1595, Loss 3.046768\n",
      "grad:tensor([-0.0350,  0.1981])\n",
      "params:tensor([  5.1621, -16.1406])\n",
      "Epoch 1596, Loss 3.046362\n",
      "grad:tensor([-0.0349,  0.1978])\n",
      "params:tensor([  5.1624, -16.1425])\n",
      "Epoch 1597, Loss 3.045960\n",
      "grad:tensor([-0.0349,  0.1974])\n",
      "params:tensor([  5.1628, -16.1445])\n",
      "Epoch 1598, Loss 3.045559\n",
      "grad:tensor([-0.0348,  0.1971])\n",
      "params:tensor([  5.1631, -16.1465])\n",
      "Epoch 1599, Loss 3.045160\n",
      "grad:tensor([-0.0348,  0.1968])\n",
      "params:tensor([  5.1635, -16.1485])\n",
      "Epoch 1600, Loss 3.044759\n",
      "grad:tensor([-0.0347,  0.1964])\n",
      "params:tensor([  5.1638, -16.1504])\n",
      "Epoch 1601, Loss 3.044361\n",
      "grad:tensor([-0.0346,  0.1961])\n",
      "params:tensor([  5.1641, -16.1524])\n",
      "Epoch 1602, Loss 3.043966\n",
      "grad:tensor([-0.0346,  0.1958])\n",
      "params:tensor([  5.1645, -16.1543])\n",
      "Epoch 1603, Loss 3.043571\n",
      "grad:tensor([-0.0345,  0.1954])\n",
      "params:tensor([  5.1648, -16.1563])\n",
      "Epoch 1604, Loss 3.043176\n",
      "grad:tensor([-0.0345,  0.1951])\n",
      "params:tensor([  5.1652, -16.1582])\n",
      "Epoch 1605, Loss 3.042785\n",
      "grad:tensor([-0.0344,  0.1948])\n",
      "params:tensor([  5.1655, -16.1602])\n",
      "Epoch 1606, Loss 3.042395\n",
      "grad:tensor([-0.0343,  0.1944])\n",
      "params:tensor([  5.1659, -16.1621])\n",
      "Epoch 1607, Loss 3.042005\n",
      "grad:tensor([-0.0343,  0.1941])\n",
      "params:tensor([  5.1662, -16.1641])\n",
      "Epoch 1608, Loss 3.041615\n",
      "grad:tensor([-0.0342,  0.1938])\n",
      "params:tensor([  5.1666, -16.1660])\n",
      "Epoch 1609, Loss 3.041230\n",
      "grad:tensor([-0.0342,  0.1934])\n",
      "params:tensor([  5.1669, -16.1680])\n",
      "Epoch 1610, Loss 3.040844\n",
      "grad:tensor([-0.0341,  0.1931])\n",
      "params:tensor([  5.1672, -16.1699])\n",
      "Epoch 1611, Loss 3.040461\n",
      "grad:tensor([-0.0341,  0.1928])\n",
      "params:tensor([  5.1676, -16.1718])\n",
      "Epoch 1612, Loss 3.040077\n",
      "grad:tensor([-0.0340,  0.1925])\n",
      "params:tensor([  5.1679, -16.1737])\n",
      "Epoch 1613, Loss 3.039695\n",
      "grad:tensor([-0.0339,  0.1921])\n",
      "params:tensor([  5.1683, -16.1757])\n",
      "Epoch 1614, Loss 3.039314\n",
      "grad:tensor([-0.0339,  0.1918])\n",
      "params:tensor([  5.1686, -16.1776])\n",
      "Epoch 1615, Loss 3.038934\n",
      "grad:tensor([-0.0338,  0.1915])\n",
      "params:tensor([  5.1689, -16.1795])\n",
      "Epoch 1616, Loss 3.038557\n",
      "grad:tensor([-0.0338,  0.1912])\n",
      "params:tensor([  5.1693, -16.1814])\n",
      "Epoch 1617, Loss 3.038181\n",
      "grad:tensor([-0.0337,  0.1908])\n",
      "params:tensor([  5.1696, -16.1833])\n",
      "Epoch 1618, Loss 3.037805\n",
      "grad:tensor([-0.0337,  0.1905])\n",
      "params:tensor([  5.1699, -16.1852])\n",
      "Epoch 1619, Loss 3.037432\n",
      "grad:tensor([-0.0336,  0.1902])\n",
      "params:tensor([  5.1703, -16.1871])\n",
      "Epoch 1620, Loss 3.037059\n",
      "grad:tensor([-0.0335,  0.1899])\n",
      "params:tensor([  5.1706, -16.1890])\n",
      "Epoch 1621, Loss 3.036689\n",
      "grad:tensor([-0.0335,  0.1895])\n",
      "params:tensor([  5.1710, -16.1909])\n",
      "Epoch 1622, Loss 3.036319\n",
      "grad:tensor([-0.0334,  0.1892])\n",
      "params:tensor([  5.1713, -16.1928])\n",
      "Epoch 1623, Loss 3.035949\n",
      "grad:tensor([-0.0334,  0.1889])\n",
      "params:tensor([  5.1716, -16.1947])\n",
      "Epoch 1624, Loss 3.035583\n",
      "grad:tensor([-0.0333,  0.1886])\n",
      "params:tensor([  5.1720, -16.1966])\n",
      "Epoch 1625, Loss 3.035216\n",
      "grad:tensor([-0.0333,  0.1883])\n",
      "params:tensor([  5.1723, -16.1985])\n",
      "Epoch 1626, Loss 3.034849\n",
      "grad:tensor([-0.0332,  0.1879])\n",
      "params:tensor([  5.1726, -16.2003])\n",
      "Epoch 1627, Loss 3.034485\n",
      "grad:tensor([-0.0331,  0.1876])\n",
      "params:tensor([  5.1729, -16.2022])\n",
      "Epoch 1628, Loss 3.034123\n",
      "grad:tensor([-0.0331,  0.1873])\n",
      "params:tensor([  5.1733, -16.2041])\n",
      "Epoch 1629, Loss 3.033762\n",
      "grad:tensor([-0.0330,  0.1870])\n",
      "params:tensor([  5.1736, -16.2060])\n",
      "Epoch 1630, Loss 3.033402\n",
      "grad:tensor([-0.0330,  0.1867])\n",
      "params:tensor([  5.1739, -16.2078])\n",
      "Epoch 1631, Loss 3.033041\n",
      "grad:tensor([-0.0329,  0.1863])\n",
      "params:tensor([  5.1743, -16.2097])\n",
      "Epoch 1632, Loss 3.032685\n",
      "grad:tensor([-0.0329,  0.1860])\n",
      "params:tensor([  5.1746, -16.2116])\n",
      "Epoch 1633, Loss 3.032329\n",
      "grad:tensor([-0.0328,  0.1857])\n",
      "params:tensor([  5.1749, -16.2134])\n",
      "Epoch 1634, Loss 3.031973\n",
      "grad:tensor([-0.0327,  0.1854])\n",
      "params:tensor([  5.1753, -16.2153])\n",
      "Epoch 1635, Loss 3.031619\n",
      "grad:tensor([-0.0327,  0.1851])\n",
      "params:tensor([  5.1756, -16.2171])\n",
      "Epoch 1636, Loss 3.031265\n",
      "grad:tensor([-0.0326,  0.1848])\n",
      "params:tensor([  5.1759, -16.2190])\n",
      "Epoch 1637, Loss 3.030913\n",
      "grad:tensor([-0.0326,  0.1845])\n",
      "params:tensor([  5.1762, -16.2208])\n",
      "Epoch 1638, Loss 3.030564\n",
      "grad:tensor([-0.0325,  0.1841])\n",
      "params:tensor([  5.1766, -16.2226])\n",
      "Epoch 1639, Loss 3.030215\n",
      "grad:tensor([-0.0325,  0.1838])\n",
      "params:tensor([  5.1769, -16.2245])\n",
      "Epoch 1640, Loss 3.029867\n",
      "grad:tensor([-0.0324,  0.1835])\n",
      "params:tensor([  5.1772, -16.2263])\n",
      "Epoch 1641, Loss 3.029518\n",
      "grad:tensor([-0.0324,  0.1832])\n",
      "params:tensor([  5.1775, -16.2282])\n",
      "Epoch 1642, Loss 3.029173\n",
      "grad:tensor([-0.0323,  0.1829])\n",
      "params:tensor([  5.1779, -16.2300])\n",
      "Epoch 1643, Loss 3.028828\n",
      "grad:tensor([-0.0323,  0.1826])\n",
      "params:tensor([  5.1782, -16.2318])\n",
      "Epoch 1644, Loss 3.028486\n",
      "grad:tensor([-0.0322,  0.1823])\n",
      "params:tensor([  5.1785, -16.2336])\n",
      "Epoch 1645, Loss 3.028142\n",
      "grad:tensor([-0.0321,  0.1820])\n",
      "params:tensor([  5.1788, -16.2355])\n",
      "Epoch 1646, Loss 3.027802\n",
      "grad:tensor([-0.0321,  0.1817])\n",
      "params:tensor([  5.1791, -16.2373])\n",
      "Epoch 1647, Loss 3.027463\n",
      "grad:tensor([-0.0320,  0.1813])\n",
      "params:tensor([  5.1795, -16.2391])\n",
      "Epoch 1648, Loss 3.027122\n",
      "grad:tensor([-0.0320,  0.1810])\n",
      "params:tensor([  5.1798, -16.2409])\n",
      "Epoch 1649, Loss 3.026784\n",
      "grad:tensor([-0.0319,  0.1807])\n",
      "params:tensor([  5.1801, -16.2427])\n",
      "Epoch 1650, Loss 3.026447\n",
      "grad:tensor([-0.0319,  0.1804])\n",
      "params:tensor([  5.1804, -16.2445])\n",
      "Epoch 1651, Loss 3.026111\n",
      "grad:tensor([-0.0318,  0.1801])\n",
      "params:tensor([  5.1807, -16.2463])\n",
      "Epoch 1652, Loss 3.025780\n",
      "grad:tensor([-0.0318,  0.1798])\n",
      "params:tensor([  5.1811, -16.2481])\n",
      "Epoch 1653, Loss 3.025447\n",
      "grad:tensor([-0.0317,  0.1795])\n",
      "params:tensor([  5.1814, -16.2499])\n",
      "Epoch 1654, Loss 3.025114\n",
      "grad:tensor([-0.0317,  0.1792])\n",
      "params:tensor([  5.1817, -16.2517])\n",
      "Epoch 1655, Loss 3.024782\n",
      "grad:tensor([-0.0316,  0.1789])\n",
      "params:tensor([  5.1820, -16.2535])\n",
      "Epoch 1656, Loss 3.024452\n",
      "grad:tensor([-0.0316,  0.1786])\n",
      "params:tensor([  5.1823, -16.2553])\n",
      "Epoch 1657, Loss 3.024125\n",
      "grad:tensor([-0.0315,  0.1783])\n",
      "params:tensor([  5.1826, -16.2570])\n",
      "Epoch 1658, Loss 3.023796\n",
      "grad:tensor([-0.0315,  0.1780])\n",
      "params:tensor([  5.1829, -16.2588])\n",
      "Epoch 1659, Loss 3.023471\n",
      "grad:tensor([-0.0314,  0.1777])\n",
      "params:tensor([  5.1833, -16.2606])\n",
      "Epoch 1660, Loss 3.023145\n",
      "grad:tensor([-0.0313,  0.1774])\n",
      "params:tensor([  5.1836, -16.2624])\n",
      "Epoch 1661, Loss 3.022820\n",
      "grad:tensor([-0.0313,  0.1771])\n",
      "params:tensor([  5.1839, -16.2641])\n",
      "Epoch 1662, Loss 3.022498\n",
      "grad:tensor([-0.0312,  0.1768])\n",
      "params:tensor([  5.1842, -16.2659])\n",
      "Epoch 1663, Loss 3.022177\n",
      "grad:tensor([-0.0312,  0.1765])\n",
      "params:tensor([  5.1845, -16.2677])\n",
      "Epoch 1664, Loss 3.021855\n",
      "grad:tensor([-0.0311,  0.1762])\n",
      "params:tensor([  5.1848, -16.2694])\n",
      "Epoch 1665, Loss 3.021534\n",
      "grad:tensor([-0.0311,  0.1759])\n",
      "params:tensor([  5.1851, -16.2712])\n",
      "Epoch 1666, Loss 3.021217\n",
      "grad:tensor([-0.0310,  0.1756])\n",
      "params:tensor([  5.1854, -16.2730])\n",
      "Epoch 1667, Loss 3.020898\n",
      "grad:tensor([-0.0310,  0.1753])\n",
      "params:tensor([  5.1858, -16.2747])\n",
      "Epoch 1668, Loss 3.020582\n",
      "grad:tensor([-0.0309,  0.1750])\n",
      "params:tensor([  5.1861, -16.2765])\n",
      "Epoch 1669, Loss 3.020265\n",
      "grad:tensor([-0.0309,  0.1747])\n",
      "params:tensor([  5.1864, -16.2782])\n",
      "Epoch 1670, Loss 3.019952\n",
      "grad:tensor([-0.0308,  0.1744])\n",
      "params:tensor([  5.1867, -16.2800])\n",
      "Epoch 1671, Loss 3.019639\n",
      "grad:tensor([-0.0308,  0.1741])\n",
      "params:tensor([  5.1870, -16.2817])\n",
      "Epoch 1672, Loss 3.019325\n",
      "grad:tensor([-0.0307,  0.1738])\n",
      "params:tensor([  5.1873, -16.2834])\n",
      "Epoch 1673, Loss 3.019016\n",
      "grad:tensor([-0.0307,  0.1735])\n",
      "params:tensor([  5.1876, -16.2852])\n",
      "Epoch 1674, Loss 3.018706\n",
      "grad:tensor([-0.0306,  0.1732])\n",
      "params:tensor([  5.1879, -16.2869])\n",
      "Epoch 1675, Loss 3.018395\n",
      "grad:tensor([-0.0305,  0.1729])\n",
      "params:tensor([  5.1882, -16.2886])\n",
      "Epoch 1676, Loss 3.018089\n",
      "grad:tensor([-0.0305,  0.1726])\n",
      "params:tensor([  5.1885, -16.2904])\n",
      "Epoch 1677, Loss 3.017780\n",
      "grad:tensor([-0.0304,  0.1723])\n",
      "params:tensor([  5.1888, -16.2921])\n",
      "Epoch 1678, Loss 3.017475\n",
      "grad:tensor([-0.0304,  0.1720])\n",
      "params:tensor([  5.1891, -16.2938])\n",
      "Epoch 1679, Loss 3.017170\n",
      "grad:tensor([-0.0303,  0.1717])\n",
      "params:tensor([  5.1894, -16.2955])\n",
      "Epoch 1680, Loss 3.016867\n",
      "grad:tensor([-0.0303,  0.1715])\n",
      "params:tensor([  5.1897, -16.2972])\n",
      "Epoch 1681, Loss 3.016564\n",
      "grad:tensor([-0.0302,  0.1712])\n",
      "params:tensor([  5.1900, -16.2989])\n",
      "Epoch 1682, Loss 3.016262\n",
      "grad:tensor([-0.0302,  0.1709])\n",
      "params:tensor([  5.1903, -16.3006])\n",
      "Epoch 1683, Loss 3.015959\n",
      "grad:tensor([-0.0301,  0.1706])\n",
      "params:tensor([  5.1906, -16.3024])\n",
      "Epoch 1684, Loss 3.015662\n",
      "grad:tensor([-0.0301,  0.1703])\n",
      "params:tensor([  5.1909, -16.3041])\n",
      "Epoch 1685, Loss 3.015361\n",
      "grad:tensor([-0.0300,  0.1700])\n",
      "params:tensor([  5.1912, -16.3058])\n",
      "Epoch 1686, Loss 3.015064\n",
      "grad:tensor([-0.0300,  0.1697])\n",
      "params:tensor([  5.1915, -16.3075])\n",
      "Epoch 1687, Loss 3.014768\n",
      "grad:tensor([-0.0299,  0.1694])\n",
      "params:tensor([  5.1918, -16.3091])\n",
      "Epoch 1688, Loss 3.014472\n",
      "grad:tensor([-0.0299,  0.1691])\n",
      "params:tensor([  5.1921, -16.3108])\n",
      "Epoch 1689, Loss 3.014179\n",
      "grad:tensor([-0.0298,  0.1688])\n",
      "params:tensor([  5.1924, -16.3125])\n",
      "Epoch 1690, Loss 3.013884\n",
      "grad:tensor([-0.0298,  0.1686])\n",
      "params:tensor([  5.1927, -16.3142])\n",
      "Epoch 1691, Loss 3.013591\n",
      "grad:tensor([-0.0297,  0.1683])\n",
      "params:tensor([  5.1930, -16.3159])\n",
      "Epoch 1692, Loss 3.013299\n",
      "grad:tensor([-0.0297,  0.1680])\n",
      "params:tensor([  5.1933, -16.3176])\n",
      "Epoch 1693, Loss 3.013008\n",
      "grad:tensor([-0.0296,  0.1677])\n",
      "params:tensor([  5.1936, -16.3193])\n",
      "Epoch 1694, Loss 3.012719\n",
      "grad:tensor([-0.0296,  0.1674])\n",
      "params:tensor([  5.1939, -16.3209])\n",
      "Epoch 1695, Loss 3.012431\n",
      "grad:tensor([-0.0295,  0.1671])\n",
      "params:tensor([  5.1942, -16.3226])\n",
      "Epoch 1696, Loss 3.012141\n",
      "grad:tensor([-0.0295,  0.1668])\n",
      "params:tensor([  5.1945, -16.3243])\n",
      "Epoch 1697, Loss 3.011855\n",
      "grad:tensor([-0.0294,  0.1666])\n",
      "params:tensor([  5.1948, -16.3259])\n",
      "Epoch 1698, Loss 3.011570\n",
      "grad:tensor([-0.0294,  0.1663])\n",
      "params:tensor([  5.1951, -16.3276])\n",
      "Epoch 1699, Loss 3.011284\n",
      "grad:tensor([-0.0293,  0.1660])\n",
      "params:tensor([  5.1954, -16.3293])\n",
      "Epoch 1700, Loss 3.011001\n",
      "grad:tensor([-0.0293,  0.1657])\n",
      "params:tensor([  5.1957, -16.3309])\n",
      "Epoch 1701, Loss 3.010718\n",
      "grad:tensor([-0.0292,  0.1654])\n",
      "params:tensor([  5.1960, -16.3326])\n",
      "Epoch 1702, Loss 3.010436\n",
      "grad:tensor([-0.0292,  0.1652])\n",
      "params:tensor([  5.1963, -16.3342])\n",
      "Epoch 1703, Loss 3.010156\n",
      "grad:tensor([-0.0291,  0.1649])\n",
      "params:tensor([  5.1966, -16.3359])\n",
      "Epoch 1704, Loss 3.009876\n",
      "grad:tensor([-0.0291,  0.1646])\n",
      "params:tensor([  5.1968, -16.3375])\n",
      "Epoch 1705, Loss 3.009595\n",
      "grad:tensor([-0.0290,  0.1643])\n",
      "params:tensor([  5.1971, -16.3392])\n",
      "Epoch 1706, Loss 3.009319\n",
      "grad:tensor([-0.0290,  0.1640])\n",
      "params:tensor([  5.1974, -16.3408])\n",
      "Epoch 1707, Loss 3.009040\n",
      "grad:tensor([-0.0289,  0.1638])\n",
      "params:tensor([  5.1977, -16.3424])\n",
      "Epoch 1708, Loss 3.008763\n",
      "grad:tensor([-0.0289,  0.1635])\n",
      "params:tensor([  5.1980, -16.3441])\n",
      "Epoch 1709, Loss 3.008487\n",
      "grad:tensor([-0.0288,  0.1632])\n",
      "params:tensor([  5.1983, -16.3457])\n",
      "Epoch 1710, Loss 3.008215\n",
      "grad:tensor([-0.0288,  0.1629])\n",
      "params:tensor([  5.1986, -16.3473])\n",
      "Epoch 1711, Loss 3.007941\n",
      "grad:tensor([-0.0287,  0.1626])\n",
      "params:tensor([  5.1989, -16.3490])\n",
      "Epoch 1712, Loss 3.007668\n",
      "grad:tensor([-0.0287,  0.1624])\n",
      "params:tensor([  5.1992, -16.3506])\n",
      "Epoch 1713, Loss 3.007397\n",
      "grad:tensor([-0.0286,  0.1621])\n",
      "params:tensor([  5.1994, -16.3522])\n",
      "Epoch 1714, Loss 3.007126\n",
      "grad:tensor([-0.0286,  0.1618])\n",
      "params:tensor([  5.1997, -16.3538])\n",
      "Epoch 1715, Loss 3.006857\n",
      "grad:tensor([-0.0285,  0.1615])\n",
      "params:tensor([  5.2000, -16.3554])\n",
      "Epoch 1716, Loss 3.006586\n",
      "grad:tensor([-0.0285,  0.1613])\n",
      "params:tensor([  5.2003, -16.3570])\n",
      "Epoch 1717, Loss 3.006318\n",
      "grad:tensor([-0.0284,  0.1610])\n",
      "params:tensor([  5.2006, -16.3587])\n",
      "Epoch 1718, Loss 3.006052\n",
      "grad:tensor([-0.0284,  0.1607])\n",
      "params:tensor([  5.2009, -16.3603])\n",
      "Epoch 1719, Loss 3.005785\n",
      "grad:tensor([-0.0284,  0.1604])\n",
      "params:tensor([  5.2012, -16.3619])\n",
      "Epoch 1720, Loss 3.005521\n",
      "grad:tensor([-0.0283,  0.1602])\n",
      "params:tensor([  5.2014, -16.3635])\n",
      "Epoch 1721, Loss 3.005256\n",
      "grad:tensor([-0.0283,  0.1599])\n",
      "params:tensor([  5.2017, -16.3651])\n",
      "Epoch 1722, Loss 3.004993\n",
      "grad:tensor([-0.0282,  0.1596])\n",
      "params:tensor([  5.2020, -16.3667])\n",
      "Epoch 1723, Loss 3.004729\n",
      "grad:tensor([-0.0281,  0.1594])\n",
      "params:tensor([  5.2023, -16.3683])\n",
      "Epoch 1724, Loss 3.004467\n",
      "grad:tensor([-0.0281,  0.1591])\n",
      "params:tensor([  5.2026, -16.3699])\n",
      "Epoch 1725, Loss 3.004207\n",
      "grad:tensor([-0.0280,  0.1588])\n",
      "params:tensor([  5.2028, -16.3714])\n",
      "Epoch 1726, Loss 3.003947\n",
      "grad:tensor([-0.0280,  0.1586])\n",
      "params:tensor([  5.2031, -16.3730])\n",
      "Epoch 1727, Loss 3.003690\n",
      "grad:tensor([-0.0280,  0.1583])\n",
      "params:tensor([  5.2034, -16.3746])\n",
      "Epoch 1728, Loss 3.003431\n",
      "grad:tensor([-0.0279,  0.1580])\n",
      "params:tensor([  5.2037, -16.3762])\n",
      "Epoch 1729, Loss 3.003174\n",
      "grad:tensor([-0.0279,  0.1577])\n",
      "params:tensor([  5.2040, -16.3778])\n",
      "Epoch 1730, Loss 3.002918\n",
      "grad:tensor([-0.0278,  0.1575])\n",
      "params:tensor([  5.2042, -16.3793])\n",
      "Epoch 1731, Loss 3.002661\n",
      "grad:tensor([-0.0278,  0.1572])\n",
      "params:tensor([  5.2045, -16.3809])\n",
      "Epoch 1732, Loss 3.002406\n",
      "grad:tensor([-0.0277,  0.1569])\n",
      "params:tensor([  5.2048, -16.3825])\n",
      "Epoch 1733, Loss 3.002152\n",
      "grad:tensor([-0.0277,  0.1567])\n",
      "params:tensor([  5.2051, -16.3840])\n",
      "Epoch 1734, Loss 3.001901\n",
      "grad:tensor([-0.0276,  0.1564])\n",
      "params:tensor([  5.2053, -16.3856])\n",
      "Epoch 1735, Loss 3.001649\n",
      "grad:tensor([-0.0276,  0.1561])\n",
      "params:tensor([  5.2056, -16.3872])\n",
      "Epoch 1736, Loss 3.001395\n",
      "grad:tensor([-0.0275,  0.1559])\n",
      "params:tensor([  5.2059, -16.3887])\n",
      "Epoch 1737, Loss 3.001145\n",
      "grad:tensor([-0.0275,  0.1556])\n",
      "params:tensor([  5.2062, -16.3903])\n",
      "Epoch 1738, Loss 3.000898\n",
      "grad:tensor([-0.0274,  0.1553])\n",
      "params:tensor([  5.2064, -16.3918])\n",
      "Epoch 1739, Loss 3.000648\n",
      "grad:tensor([-0.0274,  0.1551])\n",
      "params:tensor([  5.2067, -16.3934])\n",
      "Epoch 1740, Loss 3.000400\n",
      "grad:tensor([-0.0273,  0.1548])\n",
      "params:tensor([  5.2070, -16.3949])\n",
      "Epoch 1741, Loss 3.000154\n",
      "grad:tensor([-0.0273,  0.1546])\n",
      "params:tensor([  5.2073, -16.3965])\n",
      "Epoch 1742, Loss 2.999907\n",
      "grad:tensor([-0.0273,  0.1543])\n",
      "params:tensor([  5.2075, -16.3980])\n",
      "Epoch 1743, Loss 2.999662\n",
      "grad:tensor([-0.0272,  0.1540])\n",
      "params:tensor([  5.2078, -16.3996])\n",
      "Epoch 1744, Loss 2.999417\n",
      "grad:tensor([-0.0272,  0.1538])\n",
      "params:tensor([  5.2081, -16.4011])\n",
      "Epoch 1745, Loss 2.999174\n",
      "grad:tensor([-0.0271,  0.1535])\n",
      "params:tensor([  5.2084, -16.4026])\n",
      "Epoch 1746, Loss 2.998930\n",
      "grad:tensor([-0.0271,  0.1533])\n",
      "params:tensor([  5.2086, -16.4042])\n",
      "Epoch 1747, Loss 2.998688\n",
      "grad:tensor([-0.0270,  0.1530])\n",
      "params:tensor([  5.2089, -16.4057])\n",
      "Epoch 1748, Loss 2.998448\n",
      "grad:tensor([-0.0270,  0.1527])\n",
      "params:tensor([  5.2092, -16.4072])\n",
      "Epoch 1749, Loss 2.998208\n",
      "grad:tensor([-0.0269,  0.1525])\n",
      "params:tensor([  5.2094, -16.4088])\n",
      "Epoch 1750, Loss 2.997968\n",
      "grad:tensor([-0.0269,  0.1522])\n",
      "params:tensor([  5.2097, -16.4103])\n",
      "Epoch 1751, Loss 2.997730\n",
      "grad:tensor([-0.0268,  0.1520])\n",
      "params:tensor([  5.2100, -16.4118])\n",
      "Epoch 1752, Loss 2.997490\n",
      "grad:tensor([-0.0268,  0.1517])\n",
      "params:tensor([  5.2102, -16.4133])\n",
      "Epoch 1753, Loss 2.997254\n",
      "grad:tensor([-0.0267,  0.1514])\n",
      "params:tensor([  5.2105, -16.4148])\n",
      "Epoch 1754, Loss 2.997018\n",
      "grad:tensor([-0.0267,  0.1512])\n",
      "params:tensor([  5.2108, -16.4163])\n",
      "Epoch 1755, Loss 2.996782\n",
      "grad:tensor([-0.0266,  0.1509])\n",
      "params:tensor([  5.2110, -16.4179])\n",
      "Epoch 1756, Loss 2.996548\n",
      "grad:tensor([-0.0266,  0.1507])\n",
      "params:tensor([  5.2113, -16.4194])\n",
      "Epoch 1757, Loss 2.996313\n",
      "grad:tensor([-0.0266,  0.1504])\n",
      "params:tensor([  5.2116, -16.4209])\n",
      "Epoch 1758, Loss 2.996081\n",
      "grad:tensor([-0.0265,  0.1502])\n",
      "params:tensor([  5.2118, -16.4224])\n",
      "Epoch 1759, Loss 2.995847\n",
      "grad:tensor([-0.0265,  0.1499])\n",
      "params:tensor([  5.2121, -16.4239])\n",
      "Epoch 1760, Loss 2.995615\n",
      "grad:tensor([-0.0264,  0.1496])\n",
      "params:tensor([  5.2124, -16.4254])\n",
      "Epoch 1761, Loss 2.995387\n",
      "grad:tensor([-0.0264,  0.1494])\n",
      "params:tensor([  5.2126, -16.4269])\n",
      "Epoch 1762, Loss 2.995156\n",
      "grad:tensor([-0.0263,  0.1491])\n",
      "params:tensor([  5.2129, -16.4283])\n",
      "Epoch 1763, Loss 2.994928\n",
      "grad:tensor([-0.0263,  0.1489])\n",
      "params:tensor([  5.2132, -16.4298])\n",
      "Epoch 1764, Loss 2.994699\n",
      "grad:tensor([-0.0263,  0.1486])\n",
      "params:tensor([  5.2134, -16.4313])\n",
      "Epoch 1765, Loss 2.994471\n",
      "grad:tensor([-0.0262,  0.1484])\n",
      "params:tensor([  5.2137, -16.4328])\n",
      "Epoch 1766, Loss 2.994245\n",
      "grad:tensor([-0.0262,  0.1481])\n",
      "params:tensor([  5.2139, -16.4343])\n",
      "Epoch 1767, Loss 2.994019\n",
      "grad:tensor([-0.0261,  0.1479])\n",
      "params:tensor([  5.2142, -16.4358])\n",
      "Epoch 1768, Loss 2.993794\n",
      "grad:tensor([-0.0261,  0.1476])\n",
      "params:tensor([  5.2145, -16.4372])\n",
      "Epoch 1769, Loss 2.993569\n",
      "grad:tensor([-0.0260,  0.1474])\n",
      "params:tensor([  5.2147, -16.4387])\n",
      "Epoch 1770, Loss 2.993344\n",
      "grad:tensor([-0.0260,  0.1471])\n",
      "params:tensor([  5.2150, -16.4402])\n",
      "Epoch 1771, Loss 2.993121\n",
      "grad:tensor([-0.0260,  0.1469])\n",
      "params:tensor([  5.2152, -16.4417])\n",
      "Epoch 1772, Loss 2.992900\n",
      "grad:tensor([-0.0259,  0.1466])\n",
      "params:tensor([  5.2155, -16.4431])\n",
      "Epoch 1773, Loss 2.992678\n",
      "grad:tensor([-0.0259,  0.1464])\n",
      "params:tensor([  5.2158, -16.4446])\n",
      "Epoch 1774, Loss 2.992457\n",
      "grad:tensor([-0.0258,  0.1461])\n",
      "params:tensor([  5.2160, -16.4460])\n",
      "Epoch 1775, Loss 2.992237\n",
      "grad:tensor([-0.0258,  0.1459])\n",
      "params:tensor([  5.2163, -16.4475])\n",
      "Epoch 1776, Loss 2.992017\n",
      "grad:tensor([-0.0257,  0.1456])\n",
      "params:tensor([  5.2165, -16.4490])\n",
      "Epoch 1777, Loss 2.991798\n",
      "grad:tensor([-0.0257,  0.1454])\n",
      "params:tensor([  5.2168, -16.4504])\n",
      "Epoch 1778, Loss 2.991582\n",
      "grad:tensor([-0.0256,  0.1451])\n",
      "params:tensor([  5.2170, -16.4519])\n",
      "Epoch 1779, Loss 2.991366\n",
      "grad:tensor([-0.0256,  0.1449])\n",
      "params:tensor([  5.2173, -16.4533])\n",
      "Epoch 1780, Loss 2.991146\n",
      "grad:tensor([-0.0256,  0.1446])\n",
      "params:tensor([  5.2176, -16.4548])\n",
      "Epoch 1781, Loss 2.990932\n",
      "grad:tensor([-0.0255,  0.1444])\n",
      "params:tensor([  5.2178, -16.4562])\n",
      "Epoch 1782, Loss 2.990719\n",
      "grad:tensor([-0.0255,  0.1442])\n",
      "params:tensor([  5.2181, -16.4576])\n",
      "Epoch 1783, Loss 2.990503\n",
      "grad:tensor([-0.0254,  0.1439])\n",
      "params:tensor([  5.2183, -16.4591])\n",
      "Epoch 1784, Loss 2.990288\n",
      "grad:tensor([-0.0254,  0.1437])\n",
      "params:tensor([  5.2186, -16.4605])\n",
      "Epoch 1785, Loss 2.990078\n",
      "grad:tensor([-0.0253,  0.1434])\n",
      "params:tensor([  5.2188, -16.4620])\n",
      "Epoch 1786, Loss 2.989866\n",
      "grad:tensor([-0.0253,  0.1432])\n",
      "params:tensor([  5.2191, -16.4634])\n",
      "Epoch 1787, Loss 2.989655\n",
      "grad:tensor([-0.0252,  0.1429])\n",
      "params:tensor([  5.2193, -16.4648])\n",
      "Epoch 1788, Loss 2.989443\n",
      "grad:tensor([-0.0252,  0.1427])\n",
      "params:tensor([  5.2196, -16.4662])\n",
      "Epoch 1789, Loss 2.989233\n",
      "grad:tensor([-0.0252,  0.1424])\n",
      "params:tensor([  5.2198, -16.4677])\n",
      "Epoch 1790, Loss 2.989025\n",
      "grad:tensor([-0.0251,  0.1422])\n",
      "params:tensor([  5.2201, -16.4691])\n",
      "Epoch 1791, Loss 2.988817\n",
      "grad:tensor([-0.0251,  0.1420])\n",
      "params:tensor([  5.2203, -16.4705])\n",
      "Epoch 1792, Loss 2.988609\n",
      "grad:tensor([-0.0250,  0.1417])\n",
      "params:tensor([  5.2206, -16.4719])\n",
      "Epoch 1793, Loss 2.988401\n",
      "grad:tensor([-0.0250,  0.1415])\n",
      "params:tensor([  5.2208, -16.4733])\n",
      "Epoch 1794, Loss 2.988195\n",
      "grad:tensor([-0.0249,  0.1412])\n",
      "params:tensor([  5.2211, -16.4748])\n",
      "Epoch 1795, Loss 2.987989\n",
      "grad:tensor([-0.0249,  0.1410])\n",
      "params:tensor([  5.2213, -16.4762])\n",
      "Epoch 1796, Loss 2.987785\n",
      "grad:tensor([-0.0249,  0.1408])\n",
      "params:tensor([  5.2216, -16.4776])\n",
      "Epoch 1797, Loss 2.987582\n",
      "grad:tensor([-0.0248,  0.1405])\n",
      "params:tensor([  5.2218, -16.4790])\n",
      "Epoch 1798, Loss 2.987377\n",
      "grad:tensor([-0.0248,  0.1403])\n",
      "params:tensor([  5.2221, -16.4804])\n",
      "Epoch 1799, Loss 2.987174\n",
      "grad:tensor([-0.0247,  0.1400])\n",
      "params:tensor([  5.2223, -16.4818])\n",
      "Epoch 1800, Loss 2.986974\n",
      "grad:tensor([-0.0247,  0.1398])\n",
      "params:tensor([  5.2226, -16.4832])\n",
      "Epoch 1801, Loss 2.986771\n",
      "grad:tensor([-0.0246,  0.1396])\n",
      "params:tensor([  5.2228, -16.4846])\n",
      "Epoch 1802, Loss 2.986570\n",
      "grad:tensor([-0.0246,  0.1393])\n",
      "params:tensor([  5.2231, -16.4860])\n",
      "Epoch 1803, Loss 2.986371\n",
      "grad:tensor([-0.0246,  0.1391])\n",
      "params:tensor([  5.2233, -16.4874])\n",
      "Epoch 1804, Loss 2.986171\n",
      "grad:tensor([-0.0245,  0.1389])\n",
      "params:tensor([  5.2236, -16.4888])\n",
      "Epoch 1805, Loss 2.985972\n",
      "grad:tensor([-0.0245,  0.1386])\n",
      "params:tensor([  5.2238, -16.4901])\n",
      "Epoch 1806, Loss 2.985774\n",
      "grad:tensor([-0.0245,  0.1384])\n",
      "params:tensor([  5.2241, -16.4915])\n",
      "Epoch 1807, Loss 2.985578\n",
      "grad:tensor([-0.0244,  0.1382])\n",
      "params:tensor([  5.2243, -16.4929])\n",
      "Epoch 1808, Loss 2.985381\n",
      "grad:tensor([-0.0244,  0.1379])\n",
      "params:tensor([  5.2245, -16.4943])\n",
      "Epoch 1809, Loss 2.985184\n",
      "grad:tensor([-0.0243,  0.1377])\n",
      "params:tensor([  5.2248, -16.4957])\n",
      "Epoch 1810, Loss 2.984989\n",
      "grad:tensor([-0.0243,  0.1374])\n",
      "params:tensor([  5.2250, -16.4970])\n",
      "Epoch 1811, Loss 2.984793\n",
      "grad:tensor([-0.0243,  0.1372])\n",
      "params:tensor([  5.2253, -16.4984])\n",
      "Epoch 1812, Loss 2.984601\n",
      "grad:tensor([-0.0242,  0.1370])\n",
      "params:tensor([  5.2255, -16.4998])\n",
      "Epoch 1813, Loss 2.984407\n",
      "grad:tensor([-0.0242,  0.1368])\n",
      "params:tensor([  5.2258, -16.5011])\n",
      "Epoch 1814, Loss 2.984215\n",
      "grad:tensor([-0.0241,  0.1365])\n",
      "params:tensor([  5.2260, -16.5025])\n",
      "Epoch 1815, Loss 2.984022\n",
      "grad:tensor([-0.0241,  0.1363])\n",
      "params:tensor([  5.2262, -16.5039])\n",
      "Epoch 1816, Loss 2.983831\n",
      "grad:tensor([-0.0240,  0.1361])\n",
      "params:tensor([  5.2265, -16.5052])\n",
      "Epoch 1817, Loss 2.983639\n",
      "grad:tensor([-0.0240,  0.1358])\n",
      "params:tensor([  5.2267, -16.5066])\n",
      "Epoch 1818, Loss 2.983449\n",
      "grad:tensor([-0.0239,  0.1356])\n",
      "params:tensor([  5.2270, -16.5079])\n",
      "Epoch 1819, Loss 2.983259\n",
      "grad:tensor([-0.0239,  0.1354])\n",
      "params:tensor([  5.2272, -16.5093])\n",
      "Epoch 1820, Loss 2.983073\n",
      "grad:tensor([-0.0239,  0.1351])\n",
      "params:tensor([  5.2274, -16.5107])\n",
      "Epoch 1821, Loss 2.982884\n",
      "grad:tensor([-0.0238,  0.1349])\n",
      "params:tensor([  5.2277, -16.5120])\n",
      "Epoch 1822, Loss 2.982697\n",
      "grad:tensor([-0.0238,  0.1347])\n",
      "params:tensor([  5.2279, -16.5133])\n",
      "Epoch 1823, Loss 2.982510\n",
      "grad:tensor([-0.0237,  0.1344])\n",
      "params:tensor([  5.2281, -16.5147])\n",
      "Epoch 1824, Loss 2.982322\n",
      "grad:tensor([-0.0237,  0.1342])\n",
      "params:tensor([  5.2284, -16.5160])\n",
      "Epoch 1825, Loss 2.982137\n",
      "grad:tensor([-0.0237,  0.1340])\n",
      "params:tensor([  5.2286, -16.5174])\n",
      "Epoch 1826, Loss 2.981953\n",
      "grad:tensor([-0.0236,  0.1338])\n",
      "params:tensor([  5.2289, -16.5187])\n",
      "Epoch 1827, Loss 2.981769\n",
      "grad:tensor([-0.0236,  0.1335])\n",
      "params:tensor([  5.2291, -16.5200])\n",
      "Epoch 1828, Loss 2.981586\n",
      "grad:tensor([-0.0236,  0.1333])\n",
      "params:tensor([  5.2293, -16.5214])\n",
      "Epoch 1829, Loss 2.981402\n",
      "grad:tensor([-0.0235,  0.1331])\n",
      "params:tensor([  5.2296, -16.5227])\n",
      "Epoch 1830, Loss 2.981219\n",
      "grad:tensor([-0.0235,  0.1329])\n",
      "params:tensor([  5.2298, -16.5240])\n",
      "Epoch 1831, Loss 2.981037\n",
      "grad:tensor([-0.0235,  0.1326])\n",
      "params:tensor([  5.2300, -16.5254])\n",
      "Epoch 1832, Loss 2.980856\n",
      "grad:tensor([-0.0234,  0.1324])\n",
      "params:tensor([  5.2303, -16.5267])\n",
      "Epoch 1833, Loss 2.980675\n",
      "grad:tensor([-0.0234,  0.1322])\n",
      "params:tensor([  5.2305, -16.5280])\n",
      "Epoch 1834, Loss 2.980495\n",
      "grad:tensor([-0.0233,  0.1320])\n",
      "params:tensor([  5.2307, -16.5293])\n",
      "Epoch 1835, Loss 2.980315\n",
      "grad:tensor([-0.0233,  0.1317])\n",
      "params:tensor([  5.2310, -16.5306])\n",
      "Epoch 1836, Loss 2.980137\n",
      "grad:tensor([-0.0232,  0.1315])\n",
      "params:tensor([  5.2312, -16.5320])\n",
      "Epoch 1837, Loss 2.979958\n",
      "grad:tensor([-0.0232,  0.1313])\n",
      "params:tensor([  5.2314, -16.5333])\n",
      "Epoch 1838, Loss 2.979782\n",
      "grad:tensor([-0.0232,  0.1311])\n",
      "params:tensor([  5.2317, -16.5346])\n",
      "Epoch 1839, Loss 2.979604\n",
      "grad:tensor([-0.0231,  0.1308])\n",
      "params:tensor([  5.2319, -16.5359])\n",
      "Epoch 1840, Loss 2.979428\n",
      "grad:tensor([-0.0231,  0.1306])\n",
      "params:tensor([  5.2321, -16.5372])\n",
      "Epoch 1841, Loss 2.979253\n",
      "grad:tensor([-0.0230,  0.1304])\n",
      "params:tensor([  5.2324, -16.5385])\n",
      "Epoch 1842, Loss 2.979078\n",
      "grad:tensor([-0.0230,  0.1302])\n",
      "params:tensor([  5.2326, -16.5398])\n",
      "Epoch 1843, Loss 2.978902\n",
      "grad:tensor([-0.0229,  0.1300])\n",
      "params:tensor([  5.2328, -16.5411])\n",
      "Epoch 1844, Loss 2.978729\n",
      "grad:tensor([-0.0229,  0.1297])\n",
      "params:tensor([  5.2330, -16.5424])\n",
      "Epoch 1845, Loss 2.978556\n",
      "grad:tensor([-0.0229,  0.1295])\n",
      "params:tensor([  5.2333, -16.5437])\n",
      "Epoch 1846, Loss 2.978382\n",
      "grad:tensor([-0.0228,  0.1293])\n",
      "params:tensor([  5.2335, -16.5450])\n",
      "Epoch 1847, Loss 2.978211\n",
      "grad:tensor([-0.0228,  0.1291])\n",
      "params:tensor([  5.2337, -16.5463])\n",
      "Epoch 1848, Loss 2.978039\n",
      "grad:tensor([-0.0228,  0.1288])\n",
      "params:tensor([  5.2340, -16.5476])\n",
      "Epoch 1849, Loss 2.977867\n",
      "grad:tensor([-0.0227,  0.1286])\n",
      "params:tensor([  5.2342, -16.5489])\n",
      "Epoch 1850, Loss 2.977696\n",
      "grad:tensor([-0.0227,  0.1284])\n",
      "params:tensor([  5.2344, -16.5501])\n",
      "Epoch 1851, Loss 2.977527\n",
      "grad:tensor([-0.0227,  0.1282])\n",
      "params:tensor([  5.2346, -16.5514])\n",
      "Epoch 1852, Loss 2.977357\n",
      "grad:tensor([-0.0226,  0.1280])\n",
      "params:tensor([  5.2349, -16.5527])\n",
      "Epoch 1853, Loss 2.977188\n",
      "grad:tensor([-0.0226,  0.1278])\n",
      "params:tensor([  5.2351, -16.5540])\n",
      "Epoch 1854, Loss 2.977021\n",
      "grad:tensor([-0.0225,  0.1275])\n",
      "params:tensor([  5.2353, -16.5553])\n",
      "Epoch 1855, Loss 2.976853\n",
      "grad:tensor([-0.0225,  0.1273])\n",
      "params:tensor([  5.2355, -16.5565])\n",
      "Epoch 1856, Loss 2.976687\n",
      "grad:tensor([-0.0225,  0.1271])\n",
      "params:tensor([  5.2358, -16.5578])\n",
      "Epoch 1857, Loss 2.976520\n",
      "grad:tensor([-0.0224,  0.1269])\n",
      "params:tensor([  5.2360, -16.5591])\n",
      "Epoch 1858, Loss 2.976354\n",
      "grad:tensor([-0.0224,  0.1267])\n",
      "params:tensor([  5.2362, -16.5603])\n",
      "Epoch 1859, Loss 2.976189\n",
      "grad:tensor([-0.0223,  0.1265])\n",
      "params:tensor([  5.2364, -16.5616])\n",
      "Epoch 1860, Loss 2.976023\n",
      "grad:tensor([-0.0223,  0.1263])\n",
      "params:tensor([  5.2367, -16.5629])\n",
      "Epoch 1861, Loss 2.975860\n",
      "grad:tensor([-0.0223,  0.1260])\n",
      "params:tensor([  5.2369, -16.5641])\n",
      "Epoch 1862, Loss 2.975697\n",
      "grad:tensor([-0.0222,  0.1258])\n",
      "params:tensor([  5.2371, -16.5654])\n",
      "Epoch 1863, Loss 2.975533\n",
      "grad:tensor([-0.0222,  0.1256])\n",
      "params:tensor([  5.2373, -16.5666])\n",
      "Epoch 1864, Loss 2.975369\n",
      "grad:tensor([-0.0222,  0.1254])\n",
      "params:tensor([  5.2375, -16.5679])\n",
      "Epoch 1865, Loss 2.975208\n",
      "grad:tensor([-0.0221,  0.1252])\n",
      "params:tensor([  5.2378, -16.5691])\n",
      "Epoch 1866, Loss 2.975046\n",
      "grad:tensor([-0.0221,  0.1250])\n",
      "params:tensor([  5.2380, -16.5704])\n",
      "Epoch 1867, Loss 2.974886\n",
      "grad:tensor([-0.0220,  0.1248])\n",
      "params:tensor([  5.2382, -16.5716])\n",
      "Epoch 1868, Loss 2.974725\n",
      "grad:tensor([-0.0220,  0.1245])\n",
      "params:tensor([  5.2384, -16.5729])\n",
      "Epoch 1869, Loss 2.974565\n",
      "grad:tensor([-0.0220,  0.1243])\n",
      "params:tensor([  5.2386, -16.5741])\n",
      "Epoch 1870, Loss 2.974406\n",
      "grad:tensor([-0.0219,  0.1241])\n",
      "params:tensor([  5.2389, -16.5754])\n",
      "Epoch 1871, Loss 2.974248\n",
      "grad:tensor([-0.0219,  0.1239])\n",
      "params:tensor([  5.2391, -16.5766])\n",
      "Epoch 1872, Loss 2.974088\n",
      "grad:tensor([-0.0219,  0.1237])\n",
      "params:tensor([  5.2393, -16.5778])\n",
      "Epoch 1873, Loss 2.973930\n",
      "grad:tensor([-0.0218,  0.1235])\n",
      "params:tensor([  5.2395, -16.5791])\n",
      "Epoch 1874, Loss 2.973776\n",
      "grad:tensor([-0.0218,  0.1233])\n",
      "params:tensor([  5.2397, -16.5803])\n",
      "Epoch 1875, Loss 2.973618\n",
      "grad:tensor([-0.0217,  0.1231])\n",
      "params:tensor([  5.2400, -16.5815])\n",
      "Epoch 1876, Loss 2.973463\n",
      "grad:tensor([-0.0217,  0.1229])\n",
      "params:tensor([  5.2402, -16.5828])\n",
      "Epoch 1877, Loss 2.973307\n",
      "grad:tensor([-0.0217,  0.1227])\n",
      "params:tensor([  5.2404, -16.5840])\n",
      "Epoch 1878, Loss 2.973151\n",
      "grad:tensor([-0.0216,  0.1224])\n",
      "params:tensor([  5.2406, -16.5852])\n",
      "Epoch 1879, Loss 2.972996\n",
      "grad:tensor([-0.0216,  0.1222])\n",
      "params:tensor([  5.2408, -16.5864])\n",
      "Epoch 1880, Loss 2.972843\n",
      "grad:tensor([-0.0215,  0.1220])\n",
      "params:tensor([  5.2410, -16.5877])\n",
      "Epoch 1881, Loss 2.972690\n",
      "grad:tensor([-0.0215,  0.1218])\n",
      "params:tensor([  5.2413, -16.5889])\n",
      "Epoch 1882, Loss 2.972536\n",
      "grad:tensor([-0.0215,  0.1216])\n",
      "params:tensor([  5.2415, -16.5901])\n",
      "Epoch 1883, Loss 2.972383\n",
      "grad:tensor([-0.0214,  0.1214])\n",
      "params:tensor([  5.2417, -16.5913])\n",
      "Epoch 1884, Loss 2.972232\n",
      "grad:tensor([-0.0214,  0.1212])\n",
      "params:tensor([  5.2419, -16.5925])\n",
      "Epoch 1885, Loss 2.972081\n",
      "grad:tensor([-0.0214,  0.1210])\n",
      "params:tensor([  5.2421, -16.5937])\n",
      "Epoch 1886, Loss 2.971931\n",
      "grad:tensor([-0.0213,  0.1208])\n",
      "params:tensor([  5.2423, -16.5949])\n",
      "Epoch 1887, Loss 2.971780\n",
      "grad:tensor([-0.0213,  0.1206])\n",
      "params:tensor([  5.2425, -16.5961])\n",
      "Epoch 1888, Loss 2.971630\n",
      "grad:tensor([-0.0213,  0.1204])\n",
      "params:tensor([  5.2427, -16.5974])\n",
      "Epoch 1889, Loss 2.971481\n",
      "grad:tensor([-0.0212,  0.1202])\n",
      "params:tensor([  5.2430, -16.5986])\n",
      "Epoch 1890, Loss 2.971332\n",
      "grad:tensor([-0.0212,  0.1200])\n",
      "params:tensor([  5.2432, -16.5998])\n",
      "Epoch 1891, Loss 2.971184\n",
      "grad:tensor([-0.0212,  0.1198])\n",
      "params:tensor([  5.2434, -16.6010])\n",
      "Epoch 1892, Loss 2.971035\n",
      "grad:tensor([-0.0211,  0.1196])\n",
      "params:tensor([  5.2436, -16.6021])\n",
      "Epoch 1893, Loss 2.970888\n",
      "grad:tensor([-0.0211,  0.1194])\n",
      "params:tensor([  5.2438, -16.6033])\n",
      "Epoch 1894, Loss 2.970741\n",
      "grad:tensor([-0.0211,  0.1192])\n",
      "params:tensor([  5.2440, -16.6045])\n",
      "Epoch 1895, Loss 2.970596\n",
      "grad:tensor([-0.0210,  0.1190])\n",
      "params:tensor([  5.2442, -16.6057])\n",
      "Epoch 1896, Loss 2.970449\n",
      "grad:tensor([-0.0210,  0.1188])\n",
      "params:tensor([  5.2444, -16.6069])\n",
      "Epoch 1897, Loss 2.970304\n",
      "grad:tensor([-0.0209,  0.1186])\n",
      "params:tensor([  5.2446, -16.6081])\n",
      "Epoch 1898, Loss 2.970159\n",
      "grad:tensor([-0.0209,  0.1183])\n",
      "params:tensor([  5.2449, -16.6093])\n",
      "Epoch 1899, Loss 2.970016\n",
      "grad:tensor([-0.0209,  0.1182])\n",
      "params:tensor([  5.2451, -16.6105])\n",
      "Epoch 1900, Loss 2.969871\n",
      "grad:tensor([-0.0208,  0.1180])\n",
      "params:tensor([  5.2453, -16.6116])\n",
      "Epoch 1901, Loss 2.969727\n",
      "grad:tensor([-0.0208,  0.1178])\n",
      "params:tensor([  5.2455, -16.6128])\n",
      "Epoch 1902, Loss 2.969586\n",
      "grad:tensor([-0.0208,  0.1175])\n",
      "params:tensor([  5.2457, -16.6140])\n",
      "Epoch 1903, Loss 2.969443\n",
      "grad:tensor([-0.0207,  0.1173])\n",
      "params:tensor([  5.2459, -16.6152])\n",
      "Epoch 1904, Loss 2.969302\n",
      "grad:tensor([-0.0207,  0.1172])\n",
      "params:tensor([  5.2461, -16.6163])\n",
      "Epoch 1905, Loss 2.969160\n",
      "grad:tensor([-0.0206,  0.1170])\n",
      "params:tensor([  5.2463, -16.6175])\n",
      "Epoch 1906, Loss 2.969017\n",
      "grad:tensor([-0.0206,  0.1168])\n",
      "params:tensor([  5.2465, -16.6187])\n",
      "Epoch 1907, Loss 2.968879\n",
      "grad:tensor([-0.0206,  0.1166])\n",
      "params:tensor([  5.2467, -16.6198])\n",
      "Epoch 1908, Loss 2.968739\n",
      "grad:tensor([-0.0205,  0.1164])\n",
      "params:tensor([  5.2469, -16.6210])\n",
      "Epoch 1909, Loss 2.968599\n",
      "grad:tensor([-0.0205,  0.1162])\n",
      "params:tensor([  5.2471, -16.6222])\n",
      "Epoch 1910, Loss 2.968460\n",
      "grad:tensor([-0.0205,  0.1160])\n",
      "params:tensor([  5.2473, -16.6233])\n",
      "Epoch 1911, Loss 2.968321\n",
      "grad:tensor([-0.0204,  0.1158])\n",
      "params:tensor([  5.2475, -16.6245])\n",
      "Epoch 1912, Loss 2.968183\n",
      "grad:tensor([-0.0204,  0.1156])\n",
      "params:tensor([  5.2477, -16.6256])\n",
      "Epoch 1913, Loss 2.968046\n",
      "grad:tensor([-0.0204,  0.1154])\n",
      "params:tensor([  5.2479, -16.6268])\n",
      "Epoch 1914, Loss 2.967908\n",
      "grad:tensor([-0.0204,  0.1152])\n",
      "params:tensor([  5.2482, -16.6279])\n",
      "Epoch 1915, Loss 2.967772\n",
      "grad:tensor([-0.0203,  0.1150])\n",
      "params:tensor([  5.2484, -16.6291])\n",
      "Epoch 1916, Loss 2.967636\n",
      "grad:tensor([-0.0203,  0.1148])\n",
      "params:tensor([  5.2486, -16.6302])\n",
      "Epoch 1917, Loss 2.967499\n",
      "grad:tensor([-0.0202,  0.1146])\n",
      "params:tensor([  5.2488, -16.6314])\n",
      "Epoch 1918, Loss 2.967365\n",
      "grad:tensor([-0.0202,  0.1144])\n",
      "params:tensor([  5.2490, -16.6325])\n",
      "Epoch 1919, Loss 2.967230\n",
      "grad:tensor([-0.0202,  0.1142])\n",
      "params:tensor([  5.2492, -16.6337])\n",
      "Epoch 1920, Loss 2.967095\n",
      "grad:tensor([-0.0202,  0.1140])\n",
      "params:tensor([  5.2494, -16.6348])\n",
      "Epoch 1921, Loss 2.966961\n",
      "grad:tensor([-0.0201,  0.1138])\n",
      "params:tensor([  5.2496, -16.6360])\n",
      "Epoch 1922, Loss 2.966828\n",
      "grad:tensor([-0.0201,  0.1136])\n",
      "params:tensor([  5.2498, -16.6371])\n",
      "Epoch 1923, Loss 2.966693\n",
      "grad:tensor([-0.0200,  0.1134])\n",
      "params:tensor([  5.2500, -16.6382])\n",
      "Epoch 1924, Loss 2.966561\n",
      "grad:tensor([-0.0200,  0.1132])\n",
      "params:tensor([  5.2502, -16.6394])\n",
      "Epoch 1925, Loss 2.966429\n",
      "grad:tensor([-0.0200,  0.1130])\n",
      "params:tensor([  5.2504, -16.6405])\n",
      "Epoch 1926, Loss 2.966297\n",
      "grad:tensor([-0.0199,  0.1128])\n",
      "params:tensor([  5.2506, -16.6416])\n",
      "Epoch 1927, Loss 2.966168\n",
      "grad:tensor([-0.0199,  0.1127])\n",
      "params:tensor([  5.2508, -16.6427])\n",
      "Epoch 1928, Loss 2.966036\n",
      "grad:tensor([-0.0199,  0.1125])\n",
      "params:tensor([  5.2510, -16.6439])\n",
      "Epoch 1929, Loss 2.965904\n",
      "grad:tensor([-0.0198,  0.1123])\n",
      "params:tensor([  5.2512, -16.6450])\n",
      "Epoch 1930, Loss 2.965777\n",
      "grad:tensor([-0.0198,  0.1121])\n",
      "params:tensor([  5.2514, -16.6461])\n",
      "Epoch 1931, Loss 2.965647\n",
      "grad:tensor([-0.0198,  0.1119])\n",
      "params:tensor([  5.2516, -16.6472])\n",
      "Epoch 1932, Loss 2.965516\n",
      "grad:tensor([-0.0197,  0.1117])\n",
      "params:tensor([  5.2518, -16.6484])\n",
      "Epoch 1933, Loss 2.965388\n",
      "grad:tensor([-0.0197,  0.1115])\n",
      "params:tensor([  5.2520, -16.6495])\n",
      "Epoch 1934, Loss 2.965261\n",
      "grad:tensor([-0.0197,  0.1113])\n",
      "params:tensor([  5.2522, -16.6506])\n",
      "Epoch 1935, Loss 2.965131\n",
      "grad:tensor([-0.0196,  0.1111])\n",
      "params:tensor([  5.2523, -16.6517])\n",
      "Epoch 1936, Loss 2.965006\n",
      "grad:tensor([-0.0196,  0.1109])\n",
      "params:tensor([  5.2525, -16.6528])\n",
      "Epoch 1937, Loss 2.964877\n",
      "grad:tensor([-0.0196,  0.1108])\n",
      "params:tensor([  5.2527, -16.6539])\n",
      "Epoch 1938, Loss 2.964751\n",
      "grad:tensor([-0.0195,  0.1106])\n",
      "params:tensor([  5.2529, -16.6550])\n",
      "Epoch 1939, Loss 2.964625\n",
      "grad:tensor([-0.0195,  0.1104])\n",
      "params:tensor([  5.2531, -16.6561])\n",
      "Epoch 1940, Loss 2.964500\n",
      "grad:tensor([-0.0195,  0.1102])\n",
      "params:tensor([  5.2533, -16.6572])\n",
      "Epoch 1941, Loss 2.964375\n",
      "grad:tensor([-0.0195,  0.1100])\n",
      "params:tensor([  5.2535, -16.6583])\n",
      "Epoch 1942, Loss 2.964250\n",
      "grad:tensor([-0.0194,  0.1098])\n",
      "params:tensor([  5.2537, -16.6594])\n",
      "Epoch 1943, Loss 2.964126\n",
      "grad:tensor([-0.0194,  0.1096])\n",
      "params:tensor([  5.2539, -16.6605])\n",
      "Epoch 1944, Loss 2.964001\n",
      "grad:tensor([-0.0194,  0.1094])\n",
      "params:tensor([  5.2541, -16.6616])\n",
      "Epoch 1945, Loss 2.963879\n",
      "grad:tensor([-0.0193,  0.1093])\n",
      "params:tensor([  5.2543, -16.6627])\n",
      "Epoch 1946, Loss 2.963756\n",
      "grad:tensor([-0.0193,  0.1091])\n",
      "params:tensor([  5.2545, -16.6638])\n",
      "Epoch 1947, Loss 2.963632\n",
      "grad:tensor([-0.0192,  0.1089])\n",
      "params:tensor([  5.2547, -16.6649])\n",
      "Epoch 1948, Loss 2.963511\n",
      "grad:tensor([-0.0192,  0.1087])\n",
      "params:tensor([  5.2549, -16.6660])\n",
      "Epoch 1949, Loss 2.963388\n",
      "grad:tensor([-0.0192,  0.1085])\n",
      "params:tensor([  5.2551, -16.6671])\n",
      "Epoch 1950, Loss 2.963266\n",
      "grad:tensor([-0.0191,  0.1083])\n",
      "params:tensor([  5.2553, -16.6681])\n",
      "Epoch 1951, Loss 2.963149\n",
      "grad:tensor([-0.0191,  0.1081])\n",
      "params:tensor([  5.2554, -16.6692])\n",
      "Epoch 1952, Loss 2.963026\n",
      "grad:tensor([-0.0191,  0.1080])\n",
      "params:tensor([  5.2556, -16.6703])\n",
      "Epoch 1953, Loss 2.962907\n",
      "grad:tensor([-0.0190,  0.1078])\n",
      "params:tensor([  5.2558, -16.6714])\n",
      "Epoch 1954, Loss 2.962788\n",
      "grad:tensor([-0.0190,  0.1076])\n",
      "params:tensor([  5.2560, -16.6725])\n",
      "Epoch 1955, Loss 2.962667\n",
      "grad:tensor([-0.0190,  0.1074])\n",
      "params:tensor([  5.2562, -16.6735])\n",
      "Epoch 1956, Loss 2.962547\n",
      "grad:tensor([-0.0189,  0.1072])\n",
      "params:tensor([  5.2564, -16.6746])\n",
      "Epoch 1957, Loss 2.962429\n",
      "grad:tensor([-0.0189,  0.1071])\n",
      "params:tensor([  5.2566, -16.6757])\n",
      "Epoch 1958, Loss 2.962312\n",
      "grad:tensor([-0.0189,  0.1069])\n",
      "params:tensor([  5.2568, -16.6767])\n",
      "Epoch 1959, Loss 2.962195\n",
      "grad:tensor([-0.0188,  0.1067])\n",
      "params:tensor([  5.2570, -16.6778])\n",
      "Epoch 1960, Loss 2.962078\n",
      "grad:tensor([-0.0188,  0.1065])\n",
      "params:tensor([  5.2572, -16.6789])\n",
      "Epoch 1961, Loss 2.961959\n",
      "grad:tensor([-0.0188,  0.1063])\n",
      "params:tensor([  5.2573, -16.6799])\n",
      "Epoch 1962, Loss 2.961843\n",
      "grad:tensor([-0.0187,  0.1062])\n",
      "params:tensor([  5.2575, -16.6810])\n",
      "Epoch 1963, Loss 2.961728\n",
      "grad:tensor([-0.0187,  0.1060])\n",
      "params:tensor([  5.2577, -16.6821])\n",
      "Epoch 1964, Loss 2.961611\n",
      "grad:tensor([-0.0187,  0.1058])\n",
      "params:tensor([  5.2579, -16.6831])\n",
      "Epoch 1965, Loss 2.961496\n",
      "grad:tensor([-0.0187,  0.1056])\n",
      "params:tensor([  5.2581, -16.6842])\n",
      "Epoch 1966, Loss 2.961382\n",
      "grad:tensor([-0.0186,  0.1054])\n",
      "params:tensor([  5.2583, -16.6852])\n",
      "Epoch 1967, Loss 2.961267\n",
      "grad:tensor([-0.0186,  0.1052])\n",
      "params:tensor([  5.2585, -16.6863])\n",
      "Epoch 1968, Loss 2.961153\n",
      "grad:tensor([-0.0186,  0.1051])\n",
      "params:tensor([  5.2586, -16.6873])\n",
      "Epoch 1969, Loss 2.961038\n",
      "grad:tensor([-0.0185,  0.1049])\n",
      "params:tensor([  5.2588, -16.6884])\n",
      "Epoch 1970, Loss 2.960926\n",
      "grad:tensor([-0.0185,  0.1047])\n",
      "params:tensor([  5.2590, -16.6894])\n",
      "Epoch 1971, Loss 2.960813\n",
      "grad:tensor([-0.0185,  0.1045])\n",
      "params:tensor([  5.2592, -16.6905])\n",
      "Epoch 1972, Loss 2.960700\n",
      "grad:tensor([-0.0184,  0.1044])\n",
      "params:tensor([  5.2594, -16.6915])\n",
      "Epoch 1973, Loss 2.960587\n",
      "grad:tensor([-0.0184,  0.1042])\n",
      "params:tensor([  5.2596, -16.6926])\n",
      "Epoch 1974, Loss 2.960475\n",
      "grad:tensor([-0.0184,  0.1040])\n",
      "params:tensor([  5.2598, -16.6936])\n",
      "Epoch 1975, Loss 2.960365\n",
      "grad:tensor([-0.0183,  0.1038])\n",
      "params:tensor([  5.2599, -16.6946])\n",
      "Epoch 1976, Loss 2.960255\n",
      "grad:tensor([-0.0183,  0.1037])\n",
      "params:tensor([  5.2601, -16.6957])\n",
      "Epoch 1977, Loss 2.960143\n",
      "grad:tensor([-0.0183,  0.1035])\n",
      "params:tensor([  5.2603, -16.6967])\n",
      "Epoch 1978, Loss 2.960033\n",
      "grad:tensor([-0.0182,  0.1033])\n",
      "params:tensor([  5.2605, -16.6977])\n",
      "Epoch 1979, Loss 2.959923\n",
      "grad:tensor([-0.0182,  0.1031])\n",
      "params:tensor([  5.2607, -16.6988])\n",
      "Epoch 1980, Loss 2.959812\n",
      "grad:tensor([-0.0182,  0.1029])\n",
      "params:tensor([  5.2608, -16.6998])\n",
      "Epoch 1981, Loss 2.959703\n",
      "grad:tensor([-0.0182,  0.1028])\n",
      "params:tensor([  5.2610, -16.7008])\n",
      "Epoch 1982, Loss 2.959594\n",
      "grad:tensor([-0.0181,  0.1026])\n",
      "params:tensor([  5.2612, -16.7019])\n",
      "Epoch 1983, Loss 2.959486\n",
      "grad:tensor([-0.0181,  0.1024])\n",
      "params:tensor([  5.2614, -16.7029])\n",
      "Epoch 1984, Loss 2.959378\n",
      "grad:tensor([-0.0181,  0.1022])\n",
      "params:tensor([  5.2616, -16.7039])\n",
      "Epoch 1985, Loss 2.959271\n",
      "grad:tensor([-0.0180,  0.1021])\n",
      "params:tensor([  5.2618, -16.7049])\n",
      "Epoch 1986, Loss 2.959162\n",
      "grad:tensor([-0.0180,  0.1019])\n",
      "params:tensor([  5.2619, -16.7059])\n",
      "Epoch 1987, Loss 2.959055\n",
      "grad:tensor([-0.0180,  0.1017])\n",
      "params:tensor([  5.2621, -16.7070])\n",
      "Epoch 1988, Loss 2.958950\n",
      "grad:tensor([-0.0179,  0.1016])\n",
      "params:tensor([  5.2623, -16.7080])\n",
      "Epoch 1989, Loss 2.958842\n",
      "grad:tensor([-0.0179,  0.1014])\n",
      "params:tensor([  5.2625, -16.7090])\n",
      "Epoch 1990, Loss 2.958738\n",
      "grad:tensor([-0.0179,  0.1012])\n",
      "params:tensor([  5.2626, -16.7100])\n",
      "Epoch 1991, Loss 2.958632\n",
      "grad:tensor([-0.0179,  0.1010])\n",
      "params:tensor([  5.2628, -16.7110])\n",
      "Epoch 1992, Loss 2.958526\n",
      "grad:tensor([-0.0178,  0.1009])\n",
      "params:tensor([  5.2630, -16.7120])\n",
      "Epoch 1993, Loss 2.958422\n",
      "grad:tensor([-0.0178,  0.1007])\n",
      "params:tensor([  5.2632, -16.7130])\n",
      "Epoch 1994, Loss 2.958317\n",
      "grad:tensor([-0.0178,  0.1005])\n",
      "params:tensor([  5.2634, -16.7140])\n",
      "Epoch 1995, Loss 2.958212\n",
      "grad:tensor([-0.0177,  0.1004])\n",
      "params:tensor([  5.2635, -16.7150])\n",
      "Epoch 1996, Loss 2.958109\n",
      "grad:tensor([-0.0177,  0.1002])\n",
      "params:tensor([  5.2637, -16.7160])\n",
      "Epoch 1997, Loss 2.958006\n",
      "grad:tensor([-0.0176,  0.1000])\n",
      "params:tensor([  5.2639, -16.7170])\n",
      "Epoch 1998, Loss 2.957904\n",
      "grad:tensor([-0.0176,  0.0998])\n",
      "params:tensor([  5.2641, -16.7180])\n",
      "Epoch 1999, Loss 2.957801\n",
      "grad:tensor([-0.0176,  0.0997])\n",
      "params:tensor([  5.2642, -16.7190])\n",
      "Epoch 2000, Loss 2.957698\n",
      "grad:tensor([-0.0176,  0.0995])\n",
      "params:tensor([  5.2644, -16.7200])\n",
      "Epoch 2001, Loss 2.957596\n",
      "grad:tensor([-0.0176,  0.0993])\n",
      "params:tensor([  5.2646, -16.7210])\n",
      "Epoch 2002, Loss 2.957494\n",
      "grad:tensor([-0.0175,  0.0992])\n",
      "params:tensor([  5.2648, -16.7220])\n",
      "Epoch 2003, Loss 2.957393\n",
      "grad:tensor([-0.0175,  0.0990])\n",
      "params:tensor([  5.2649, -16.7230])\n",
      "Epoch 2004, Loss 2.957292\n",
      "grad:tensor([-0.0174,  0.0988])\n",
      "params:tensor([  5.2651, -16.7240])\n",
      "Epoch 2005, Loss 2.957193\n",
      "grad:tensor([-0.0174,  0.0987])\n",
      "params:tensor([  5.2653, -16.7250])\n",
      "Epoch 2006, Loss 2.957091\n",
      "grad:tensor([-0.0174,  0.0985])\n",
      "params:tensor([  5.2655, -16.7260])\n",
      "Epoch 2007, Loss 2.956992\n",
      "grad:tensor([-0.0174,  0.0983])\n",
      "params:tensor([  5.2656, -16.7269])\n",
      "Epoch 2008, Loss 2.956892\n",
      "grad:tensor([-0.0173,  0.0982])\n",
      "params:tensor([  5.2658, -16.7279])\n",
      "Epoch 2009, Loss 2.956792\n",
      "grad:tensor([-0.0173,  0.0980])\n",
      "params:tensor([  5.2660, -16.7289])\n",
      "Epoch 2010, Loss 2.956694\n",
      "grad:tensor([-0.0173,  0.0978])\n",
      "params:tensor([  5.2662, -16.7299])\n",
      "Epoch 2011, Loss 2.956595\n",
      "grad:tensor([-0.0172,  0.0977])\n",
      "params:tensor([  5.2663, -16.7309])\n",
      "Epoch 2012, Loss 2.956496\n",
      "grad:tensor([-0.0172,  0.0975])\n",
      "params:tensor([  5.2665, -16.7318])\n",
      "Epoch 2013, Loss 2.956397\n",
      "grad:tensor([-0.0172,  0.0973])\n",
      "params:tensor([  5.2667, -16.7328])\n",
      "Epoch 2014, Loss 2.956300\n",
      "grad:tensor([-0.0172,  0.0972])\n",
      "params:tensor([  5.2668, -16.7338])\n",
      "Epoch 2015, Loss 2.956204\n",
      "grad:tensor([-0.0171,  0.0970])\n",
      "params:tensor([  5.2670, -16.7348])\n",
      "Epoch 2016, Loss 2.956108\n",
      "grad:tensor([-0.0171,  0.0968])\n",
      "params:tensor([  5.2672, -16.7357])\n",
      "Epoch 2017, Loss 2.956010\n",
      "grad:tensor([-0.0171,  0.0967])\n",
      "params:tensor([  5.2674, -16.7367])\n",
      "Epoch 2018, Loss 2.955914\n",
      "grad:tensor([-0.0171,  0.0965])\n",
      "params:tensor([  5.2675, -16.7377])\n",
      "Epoch 2019, Loss 2.955817\n",
      "grad:tensor([-0.0170,  0.0963])\n",
      "params:tensor([  5.2677, -16.7386])\n",
      "Epoch 2020, Loss 2.955722\n",
      "grad:tensor([-0.0170,  0.0962])\n",
      "params:tensor([  5.2679, -16.7396])\n",
      "Epoch 2021, Loss 2.955627\n",
      "grad:tensor([-0.0170,  0.0960])\n",
      "params:tensor([  5.2680, -16.7405])\n",
      "Epoch 2022, Loss 2.955533\n",
      "grad:tensor([-0.0169,  0.0959])\n",
      "params:tensor([  5.2682, -16.7415])\n",
      "Epoch 2023, Loss 2.955436\n",
      "grad:tensor([-0.0169,  0.0957])\n",
      "params:tensor([  5.2684, -16.7425])\n",
      "Epoch 2024, Loss 2.955343\n",
      "grad:tensor([-0.0169,  0.0955])\n",
      "params:tensor([  5.2686, -16.7434])\n",
      "Epoch 2025, Loss 2.955250\n",
      "grad:tensor([-0.0169,  0.0954])\n",
      "params:tensor([  5.2687, -16.7444])\n",
      "Epoch 2026, Loss 2.955154\n",
      "grad:tensor([-0.0168,  0.0952])\n",
      "params:tensor([  5.2689, -16.7453])\n",
      "Epoch 2027, Loss 2.955062\n",
      "grad:tensor([-0.0168,  0.0950])\n",
      "params:tensor([  5.2691, -16.7463])\n",
      "Epoch 2028, Loss 2.954969\n",
      "grad:tensor([-0.0168,  0.0949])\n",
      "params:tensor([  5.2692, -16.7472])\n",
      "Epoch 2029, Loss 2.954875\n",
      "grad:tensor([-0.0167,  0.0947])\n",
      "params:tensor([  5.2694, -16.7482])\n",
      "Epoch 2030, Loss 2.954783\n",
      "grad:tensor([-0.0167,  0.0946])\n",
      "params:tensor([  5.2696, -16.7491])\n",
      "Epoch 2031, Loss 2.954691\n",
      "grad:tensor([-0.0167,  0.0944])\n",
      "params:tensor([  5.2697, -16.7501])\n",
      "Epoch 2032, Loss 2.954600\n",
      "grad:tensor([-0.0167,  0.0942])\n",
      "params:tensor([  5.2699, -16.7510])\n",
      "Epoch 2033, Loss 2.954507\n",
      "grad:tensor([-0.0166,  0.0941])\n",
      "params:tensor([  5.2701, -16.7519])\n",
      "Epoch 2034, Loss 2.954417\n",
      "grad:tensor([-0.0166,  0.0939])\n",
      "params:tensor([  5.2702, -16.7529])\n",
      "Epoch 2035, Loss 2.954326\n",
      "grad:tensor([-0.0165,  0.0938])\n",
      "params:tensor([  5.2704, -16.7538])\n",
      "Epoch 2036, Loss 2.954235\n",
      "grad:tensor([-0.0165,  0.0936])\n",
      "params:tensor([  5.2706, -16.7547])\n",
      "Epoch 2037, Loss 2.954145\n",
      "grad:tensor([-0.0165,  0.0934])\n",
      "params:tensor([  5.2707, -16.7557])\n",
      "Epoch 2038, Loss 2.954055\n",
      "grad:tensor([-0.0165,  0.0933])\n",
      "params:tensor([  5.2709, -16.7566])\n",
      "Epoch 2039, Loss 2.953966\n",
      "grad:tensor([-0.0164,  0.0931])\n",
      "params:tensor([  5.2710, -16.7575])\n",
      "Epoch 2040, Loss 2.953876\n",
      "grad:tensor([-0.0164,  0.0930])\n",
      "params:tensor([  5.2712, -16.7585])\n",
      "Epoch 2041, Loss 2.953787\n",
      "grad:tensor([-0.0164,  0.0928])\n",
      "params:tensor([  5.2714, -16.7594])\n",
      "Epoch 2042, Loss 2.953698\n",
      "grad:tensor([-0.0164,  0.0926])\n",
      "params:tensor([  5.2715, -16.7603])\n",
      "Epoch 2043, Loss 2.953610\n",
      "grad:tensor([-0.0163,  0.0925])\n",
      "params:tensor([  5.2717, -16.7613])\n",
      "Epoch 2044, Loss 2.953521\n",
      "grad:tensor([-0.0163,  0.0923])\n",
      "params:tensor([  5.2719, -16.7622])\n",
      "Epoch 2045, Loss 2.953434\n",
      "grad:tensor([-0.0163,  0.0922])\n",
      "params:tensor([  5.2720, -16.7631])\n",
      "Epoch 2046, Loss 2.953346\n",
      "grad:tensor([-0.0163,  0.0920])\n",
      "params:tensor([  5.2722, -16.7640])\n",
      "Epoch 2047, Loss 2.953259\n",
      "grad:tensor([-0.0162,  0.0919])\n",
      "params:tensor([  5.2724, -16.7649])\n",
      "Epoch 2048, Loss 2.953171\n",
      "grad:tensor([-0.0162,  0.0917])\n",
      "params:tensor([  5.2725, -16.7659])\n",
      "Epoch 2049, Loss 2.953085\n",
      "grad:tensor([-0.0162,  0.0915])\n",
      "params:tensor([  5.2727, -16.7668])\n",
      "Epoch 2050, Loss 2.953000\n",
      "grad:tensor([-0.0162,  0.0914])\n",
      "params:tensor([  5.2728, -16.7677])\n",
      "Epoch 2051, Loss 2.952913\n",
      "grad:tensor([-0.0161,  0.0912])\n",
      "params:tensor([  5.2730, -16.7686])\n",
      "Epoch 2052, Loss 2.952828\n",
      "grad:tensor([-0.0161,  0.0911])\n",
      "params:tensor([  5.2732, -16.7695])\n",
      "Epoch 2053, Loss 2.952742\n",
      "grad:tensor([-0.0161,  0.0909])\n",
      "params:tensor([  5.2733, -16.7704])\n",
      "Epoch 2054, Loss 2.952657\n",
      "grad:tensor([-0.0160,  0.0908])\n",
      "params:tensor([  5.2735, -16.7713])\n",
      "Epoch 2055, Loss 2.952571\n",
      "grad:tensor([-0.0160,  0.0906])\n",
      "params:tensor([  5.2736, -16.7722])\n",
      "Epoch 2056, Loss 2.952487\n",
      "grad:tensor([-0.0160,  0.0905])\n",
      "params:tensor([  5.2738, -16.7731])\n",
      "Epoch 2057, Loss 2.952403\n",
      "grad:tensor([-0.0160,  0.0903])\n",
      "params:tensor([  5.2740, -16.7740])\n",
      "Epoch 2058, Loss 2.952318\n",
      "grad:tensor([-0.0159,  0.0902])\n",
      "params:tensor([  5.2741, -16.7749])\n",
      "Epoch 2059, Loss 2.952235\n",
      "grad:tensor([-0.0159,  0.0900])\n",
      "params:tensor([  5.2743, -16.7758])\n",
      "Epoch 2060, Loss 2.952152\n",
      "grad:tensor([-0.0159,  0.0899])\n",
      "params:tensor([  5.2744, -16.7767])\n",
      "Epoch 2061, Loss 2.952068\n",
      "grad:tensor([-0.0158,  0.0897])\n",
      "params:tensor([  5.2746, -16.7776])\n",
      "Epoch 2062, Loss 2.951985\n",
      "grad:tensor([-0.0158,  0.0895])\n",
      "params:tensor([  5.2748, -16.7785])\n",
      "Epoch 2063, Loss 2.951902\n",
      "grad:tensor([-0.0158,  0.0894])\n",
      "params:tensor([  5.2749, -16.7794])\n",
      "Epoch 2064, Loss 2.951820\n",
      "grad:tensor([-0.0158,  0.0892])\n",
      "params:tensor([  5.2751, -16.7803])\n",
      "Epoch 2065, Loss 2.951738\n",
      "grad:tensor([-0.0157,  0.0891])\n",
      "params:tensor([  5.2752, -16.7812])\n",
      "Epoch 2066, Loss 2.951656\n",
      "grad:tensor([-0.0157,  0.0889])\n",
      "params:tensor([  5.2754, -16.7821])\n",
      "Epoch 2067, Loss 2.951576\n",
      "grad:tensor([-0.0157,  0.0888])\n",
      "params:tensor([  5.2755, -16.7830])\n",
      "Epoch 2068, Loss 2.951494\n",
      "grad:tensor([-0.0157,  0.0886])\n",
      "params:tensor([  5.2757, -16.7839])\n",
      "Epoch 2069, Loss 2.951413\n",
      "grad:tensor([-0.0157,  0.0885])\n",
      "params:tensor([  5.2759, -16.7848])\n",
      "Epoch 2070, Loss 2.951333\n",
      "grad:tensor([-0.0156,  0.0883])\n",
      "params:tensor([  5.2760, -16.7856])\n",
      "Epoch 2071, Loss 2.951252\n",
      "grad:tensor([-0.0156,  0.0882])\n",
      "params:tensor([  5.2762, -16.7865])\n",
      "Epoch 2072, Loss 2.951171\n",
      "grad:tensor([-0.0155,  0.0880])\n",
      "params:tensor([  5.2763, -16.7874])\n",
      "Epoch 2073, Loss 2.951093\n",
      "grad:tensor([-0.0155,  0.0879])\n",
      "params:tensor([  5.2765, -16.7883])\n",
      "Epoch 2074, Loss 2.951012\n",
      "grad:tensor([-0.0155,  0.0877])\n",
      "params:tensor([  5.2766, -16.7892])\n",
      "Epoch 2075, Loss 2.950932\n",
      "grad:tensor([-0.0155,  0.0876])\n",
      "params:tensor([  5.2768, -16.7900])\n",
      "Epoch 2076, Loss 2.950853\n",
      "grad:tensor([-0.0154,  0.0874])\n",
      "params:tensor([  5.2769, -16.7909])\n",
      "Epoch 2077, Loss 2.950774\n",
      "grad:tensor([-0.0154,  0.0873])\n",
      "params:tensor([  5.2771, -16.7918])\n",
      "Epoch 2078, Loss 2.950697\n",
      "grad:tensor([-0.0154,  0.0871])\n",
      "params:tensor([  5.2772, -16.7927])\n",
      "Epoch 2079, Loss 2.950618\n",
      "grad:tensor([-0.0154,  0.0870])\n",
      "params:tensor([  5.2774, -16.7935])\n",
      "Epoch 2080, Loss 2.950540\n",
      "grad:tensor([-0.0154,  0.0868])\n",
      "params:tensor([  5.2776, -16.7944])\n",
      "Epoch 2081, Loss 2.950463\n",
      "grad:tensor([-0.0153,  0.0867])\n",
      "params:tensor([  5.2777, -16.7953])\n",
      "Epoch 2082, Loss 2.950385\n",
      "grad:tensor([-0.0153,  0.0866])\n",
      "params:tensor([  5.2779, -16.7961])\n",
      "Epoch 2083, Loss 2.950308\n",
      "grad:tensor([-0.0153,  0.0864])\n",
      "params:tensor([  5.2780, -16.7970])\n",
      "Epoch 2084, Loss 2.950231\n",
      "grad:tensor([-0.0152,  0.0863])\n",
      "params:tensor([  5.2782, -16.7979])\n",
      "Epoch 2085, Loss 2.950154\n",
      "grad:tensor([-0.0152,  0.0861])\n",
      "params:tensor([  5.2783, -16.7987])\n",
      "Epoch 2086, Loss 2.950078\n",
      "grad:tensor([-0.0152,  0.0860])\n",
      "params:tensor([  5.2785, -16.7996])\n",
      "Epoch 2087, Loss 2.950003\n",
      "grad:tensor([-0.0152,  0.0858])\n",
      "params:tensor([  5.2786, -16.8004])\n",
      "Epoch 2088, Loss 2.949925\n",
      "grad:tensor([-0.0152,  0.0857])\n",
      "params:tensor([  5.2788, -16.8013])\n",
      "Epoch 2089, Loss 2.949850\n",
      "grad:tensor([-0.0151,  0.0855])\n",
      "params:tensor([  5.2789, -16.8021])\n",
      "Epoch 2090, Loss 2.949776\n",
      "grad:tensor([-0.0151,  0.0854])\n",
      "params:tensor([  5.2791, -16.8030])\n",
      "Epoch 2091, Loss 2.949699\n",
      "grad:tensor([-0.0151,  0.0852])\n",
      "params:tensor([  5.2792, -16.8039])\n",
      "Epoch 2092, Loss 2.949626\n",
      "grad:tensor([-0.0150,  0.0851])\n",
      "params:tensor([  5.2794, -16.8047])\n",
      "Epoch 2093, Loss 2.949550\n",
      "grad:tensor([-0.0150,  0.0850])\n",
      "params:tensor([  5.2795, -16.8056])\n",
      "Epoch 2094, Loss 2.949476\n",
      "grad:tensor([-0.0150,  0.0848])\n",
      "params:tensor([  5.2797, -16.8064])\n",
      "Epoch 2095, Loss 2.949401\n",
      "grad:tensor([-0.0149,  0.0847])\n",
      "params:tensor([  5.2798, -16.8072])\n",
      "Epoch 2096, Loss 2.949328\n",
      "grad:tensor([-0.0150,  0.0845])\n",
      "params:tensor([  5.2800, -16.8081])\n",
      "Epoch 2097, Loss 2.949254\n",
      "grad:tensor([-0.0149,  0.0844])\n",
      "params:tensor([  5.2801, -16.8089])\n",
      "Epoch 2098, Loss 2.949182\n",
      "grad:tensor([-0.0149,  0.0842])\n",
      "params:tensor([  5.2803, -16.8098])\n",
      "Epoch 2099, Loss 2.949108\n",
      "grad:tensor([-0.0149,  0.0841])\n",
      "params:tensor([  5.2804, -16.8106])\n",
      "Epoch 2100, Loss 2.949035\n",
      "grad:tensor([-0.0148,  0.0839])\n",
      "params:tensor([  5.2806, -16.8115])\n",
      "Epoch 2101, Loss 2.948962\n",
      "grad:tensor([-0.0148,  0.0838])\n",
      "params:tensor([  5.2807, -16.8123])\n",
      "Epoch 2102, Loss 2.948890\n",
      "grad:tensor([-0.0148,  0.0837])\n",
      "params:tensor([  5.2809, -16.8131])\n",
      "Epoch 2103, Loss 2.948818\n",
      "grad:tensor([-0.0148,  0.0835])\n",
      "params:tensor([  5.2810, -16.8140])\n",
      "Epoch 2104, Loss 2.948745\n",
      "grad:tensor([-0.0148,  0.0834])\n",
      "params:tensor([  5.2812, -16.8148])\n",
      "Epoch 2105, Loss 2.948675\n",
      "grad:tensor([-0.0147,  0.0832])\n",
      "params:tensor([  5.2813, -16.8156])\n",
      "Epoch 2106, Loss 2.948602\n",
      "grad:tensor([-0.0147,  0.0831])\n",
      "params:tensor([  5.2815, -16.8165])\n",
      "Epoch 2107, Loss 2.948532\n",
      "grad:tensor([-0.0146,  0.0830])\n",
      "params:tensor([  5.2816, -16.8173])\n",
      "Epoch 2108, Loss 2.948462\n",
      "grad:tensor([-0.0146,  0.0828])\n",
      "params:tensor([  5.2817, -16.8181])\n",
      "Epoch 2109, Loss 2.948391\n",
      "grad:tensor([-0.0146,  0.0827])\n",
      "params:tensor([  5.2819, -16.8189])\n",
      "Epoch 2110, Loss 2.948321\n",
      "grad:tensor([-0.0146,  0.0825])\n",
      "params:tensor([  5.2820, -16.8198])\n",
      "Epoch 2111, Loss 2.948250\n",
      "grad:tensor([-0.0145,  0.0824])\n",
      "params:tensor([  5.2822, -16.8206])\n",
      "Epoch 2112, Loss 2.948181\n",
      "grad:tensor([-0.0145,  0.0823])\n",
      "params:tensor([  5.2823, -16.8214])\n",
      "Epoch 2113, Loss 2.948109\n",
      "grad:tensor([-0.0145,  0.0821])\n",
      "params:tensor([  5.2825, -16.8222])\n",
      "Epoch 2114, Loss 2.948041\n",
      "grad:tensor([-0.0145,  0.0820])\n",
      "params:tensor([  5.2826, -16.8231])\n",
      "Epoch 2115, Loss 2.947971\n",
      "grad:tensor([-0.0144,  0.0818])\n",
      "params:tensor([  5.2828, -16.8239])\n",
      "Epoch 2116, Loss 2.947902\n",
      "grad:tensor([-0.0144,  0.0817])\n",
      "params:tensor([  5.2829, -16.8247])\n",
      "Epoch 2117, Loss 2.947833\n",
      "grad:tensor([-0.0144,  0.0816])\n",
      "params:tensor([  5.2831, -16.8255])\n",
      "Epoch 2118, Loss 2.947765\n",
      "grad:tensor([-0.0144,  0.0814])\n",
      "params:tensor([  5.2832, -16.8263])\n",
      "Epoch 2119, Loss 2.947696\n",
      "grad:tensor([-0.0144,  0.0813])\n",
      "params:tensor([  5.2833, -16.8271])\n",
      "Epoch 2120, Loss 2.947628\n",
      "grad:tensor([-0.0143,  0.0811])\n",
      "params:tensor([  5.2835, -16.8280])\n",
      "Epoch 2121, Loss 2.947560\n",
      "grad:tensor([-0.0143,  0.0810])\n",
      "params:tensor([  5.2836, -16.8288])\n",
      "Epoch 2122, Loss 2.947494\n",
      "grad:tensor([-0.0143,  0.0809])\n",
      "params:tensor([  5.2838, -16.8296])\n",
      "Epoch 2123, Loss 2.947426\n",
      "grad:tensor([-0.0143,  0.0807])\n",
      "params:tensor([  5.2839, -16.8304])\n",
      "Epoch 2124, Loss 2.947357\n",
      "grad:tensor([-0.0142,  0.0806])\n",
      "params:tensor([  5.2841, -16.8312])\n",
      "Epoch 2125, Loss 2.947293\n",
      "grad:tensor([-0.0142,  0.0805])\n",
      "params:tensor([  5.2842, -16.8320])\n",
      "Epoch 2126, Loss 2.947225\n",
      "grad:tensor([-0.0142,  0.0803])\n",
      "params:tensor([  5.2843, -16.8328])\n",
      "Epoch 2127, Loss 2.947158\n",
      "grad:tensor([-0.0142,  0.0802])\n",
      "params:tensor([  5.2845, -16.8336])\n",
      "Epoch 2128, Loss 2.947092\n",
      "grad:tensor([-0.0141,  0.0800])\n",
      "params:tensor([  5.2846, -16.8344])\n",
      "Epoch 2129, Loss 2.947026\n",
      "grad:tensor([-0.0141,  0.0799])\n",
      "params:tensor([  5.2848, -16.8352])\n",
      "Epoch 2130, Loss 2.946960\n",
      "grad:tensor([-0.0141,  0.0798])\n",
      "params:tensor([  5.2849, -16.8360])\n",
      "Epoch 2131, Loss 2.946895\n",
      "grad:tensor([-0.0141,  0.0796])\n",
      "params:tensor([  5.2850, -16.8368])\n",
      "Epoch 2132, Loss 2.946830\n",
      "grad:tensor([-0.0141,  0.0795])\n",
      "params:tensor([  5.2852, -16.8376])\n",
      "Epoch 2133, Loss 2.946764\n",
      "grad:tensor([-0.0140,  0.0794])\n",
      "params:tensor([  5.2853, -16.8384])\n",
      "Epoch 2134, Loss 2.946700\n",
      "grad:tensor([-0.0140,  0.0792])\n",
      "params:tensor([  5.2855, -16.8392])\n",
      "Epoch 2135, Loss 2.946635\n",
      "grad:tensor([-0.0140,  0.0791])\n",
      "params:tensor([  5.2856, -16.8400])\n",
      "Epoch 2136, Loss 2.946571\n",
      "grad:tensor([-0.0139,  0.0790])\n",
      "params:tensor([  5.2857, -16.8407])\n",
      "Epoch 2137, Loss 2.946507\n",
      "grad:tensor([-0.0139,  0.0788])\n",
      "params:tensor([  5.2859, -16.8415])\n",
      "Epoch 2138, Loss 2.946442\n",
      "grad:tensor([-0.0139,  0.0787])\n",
      "params:tensor([  5.2860, -16.8423])\n",
      "Epoch 2139, Loss 2.946378\n",
      "grad:tensor([-0.0139,  0.0786])\n",
      "params:tensor([  5.2862, -16.8431])\n",
      "Epoch 2140, Loss 2.946314\n",
      "grad:tensor([-0.0138,  0.0784])\n",
      "params:tensor([  5.2863, -16.8439])\n",
      "Epoch 2141, Loss 2.946251\n",
      "grad:tensor([-0.0138,  0.0783])\n",
      "params:tensor([  5.2864, -16.8447])\n",
      "Epoch 2142, Loss 2.946189\n",
      "grad:tensor([-0.0138,  0.0782])\n",
      "params:tensor([  5.2866, -16.8455])\n",
      "Epoch 2143, Loss 2.946126\n",
      "grad:tensor([-0.0138,  0.0780])\n",
      "params:tensor([  5.2867, -16.8462])\n",
      "Epoch 2144, Loss 2.946063\n",
      "grad:tensor([-0.0138,  0.0779])\n",
      "params:tensor([  5.2869, -16.8470])\n",
      "Epoch 2145, Loss 2.946001\n",
      "grad:tensor([-0.0137,  0.0778])\n",
      "params:tensor([  5.2870, -16.8478])\n",
      "Epoch 2146, Loss 2.945937\n",
      "grad:tensor([-0.0137,  0.0776])\n",
      "params:tensor([  5.2871, -16.8486])\n",
      "Epoch 2147, Loss 2.945876\n",
      "grad:tensor([-0.0137,  0.0775])\n",
      "params:tensor([  5.2873, -16.8493])\n",
      "Epoch 2148, Loss 2.945815\n",
      "grad:tensor([-0.0137,  0.0774])\n",
      "params:tensor([  5.2874, -16.8501])\n",
      "Epoch 2149, Loss 2.945753\n",
      "grad:tensor([-0.0136,  0.0772])\n",
      "params:tensor([  5.2875, -16.8509])\n",
      "Epoch 2150, Loss 2.945690\n",
      "grad:tensor([-0.0136,  0.0771])\n",
      "params:tensor([  5.2877, -16.8517])\n",
      "Epoch 2151, Loss 2.945630\n",
      "grad:tensor([-0.0136,  0.0770])\n",
      "params:tensor([  5.2878, -16.8524])\n",
      "Epoch 2152, Loss 2.945567\n",
      "grad:tensor([-0.0136,  0.0768])\n",
      "params:tensor([  5.2879, -16.8532])\n",
      "Epoch 2153, Loss 2.945508\n",
      "grad:tensor([-0.0135,  0.0767])\n",
      "params:tensor([  5.2881, -16.8540])\n",
      "Epoch 2154, Loss 2.945447\n",
      "grad:tensor([-0.0135,  0.0766])\n",
      "params:tensor([  5.2882, -16.8547])\n",
      "Epoch 2155, Loss 2.945385\n",
      "grad:tensor([-0.0135,  0.0765])\n",
      "params:tensor([  5.2884, -16.8555])\n",
      "Epoch 2156, Loss 2.945325\n",
      "grad:tensor([-0.0135,  0.0763])\n",
      "params:tensor([  5.2885, -16.8563])\n",
      "Epoch 2157, Loss 2.945267\n",
      "grad:tensor([-0.0135,  0.0762])\n",
      "params:tensor([  5.2886, -16.8570])\n",
      "Epoch 2158, Loss 2.945206\n",
      "grad:tensor([-0.0134,  0.0761])\n",
      "params:tensor([  5.2888, -16.8578])\n",
      "Epoch 2159, Loss 2.945146\n",
      "grad:tensor([-0.0134,  0.0759])\n",
      "params:tensor([  5.2889, -16.8585])\n",
      "Epoch 2160, Loss 2.945088\n",
      "grad:tensor([-0.0134,  0.0758])\n",
      "params:tensor([  5.2890, -16.8593])\n",
      "Epoch 2161, Loss 2.945028\n",
      "grad:tensor([-0.0134,  0.0757])\n",
      "params:tensor([  5.2892, -16.8601])\n",
      "Epoch 2162, Loss 2.944969\n",
      "grad:tensor([-0.0133,  0.0755])\n",
      "params:tensor([  5.2893, -16.8608])\n",
      "Epoch 2163, Loss 2.944911\n",
      "grad:tensor([-0.0133,  0.0754])\n",
      "params:tensor([  5.2894, -16.8616])\n",
      "Epoch 2164, Loss 2.944852\n",
      "grad:tensor([-0.0133,  0.0753])\n",
      "params:tensor([  5.2896, -16.8623])\n",
      "Epoch 2165, Loss 2.944792\n",
      "grad:tensor([-0.0133,  0.0752])\n",
      "params:tensor([  5.2897, -16.8631])\n",
      "Epoch 2166, Loss 2.944736\n",
      "grad:tensor([-0.0133,  0.0750])\n",
      "params:tensor([  5.2898, -16.8638])\n",
      "Epoch 2167, Loss 2.944678\n",
      "grad:tensor([-0.0132,  0.0749])\n",
      "params:tensor([  5.2900, -16.8646])\n",
      "Epoch 2168, Loss 2.944619\n",
      "grad:tensor([-0.0132,  0.0748])\n",
      "params:tensor([  5.2901, -16.8653])\n",
      "Epoch 2169, Loss 2.944562\n",
      "grad:tensor([-0.0132,  0.0747])\n",
      "params:tensor([  5.2902, -16.8661])\n",
      "Epoch 2170, Loss 2.944504\n",
      "grad:tensor([-0.0132,  0.0745])\n",
      "params:tensor([  5.2903, -16.8668])\n",
      "Epoch 2171, Loss 2.944447\n",
      "grad:tensor([-0.0132,  0.0744])\n",
      "params:tensor([  5.2905, -16.8676])\n",
      "Epoch 2172, Loss 2.944391\n",
      "grad:tensor([-0.0131,  0.0743])\n",
      "params:tensor([  5.2906, -16.8683])\n",
      "Epoch 2173, Loss 2.944332\n",
      "grad:tensor([-0.0131,  0.0742])\n",
      "params:tensor([  5.2907, -16.8690])\n",
      "Epoch 2174, Loss 2.944276\n",
      "grad:tensor([-0.0131,  0.0740])\n",
      "params:tensor([  5.2909, -16.8698])\n",
      "Epoch 2175, Loss 2.944220\n",
      "grad:tensor([-0.0131,  0.0739])\n",
      "params:tensor([  5.2910, -16.8705])\n",
      "Epoch 2176, Loss 2.944164\n",
      "grad:tensor([-0.0130,  0.0738])\n",
      "params:tensor([  5.2911, -16.8713])\n",
      "Epoch 2177, Loss 2.944108\n",
      "grad:tensor([-0.0130,  0.0736])\n",
      "params:tensor([  5.2913, -16.8720])\n",
      "Epoch 2178, Loss 2.944053\n",
      "grad:tensor([-0.0130,  0.0735])\n",
      "params:tensor([  5.2914, -16.8727])\n",
      "Epoch 2179, Loss 2.943996\n",
      "grad:tensor([-0.0130,  0.0734])\n",
      "params:tensor([  5.2915, -16.8735])\n",
      "Epoch 2180, Loss 2.943941\n",
      "grad:tensor([-0.0129,  0.0733])\n",
      "params:tensor([  5.2917, -16.8742])\n",
      "Epoch 2181, Loss 2.943887\n",
      "grad:tensor([-0.0129,  0.0731])\n",
      "params:tensor([  5.2918, -16.8749])\n",
      "Epoch 2182, Loss 2.943831\n",
      "grad:tensor([-0.0129,  0.0730])\n",
      "params:tensor([  5.2919, -16.8757])\n",
      "Epoch 2183, Loss 2.943776\n",
      "grad:tensor([-0.0129,  0.0729])\n",
      "params:tensor([  5.2920, -16.8764])\n",
      "Epoch 2184, Loss 2.943721\n",
      "grad:tensor([-0.0129,  0.0728])\n",
      "params:tensor([  5.2922, -16.8771])\n",
      "Epoch 2185, Loss 2.943666\n",
      "grad:tensor([-0.0128,  0.0727])\n",
      "params:tensor([  5.2923, -16.8778])\n",
      "Epoch 2186, Loss 2.943613\n",
      "grad:tensor([-0.0128,  0.0725])\n",
      "params:tensor([  5.2924, -16.8786])\n",
      "Epoch 2187, Loss 2.943558\n",
      "grad:tensor([-0.0128,  0.0724])\n",
      "params:tensor([  5.2926, -16.8793])\n",
      "Epoch 2188, Loss 2.943503\n",
      "grad:tensor([-0.0128,  0.0723])\n",
      "params:tensor([  5.2927, -16.8800])\n",
      "Epoch 2189, Loss 2.943451\n",
      "grad:tensor([-0.0127,  0.0722])\n",
      "params:tensor([  5.2928, -16.8807])\n",
      "Epoch 2190, Loss 2.943395\n",
      "grad:tensor([-0.0127,  0.0720])\n",
      "params:tensor([  5.2929, -16.8815])\n",
      "Epoch 2191, Loss 2.943343\n",
      "grad:tensor([-0.0127,  0.0719])\n",
      "params:tensor([  5.2931, -16.8822])\n",
      "Epoch 2192, Loss 2.943290\n",
      "grad:tensor([-0.0127,  0.0718])\n",
      "params:tensor([  5.2932, -16.8829])\n",
      "Epoch 2193, Loss 2.943235\n",
      "grad:tensor([-0.0127,  0.0717])\n",
      "params:tensor([  5.2933, -16.8836])\n",
      "Epoch 2194, Loss 2.943183\n",
      "grad:tensor([-0.0126,  0.0715])\n",
      "params:tensor([  5.2934, -16.8843])\n",
      "Epoch 2195, Loss 2.943130\n",
      "grad:tensor([-0.0126,  0.0714])\n",
      "params:tensor([  5.2936, -16.8850])\n",
      "Epoch 2196, Loss 2.943079\n",
      "grad:tensor([-0.0126,  0.0713])\n",
      "params:tensor([  5.2937, -16.8857])\n",
      "Epoch 2197, Loss 2.943027\n",
      "grad:tensor([-0.0126,  0.0712])\n",
      "params:tensor([  5.2938, -16.8865])\n",
      "Epoch 2198, Loss 2.942973\n",
      "grad:tensor([-0.0126,  0.0711])\n",
      "params:tensor([  5.2939, -16.8872])\n",
      "Epoch 2199, Loss 2.942922\n",
      "grad:tensor([-0.0125,  0.0709])\n",
      "params:tensor([  5.2941, -16.8879])\n",
      "Epoch 2200, Loss 2.942870\n",
      "grad:tensor([-0.0125,  0.0708])\n",
      "params:tensor([  5.2942, -16.8886])\n",
      "Epoch 2201, Loss 2.942818\n",
      "grad:tensor([-0.0125,  0.0707])\n",
      "params:tensor([  5.2943, -16.8893])\n",
      "Epoch 2202, Loss 2.942766\n",
      "grad:tensor([-0.0125,  0.0706])\n",
      "params:tensor([  5.2944, -16.8900])\n",
      "Epoch 2203, Loss 2.942714\n",
      "grad:tensor([-0.0124,  0.0705])\n",
      "params:tensor([  5.2946, -16.8907])\n",
      "Epoch 2204, Loss 2.942665\n",
      "grad:tensor([-0.0124,  0.0703])\n",
      "params:tensor([  5.2947, -16.8914])\n",
      "Epoch 2205, Loss 2.942612\n",
      "grad:tensor([-0.0124,  0.0702])\n",
      "params:tensor([  5.2948, -16.8921])\n",
      "Epoch 2206, Loss 2.942564\n",
      "grad:tensor([-0.0124,  0.0701])\n",
      "params:tensor([  5.2949, -16.8928])\n",
      "Epoch 2207, Loss 2.942510\n",
      "grad:tensor([-0.0124,  0.0700])\n",
      "params:tensor([  5.2951, -16.8935])\n",
      "Epoch 2208, Loss 2.942461\n",
      "grad:tensor([-0.0123,  0.0699])\n",
      "params:tensor([  5.2952, -16.8942])\n",
      "Epoch 2209, Loss 2.942411\n",
      "grad:tensor([-0.0123,  0.0697])\n",
      "params:tensor([  5.2953, -16.8949])\n",
      "Epoch 2210, Loss 2.942361\n",
      "grad:tensor([-0.0123,  0.0696])\n",
      "params:tensor([  5.2954, -16.8956])\n",
      "Epoch 2211, Loss 2.942310\n",
      "grad:tensor([-0.0123,  0.0695])\n",
      "params:tensor([  5.2956, -16.8963])\n",
      "Epoch 2212, Loss 2.942261\n",
      "grad:tensor([-0.0122,  0.0694])\n",
      "params:tensor([  5.2957, -16.8970])\n",
      "Epoch 2213, Loss 2.942211\n",
      "grad:tensor([-0.0122,  0.0693])\n",
      "params:tensor([  5.2958, -16.8977])\n",
      "Epoch 2214, Loss 2.942162\n",
      "grad:tensor([-0.0122,  0.0692])\n",
      "params:tensor([  5.2959, -16.8984])\n",
      "Epoch 2215, Loss 2.942112\n",
      "grad:tensor([-0.0122,  0.0690])\n",
      "params:tensor([  5.2960, -16.8991])\n",
      "Epoch 2216, Loss 2.942062\n",
      "grad:tensor([-0.0122,  0.0689])\n",
      "params:tensor([  5.2962, -16.8998])\n",
      "Epoch 2217, Loss 2.942014\n",
      "grad:tensor([-0.0122,  0.0688])\n",
      "params:tensor([  5.2963, -16.9004])\n",
      "Epoch 2218, Loss 2.941965\n",
      "grad:tensor([-0.0121,  0.0687])\n",
      "params:tensor([  5.2964, -16.9011])\n",
      "Epoch 2219, Loss 2.941918\n",
      "grad:tensor([-0.0121,  0.0686])\n",
      "params:tensor([  5.2965, -16.9018])\n",
      "Epoch 2220, Loss 2.941868\n",
      "grad:tensor([-0.0121,  0.0685])\n",
      "params:tensor([  5.2967, -16.9025])\n",
      "Epoch 2221, Loss 2.941821\n",
      "grad:tensor([-0.0121,  0.0683])\n",
      "params:tensor([  5.2968, -16.9032])\n",
      "Epoch 2222, Loss 2.941773\n",
      "grad:tensor([-0.0120,  0.0682])\n",
      "params:tensor([  5.2969, -16.9039])\n",
      "Epoch 2223, Loss 2.941724\n",
      "grad:tensor([-0.0120,  0.0681])\n",
      "params:tensor([  5.2970, -16.9046])\n",
      "Epoch 2224, Loss 2.941677\n",
      "grad:tensor([-0.0120,  0.0680])\n",
      "params:tensor([  5.2971, -16.9052])\n",
      "Epoch 2225, Loss 2.941629\n",
      "grad:tensor([-0.0120,  0.0679])\n",
      "params:tensor([  5.2973, -16.9059])\n",
      "Epoch 2226, Loss 2.941582\n",
      "grad:tensor([-0.0120,  0.0678])\n",
      "params:tensor([  5.2974, -16.9066])\n",
      "Epoch 2227, Loss 2.941534\n",
      "grad:tensor([-0.0119,  0.0676])\n",
      "params:tensor([  5.2975, -16.9073])\n",
      "Epoch 2228, Loss 2.941488\n",
      "grad:tensor([-0.0119,  0.0675])\n",
      "params:tensor([  5.2976, -16.9079])\n",
      "Epoch 2229, Loss 2.941440\n",
      "grad:tensor([-0.0119,  0.0674])\n",
      "params:tensor([  5.2977, -16.9086])\n",
      "Epoch 2230, Loss 2.941393\n",
      "grad:tensor([-0.0119,  0.0673])\n",
      "params:tensor([  5.2979, -16.9093])\n",
      "Epoch 2231, Loss 2.941346\n",
      "grad:tensor([-0.0119,  0.0672])\n",
      "params:tensor([  5.2980, -16.9100])\n",
      "Epoch 2232, Loss 2.941299\n",
      "grad:tensor([-0.0118,  0.0671])\n",
      "params:tensor([  5.2981, -16.9106])\n",
      "Epoch 2233, Loss 2.941253\n",
      "grad:tensor([-0.0118,  0.0670])\n",
      "params:tensor([  5.2982, -16.9113])\n",
      "Epoch 2234, Loss 2.941206\n",
      "grad:tensor([-0.0118,  0.0668])\n",
      "params:tensor([  5.2983, -16.9120])\n",
      "Epoch 2235, Loss 2.941163\n",
      "grad:tensor([-0.0118,  0.0667])\n",
      "params:tensor([  5.2984, -16.9126])\n",
      "Epoch 2236, Loss 2.941116\n",
      "grad:tensor([-0.0118,  0.0666])\n",
      "params:tensor([  5.2986, -16.9133])\n",
      "Epoch 2237, Loss 2.941070\n",
      "grad:tensor([-0.0117,  0.0665])\n",
      "params:tensor([  5.2987, -16.9140])\n",
      "Epoch 2238, Loss 2.941025\n",
      "grad:tensor([-0.0117,  0.0664])\n",
      "params:tensor([  5.2988, -16.9146])\n",
      "Epoch 2239, Loss 2.940979\n",
      "grad:tensor([-0.0117,  0.0663])\n",
      "params:tensor([  5.2989, -16.9153])\n",
      "Epoch 2240, Loss 2.940933\n",
      "grad:tensor([-0.0117,  0.0662])\n",
      "params:tensor([  5.2990, -16.9160])\n",
      "Epoch 2241, Loss 2.940890\n",
      "grad:tensor([-0.0117,  0.0661])\n",
      "params:tensor([  5.2991, -16.9166])\n",
      "Epoch 2242, Loss 2.940844\n",
      "grad:tensor([-0.0117,  0.0659])\n",
      "params:tensor([  5.2993, -16.9173])\n",
      "Epoch 2243, Loss 2.940798\n",
      "grad:tensor([-0.0116,  0.0658])\n",
      "params:tensor([  5.2994, -16.9179])\n",
      "Epoch 2244, Loss 2.940753\n",
      "grad:tensor([-0.0116,  0.0657])\n",
      "params:tensor([  5.2995, -16.9186])\n",
      "Epoch 2245, Loss 2.940711\n",
      "grad:tensor([-0.0116,  0.0656])\n",
      "params:tensor([  5.2996, -16.9192])\n",
      "Epoch 2246, Loss 2.940666\n",
      "grad:tensor([-0.0116,  0.0655])\n",
      "params:tensor([  5.2997, -16.9199])\n",
      "Epoch 2247, Loss 2.940621\n",
      "grad:tensor([-0.0115,  0.0654])\n",
      "params:tensor([  5.2998, -16.9206])\n",
      "Epoch 2248, Loss 2.940576\n",
      "grad:tensor([-0.0115,  0.0653])\n",
      "params:tensor([  5.3000, -16.9212])\n",
      "Epoch 2249, Loss 2.940533\n",
      "grad:tensor([-0.0115,  0.0652])\n",
      "params:tensor([  5.3001, -16.9219])\n",
      "Epoch 2250, Loss 2.940489\n",
      "grad:tensor([-0.0115,  0.0650])\n",
      "params:tensor([  5.3002, -16.9225])\n",
      "Epoch 2251, Loss 2.940446\n",
      "grad:tensor([-0.0115,  0.0649])\n",
      "params:tensor([  5.3003, -16.9232])\n",
      "Epoch 2252, Loss 2.940403\n",
      "grad:tensor([-0.0114,  0.0648])\n",
      "params:tensor([  5.3004, -16.9238])\n",
      "Epoch 2253, Loss 2.940358\n",
      "grad:tensor([-0.0114,  0.0647])\n",
      "params:tensor([  5.3005, -16.9245])\n",
      "Epoch 2254, Loss 2.940316\n",
      "grad:tensor([-0.0114,  0.0646])\n",
      "params:tensor([  5.3006, -16.9251])\n",
      "Epoch 2255, Loss 2.940274\n",
      "grad:tensor([-0.0114,  0.0645])\n",
      "params:tensor([  5.3008, -16.9257])\n",
      "Epoch 2256, Loss 2.940229\n",
      "grad:tensor([-0.0114,  0.0644])\n",
      "params:tensor([  5.3009, -16.9264])\n",
      "Epoch 2257, Loss 2.940188\n",
      "grad:tensor([-0.0114,  0.0643])\n",
      "params:tensor([  5.3010, -16.9270])\n",
      "Epoch 2258, Loss 2.940144\n",
      "grad:tensor([-0.0114,  0.0642])\n",
      "params:tensor([  5.3011, -16.9277])\n",
      "Epoch 2259, Loss 2.940102\n",
      "grad:tensor([-0.0113,  0.0641])\n",
      "params:tensor([  5.3012, -16.9283])\n",
      "Epoch 2260, Loss 2.940060\n",
      "grad:tensor([-0.0113,  0.0640])\n",
      "params:tensor([  5.3013, -16.9290])\n",
      "Epoch 2261, Loss 2.940018\n",
      "grad:tensor([-0.0113,  0.0638])\n",
      "params:tensor([  5.3014, -16.9296])\n",
      "Epoch 2262, Loss 2.939977\n",
      "grad:tensor([-0.0113,  0.0637])\n",
      "params:tensor([  5.3016, -16.9302])\n",
      "Epoch 2263, Loss 2.939934\n",
      "grad:tensor([-0.0112,  0.0636])\n",
      "params:tensor([  5.3017, -16.9309])\n",
      "Epoch 2264, Loss 2.939891\n",
      "grad:tensor([-0.0112,  0.0635])\n",
      "params:tensor([  5.3018, -16.9315])\n",
      "Epoch 2265, Loss 2.939851\n",
      "grad:tensor([-0.0112,  0.0634])\n",
      "params:tensor([  5.3019, -16.9321])\n",
      "Epoch 2266, Loss 2.939809\n",
      "grad:tensor([-0.0112,  0.0633])\n",
      "params:tensor([  5.3020, -16.9328])\n",
      "Epoch 2267, Loss 2.939770\n",
      "grad:tensor([-0.0112,  0.0632])\n",
      "params:tensor([  5.3021, -16.9334])\n",
      "Epoch 2268, Loss 2.939727\n",
      "grad:tensor([-0.0111,  0.0631])\n",
      "params:tensor([  5.3022, -16.9340])\n",
      "Epoch 2269, Loss 2.939686\n",
      "grad:tensor([-0.0111,  0.0630])\n",
      "params:tensor([  5.3023, -16.9347])\n",
      "Epoch 2270, Loss 2.939646\n",
      "grad:tensor([-0.0111,  0.0629])\n",
      "params:tensor([  5.3024, -16.9353])\n",
      "Epoch 2271, Loss 2.939605\n",
      "grad:tensor([-0.0111,  0.0628])\n",
      "params:tensor([  5.3026, -16.9359])\n",
      "Epoch 2272, Loss 2.939566\n",
      "grad:tensor([-0.0111,  0.0627])\n",
      "params:tensor([  5.3027, -16.9365])\n",
      "Epoch 2273, Loss 2.939522\n",
      "grad:tensor([-0.0111,  0.0626])\n",
      "params:tensor([  5.3028, -16.9372])\n",
      "Epoch 2274, Loss 2.939483\n",
      "grad:tensor([-0.0110,  0.0624])\n",
      "params:tensor([  5.3029, -16.9378])\n",
      "Epoch 2275, Loss 2.939443\n",
      "grad:tensor([-0.0110,  0.0623])\n",
      "params:tensor([  5.3030, -16.9384])\n",
      "Epoch 2276, Loss 2.939403\n",
      "grad:tensor([-0.0110,  0.0622])\n",
      "params:tensor([  5.3031, -16.9390])\n",
      "Epoch 2277, Loss 2.939361\n",
      "grad:tensor([-0.0110,  0.0621])\n",
      "params:tensor([  5.3032, -16.9397])\n",
      "Epoch 2278, Loss 2.939323\n",
      "grad:tensor([-0.0110,  0.0620])\n",
      "params:tensor([  5.3033, -16.9403])\n",
      "Epoch 2279, Loss 2.939282\n",
      "grad:tensor([-0.0109,  0.0619])\n",
      "params:tensor([  5.3034, -16.9409])\n",
      "Epoch 2280, Loss 2.939243\n",
      "grad:tensor([-0.0109,  0.0618])\n",
      "params:tensor([  5.3035, -16.9415])\n",
      "Epoch 2281, Loss 2.939205\n",
      "grad:tensor([-0.0109,  0.0617])\n",
      "params:tensor([  5.3037, -16.9421])\n",
      "Epoch 2282, Loss 2.939165\n",
      "grad:tensor([-0.0109,  0.0616])\n",
      "params:tensor([  5.3038, -16.9428])\n",
      "Epoch 2283, Loss 2.939127\n",
      "grad:tensor([-0.0109,  0.0615])\n",
      "params:tensor([  5.3039, -16.9434])\n",
      "Epoch 2284, Loss 2.939087\n",
      "grad:tensor([-0.0108,  0.0614])\n",
      "params:tensor([  5.3040, -16.9440])\n",
      "Epoch 2285, Loss 2.939049\n",
      "grad:tensor([-0.0108,  0.0613])\n",
      "params:tensor([  5.3041, -16.9446])\n",
      "Epoch 2286, Loss 2.939011\n",
      "grad:tensor([-0.0108,  0.0612])\n",
      "params:tensor([  5.3042, -16.9452])\n",
      "Epoch 2287, Loss 2.938971\n",
      "grad:tensor([-0.0108,  0.0611])\n",
      "params:tensor([  5.3043, -16.9458])\n",
      "Epoch 2288, Loss 2.938933\n",
      "grad:tensor([-0.0108,  0.0610])\n",
      "params:tensor([  5.3044, -16.9464])\n",
      "Epoch 2289, Loss 2.938893\n",
      "grad:tensor([-0.0108,  0.0609])\n",
      "params:tensor([  5.3045, -16.9470])\n",
      "Epoch 2290, Loss 2.938857\n",
      "grad:tensor([-0.0107,  0.0608])\n",
      "params:tensor([  5.3046, -16.9476])\n",
      "Epoch 2291, Loss 2.938820\n",
      "grad:tensor([-0.0107,  0.0607])\n",
      "params:tensor([  5.3047, -16.9482])\n",
      "Epoch 2292, Loss 2.938779\n",
      "grad:tensor([-0.0107,  0.0606])\n",
      "params:tensor([  5.3048, -16.9489])\n",
      "Epoch 2293, Loss 2.938743\n",
      "grad:tensor([-0.0107,  0.0605])\n",
      "params:tensor([  5.3049, -16.9495])\n",
      "Epoch 2294, Loss 2.938705\n",
      "grad:tensor([-0.0107,  0.0604])\n",
      "params:tensor([  5.3051, -16.9501])\n",
      "Epoch 2295, Loss 2.938667\n",
      "grad:tensor([-0.0106,  0.0603])\n",
      "params:tensor([  5.3052, -16.9507])\n",
      "Epoch 2296, Loss 2.938629\n",
      "grad:tensor([-0.0106,  0.0602])\n",
      "params:tensor([  5.3053, -16.9513])\n",
      "Epoch 2297, Loss 2.938593\n",
      "grad:tensor([-0.0106,  0.0601])\n",
      "params:tensor([  5.3054, -16.9519])\n",
      "Epoch 2298, Loss 2.938555\n",
      "grad:tensor([-0.0106,  0.0600])\n",
      "params:tensor([  5.3055, -16.9525])\n",
      "Epoch 2299, Loss 2.938519\n",
      "grad:tensor([-0.0106,  0.0598])\n",
      "params:tensor([  5.3056, -16.9531])\n",
      "Epoch 2300, Loss 2.938481\n",
      "grad:tensor([-0.0106,  0.0597])\n",
      "params:tensor([  5.3057, -16.9537])\n",
      "Epoch 2301, Loss 2.938444\n",
      "grad:tensor([-0.0105,  0.0596])\n",
      "params:tensor([  5.3058, -16.9543])\n",
      "Epoch 2302, Loss 2.938408\n",
      "grad:tensor([-0.0105,  0.0595])\n",
      "params:tensor([  5.3059, -16.9549])\n",
      "Epoch 2303, Loss 2.938371\n",
      "grad:tensor([-0.0105,  0.0594])\n",
      "params:tensor([  5.3060, -16.9554])\n",
      "Epoch 2304, Loss 2.938335\n",
      "grad:tensor([-0.0105,  0.0593])\n",
      "params:tensor([  5.3061, -16.9560])\n",
      "Epoch 2305, Loss 2.938299\n",
      "grad:tensor([-0.0105,  0.0592])\n",
      "params:tensor([  5.3062, -16.9566])\n",
      "Epoch 2306, Loss 2.938263\n",
      "grad:tensor([-0.0105,  0.0591])\n",
      "params:tensor([  5.3063, -16.9572])\n",
      "Epoch 2307, Loss 2.938227\n",
      "grad:tensor([-0.0104,  0.0590])\n",
      "params:tensor([  5.3064, -16.9578])\n",
      "Epoch 2308, Loss 2.938190\n",
      "grad:tensor([-0.0104,  0.0589])\n",
      "params:tensor([  5.3065, -16.9584])\n",
      "Epoch 2309, Loss 2.938155\n",
      "grad:tensor([-0.0104,  0.0588])\n",
      "params:tensor([  5.3066, -16.9590])\n",
      "Epoch 2310, Loss 2.938118\n",
      "grad:tensor([-0.0104,  0.0587])\n",
      "params:tensor([  5.3067, -16.9596])\n",
      "Epoch 2311, Loss 2.938084\n",
      "grad:tensor([-0.0104,  0.0586])\n",
      "params:tensor([  5.3068, -16.9602])\n",
      "Epoch 2312, Loss 2.938049\n",
      "grad:tensor([-0.0103,  0.0585])\n",
      "params:tensor([  5.3069, -16.9608])\n",
      "Epoch 2313, Loss 2.938014\n",
      "grad:tensor([-0.0103,  0.0584])\n",
      "params:tensor([  5.3070, -16.9613])\n",
      "Epoch 2314, Loss 2.937977\n",
      "grad:tensor([-0.0103,  0.0583])\n",
      "params:tensor([  5.3072, -16.9619])\n",
      "Epoch 2315, Loss 2.937943\n",
      "grad:tensor([-0.0103,  0.0582])\n",
      "params:tensor([  5.3073, -16.9625])\n",
      "Epoch 2316, Loss 2.937908\n",
      "grad:tensor([-0.0103,  0.0581])\n",
      "params:tensor([  5.3074, -16.9631])\n",
      "Epoch 2317, Loss 2.937872\n",
      "grad:tensor([-0.0103,  0.0580])\n",
      "params:tensor([  5.3075, -16.9637])\n",
      "Epoch 2318, Loss 2.937839\n",
      "grad:tensor([-0.0102,  0.0580])\n",
      "params:tensor([  5.3076, -16.9642])\n",
      "Epoch 2319, Loss 2.937804\n",
      "grad:tensor([-0.0102,  0.0578])\n",
      "params:tensor([  5.3077, -16.9648])\n",
      "Epoch 2320, Loss 2.937769\n",
      "grad:tensor([-0.0102,  0.0578])\n",
      "params:tensor([  5.3078, -16.9654])\n",
      "Epoch 2321, Loss 2.937734\n",
      "grad:tensor([-0.0102,  0.0577])\n",
      "params:tensor([  5.3079, -16.9660])\n",
      "Epoch 2322, Loss 2.937700\n",
      "grad:tensor([-0.0102,  0.0576])\n",
      "params:tensor([  5.3080, -16.9666])\n",
      "Epoch 2323, Loss 2.937665\n",
      "grad:tensor([-0.0102,  0.0575])\n",
      "params:tensor([  5.3081, -16.9671])\n",
      "Epoch 2324, Loss 2.937632\n",
      "grad:tensor([-0.0101,  0.0574])\n",
      "params:tensor([  5.3082, -16.9677])\n",
      "Epoch 2325, Loss 2.937598\n",
      "grad:tensor([-0.0101,  0.0573])\n",
      "params:tensor([  5.3083, -16.9683])\n",
      "Epoch 2326, Loss 2.937565\n",
      "grad:tensor([-0.0101,  0.0572])\n",
      "params:tensor([  5.3084, -16.9688])\n",
      "Epoch 2327, Loss 2.937531\n",
      "grad:tensor([-0.0101,  0.0571])\n",
      "params:tensor([  5.3085, -16.9694])\n",
      "Epoch 2328, Loss 2.937499\n",
      "grad:tensor([-0.0101,  0.0570])\n",
      "params:tensor([  5.3086, -16.9700])\n",
      "Epoch 2329, Loss 2.937465\n",
      "grad:tensor([-0.0101,  0.0569])\n",
      "params:tensor([  5.3087, -16.9706])\n",
      "Epoch 2330, Loss 2.937430\n",
      "grad:tensor([-0.0100,  0.0568])\n",
      "params:tensor([  5.3088, -16.9711])\n",
      "Epoch 2331, Loss 2.937398\n",
      "grad:tensor([-0.0100,  0.0567])\n",
      "params:tensor([  5.3089, -16.9717])\n",
      "Epoch 2332, Loss 2.937364\n",
      "grad:tensor([-0.0100,  0.0566])\n",
      "params:tensor([  5.3090, -16.9723])\n",
      "Epoch 2333, Loss 2.937332\n",
      "grad:tensor([-0.0100,  0.0565])\n",
      "params:tensor([  5.3091, -16.9728])\n",
      "Epoch 2334, Loss 2.937299\n",
      "grad:tensor([-0.0100,  0.0564])\n",
      "params:tensor([  5.3092, -16.9734])\n",
      "Epoch 2335, Loss 2.937265\n",
      "grad:tensor([-0.0100,  0.0563])\n",
      "params:tensor([  5.3093, -16.9739])\n",
      "Epoch 2336, Loss 2.937232\n",
      "grad:tensor([-0.0099,  0.0562])\n",
      "params:tensor([  5.3094, -16.9745])\n",
      "Epoch 2337, Loss 2.937201\n",
      "grad:tensor([-0.0099,  0.0561])\n",
      "params:tensor([  5.3095, -16.9751])\n",
      "Epoch 2338, Loss 2.937167\n",
      "grad:tensor([-0.0099,  0.0560])\n",
      "params:tensor([  5.3096, -16.9756])\n",
      "Epoch 2339, Loss 2.937134\n",
      "grad:tensor([-0.0099,  0.0559])\n",
      "params:tensor([  5.3097, -16.9762])\n",
      "Epoch 2340, Loss 2.937104\n",
      "grad:tensor([-0.0099,  0.0558])\n",
      "params:tensor([  5.3098, -16.9767])\n",
      "Epoch 2341, Loss 2.937071\n",
      "grad:tensor([-0.0098,  0.0557])\n",
      "params:tensor([  5.3099, -16.9773])\n",
      "Epoch 2342, Loss 2.937039\n",
      "grad:tensor([-0.0098,  0.0556])\n",
      "params:tensor([  5.3100, -16.9779])\n",
      "Epoch 2343, Loss 2.937008\n",
      "grad:tensor([-0.0098,  0.0555])\n",
      "params:tensor([  5.3101, -16.9784])\n",
      "Epoch 2344, Loss 2.936976\n",
      "grad:tensor([-0.0098,  0.0554])\n",
      "params:tensor([  5.3102, -16.9790])\n",
      "Epoch 2345, Loss 2.936945\n",
      "grad:tensor([-0.0098,  0.0553])\n",
      "params:tensor([  5.3103, -16.9795])\n",
      "Epoch 2346, Loss 2.936912\n",
      "grad:tensor([-0.0098,  0.0553])\n",
      "params:tensor([  5.3104, -16.9801])\n",
      "Epoch 2347, Loss 2.936883\n",
      "grad:tensor([-0.0097,  0.0552])\n",
      "params:tensor([  5.3105, -16.9806])\n",
      "Epoch 2348, Loss 2.936851\n",
      "grad:tensor([-0.0097,  0.0551])\n",
      "params:tensor([  5.3106, -16.9812])\n",
      "Epoch 2349, Loss 2.936819\n",
      "grad:tensor([-0.0097,  0.0550])\n",
      "params:tensor([  5.3107, -16.9817])\n",
      "Epoch 2350, Loss 2.936788\n",
      "grad:tensor([-0.0097,  0.0549])\n",
      "params:tensor([  5.3107, -16.9823])\n",
      "Epoch 2351, Loss 2.936757\n",
      "grad:tensor([-0.0097,  0.0548])\n",
      "params:tensor([  5.3108, -16.9828])\n",
      "Epoch 2352, Loss 2.936725\n",
      "grad:tensor([-0.0097,  0.0547])\n",
      "params:tensor([  5.3109, -16.9834])\n",
      "Epoch 2353, Loss 2.936694\n",
      "grad:tensor([-0.0096,  0.0546])\n",
      "params:tensor([  5.3110, -16.9839])\n",
      "Epoch 2354, Loss 2.936665\n",
      "grad:tensor([-0.0096,  0.0545])\n",
      "params:tensor([  5.3111, -16.9845])\n",
      "Epoch 2355, Loss 2.936633\n",
      "grad:tensor([-0.0096,  0.0544])\n",
      "params:tensor([  5.3112, -16.9850])\n",
      "Epoch 2356, Loss 2.936602\n",
      "grad:tensor([-0.0096,  0.0543])\n",
      "params:tensor([  5.3113, -16.9856])\n",
      "Epoch 2357, Loss 2.936572\n",
      "grad:tensor([-0.0096,  0.0542])\n",
      "params:tensor([  5.3114, -16.9861])\n",
      "Epoch 2358, Loss 2.936542\n",
      "grad:tensor([-0.0095,  0.0541])\n",
      "params:tensor([  5.3115, -16.9866])\n",
      "Epoch 2359, Loss 2.936511\n",
      "grad:tensor([-0.0096,  0.0540])\n",
      "params:tensor([  5.3116, -16.9872])\n",
      "Epoch 2360, Loss 2.936481\n",
      "grad:tensor([-0.0095,  0.0540])\n",
      "params:tensor([  5.3117, -16.9877])\n",
      "Epoch 2361, Loss 2.936451\n",
      "grad:tensor([-0.0095,  0.0539])\n",
      "params:tensor([  5.3118, -16.9883])\n",
      "Epoch 2362, Loss 2.936421\n",
      "grad:tensor([-0.0095,  0.0538])\n",
      "params:tensor([  5.3119, -16.9888])\n",
      "Epoch 2363, Loss 2.936392\n",
      "grad:tensor([-0.0095,  0.0537])\n",
      "params:tensor([  5.3120, -16.9893])\n",
      "Epoch 2364, Loss 2.936362\n",
      "grad:tensor([-0.0094,  0.0536])\n",
      "params:tensor([  5.3121, -16.9899])\n",
      "Epoch 2365, Loss 2.936332\n",
      "grad:tensor([-0.0094,  0.0535])\n",
      "params:tensor([  5.3122, -16.9904])\n",
      "Epoch 2366, Loss 2.936304\n",
      "grad:tensor([-0.0094,  0.0534])\n",
      "params:tensor([  5.3123, -16.9909])\n",
      "Epoch 2367, Loss 2.936274\n",
      "grad:tensor([-0.0094,  0.0533])\n",
      "params:tensor([  5.3124, -16.9915])\n",
      "Epoch 2368, Loss 2.936244\n",
      "grad:tensor([-0.0094,  0.0532])\n",
      "params:tensor([  5.3125, -16.9920])\n",
      "Epoch 2369, Loss 2.936216\n",
      "grad:tensor([-0.0094,  0.0531])\n",
      "params:tensor([  5.3126, -16.9925])\n",
      "Epoch 2370, Loss 2.936188\n",
      "grad:tensor([-0.0094,  0.0530])\n",
      "params:tensor([  5.3127, -16.9931])\n",
      "Epoch 2371, Loss 2.936156\n",
      "grad:tensor([-0.0094,  0.0530])\n",
      "params:tensor([  5.3127, -16.9936])\n",
      "Epoch 2372, Loss 2.936128\n",
      "grad:tensor([-0.0093,  0.0529])\n",
      "params:tensor([  5.3128, -16.9941])\n",
      "Epoch 2373, Loss 2.936100\n",
      "grad:tensor([-0.0093,  0.0528])\n",
      "params:tensor([  5.3129, -16.9946])\n",
      "Epoch 2374, Loss 2.936072\n",
      "grad:tensor([-0.0093,  0.0527])\n",
      "params:tensor([  5.3130, -16.9952])\n",
      "Epoch 2375, Loss 2.936042\n",
      "grad:tensor([-0.0093,  0.0526])\n",
      "params:tensor([  5.3131, -16.9957])\n",
      "Epoch 2376, Loss 2.936014\n",
      "grad:tensor([-0.0093,  0.0525])\n",
      "params:tensor([  5.3132, -16.9962])\n",
      "Epoch 2377, Loss 2.935986\n",
      "grad:tensor([-0.0093,  0.0524])\n",
      "params:tensor([  5.3133, -16.9967])\n",
      "Epoch 2378, Loss 2.935957\n",
      "grad:tensor([-0.0093,  0.0523])\n",
      "params:tensor([  5.3134, -16.9973])\n",
      "Epoch 2379, Loss 2.935928\n",
      "grad:tensor([-0.0092,  0.0522])\n",
      "params:tensor([  5.3135, -16.9978])\n",
      "Epoch 2380, Loss 2.935901\n",
      "grad:tensor([-0.0092,  0.0522])\n",
      "params:tensor([  5.3136, -16.9983])\n",
      "Epoch 2381, Loss 2.935873\n",
      "grad:tensor([-0.0092,  0.0521])\n",
      "params:tensor([  5.3137, -16.9988])\n",
      "Epoch 2382, Loss 2.935845\n",
      "grad:tensor([-0.0092,  0.0520])\n",
      "params:tensor([  5.3138, -16.9994])\n",
      "Epoch 2383, Loss 2.935817\n",
      "grad:tensor([-0.0092,  0.0519])\n",
      "params:tensor([  5.3139, -16.9999])\n",
      "Epoch 2384, Loss 2.935789\n",
      "grad:tensor([-0.0092,  0.0518])\n",
      "params:tensor([  5.3139, -17.0004])\n",
      "Epoch 2385, Loss 2.935762\n",
      "grad:tensor([-0.0092,  0.0517])\n",
      "params:tensor([  5.3140, -17.0009])\n",
      "Epoch 2386, Loss 2.935734\n",
      "grad:tensor([-0.0091,  0.0516])\n",
      "params:tensor([  5.3141, -17.0014])\n",
      "Epoch 2387, Loss 2.935707\n",
      "grad:tensor([-0.0091,  0.0515])\n",
      "params:tensor([  5.3142, -17.0019])\n",
      "Epoch 2388, Loss 2.935679\n",
      "grad:tensor([-0.0091,  0.0514])\n",
      "params:tensor([  5.3143, -17.0025])\n",
      "Epoch 2389, Loss 2.935650\n",
      "grad:tensor([-0.0091,  0.0514])\n",
      "params:tensor([  5.3144, -17.0030])\n",
      "Epoch 2390, Loss 2.935626\n",
      "grad:tensor([-0.0090,  0.0513])\n",
      "params:tensor([  5.3145, -17.0035])\n",
      "Epoch 2391, Loss 2.935596\n",
      "grad:tensor([-0.0090,  0.0512])\n",
      "params:tensor([  5.3146, -17.0040])\n",
      "Epoch 2392, Loss 2.935571\n",
      "grad:tensor([-0.0090,  0.0511])\n",
      "params:tensor([  5.3147, -17.0045])\n",
      "Epoch 2393, Loss 2.935544\n",
      "grad:tensor([-0.0090,  0.0510])\n",
      "params:tensor([  5.3148, -17.0050])\n",
      "Epoch 2394, Loss 2.935516\n",
      "grad:tensor([-0.0090,  0.0509])\n",
      "params:tensor([  5.3149, -17.0055])\n",
      "Epoch 2395, Loss 2.935489\n",
      "grad:tensor([-0.0090,  0.0508])\n",
      "params:tensor([  5.3149, -17.0060])\n",
      "Epoch 2396, Loss 2.935465\n",
      "grad:tensor([-0.0090,  0.0507])\n",
      "params:tensor([  5.3150, -17.0065])\n",
      "Epoch 2397, Loss 2.935436\n",
      "grad:tensor([-0.0090,  0.0507])\n",
      "params:tensor([  5.3151, -17.0070])\n",
      "Epoch 2398, Loss 2.935411\n",
      "grad:tensor([-0.0089,  0.0506])\n",
      "params:tensor([  5.3152, -17.0076])\n",
      "Epoch 2399, Loss 2.935385\n",
      "grad:tensor([-0.0089,  0.0505])\n",
      "params:tensor([  5.3153, -17.0081])\n",
      "Epoch 2400, Loss 2.935356\n",
      "grad:tensor([-0.0089,  0.0504])\n",
      "params:tensor([  5.3154, -17.0086])\n",
      "Epoch 2401, Loss 2.935332\n",
      "grad:tensor([-0.0089,  0.0503])\n",
      "params:tensor([  5.3155, -17.0091])\n",
      "Epoch 2402, Loss 2.935304\n",
      "grad:tensor([-0.0089,  0.0502])\n",
      "params:tensor([  5.3156, -17.0096])\n",
      "Epoch 2403, Loss 2.935281\n",
      "grad:tensor([-0.0088,  0.0502])\n",
      "params:tensor([  5.3157, -17.0101])\n",
      "Epoch 2404, Loss 2.935252\n",
      "grad:tensor([-0.0088,  0.0501])\n",
      "params:tensor([  5.3157, -17.0106])\n",
      "Epoch 2405, Loss 2.935228\n",
      "grad:tensor([-0.0088,  0.0500])\n",
      "params:tensor([  5.3158, -17.0111])\n",
      "Epoch 2406, Loss 2.935203\n",
      "grad:tensor([-0.0088,  0.0499])\n",
      "params:tensor([  5.3159, -17.0116])\n",
      "Epoch 2407, Loss 2.935177\n",
      "grad:tensor([-0.0088,  0.0498])\n",
      "params:tensor([  5.3160, -17.0121])\n",
      "Epoch 2408, Loss 2.935152\n",
      "grad:tensor([-0.0088,  0.0497])\n",
      "params:tensor([  5.3161, -17.0126])\n",
      "Epoch 2409, Loss 2.935126\n",
      "grad:tensor([-0.0088,  0.0496])\n",
      "params:tensor([  5.3162, -17.0131])\n",
      "Epoch 2410, Loss 2.935100\n",
      "grad:tensor([-0.0088,  0.0496])\n",
      "params:tensor([  5.3163, -17.0136])\n",
      "Epoch 2411, Loss 2.935075\n",
      "grad:tensor([-0.0087,  0.0495])\n",
      "params:tensor([  5.3164, -17.0140])\n",
      "Epoch 2412, Loss 2.935049\n",
      "grad:tensor([-0.0087,  0.0494])\n",
      "params:tensor([  5.3164, -17.0145])\n",
      "Epoch 2413, Loss 2.935024\n",
      "grad:tensor([-0.0087,  0.0493])\n",
      "params:tensor([  5.3165, -17.0150])\n",
      "Epoch 2414, Loss 2.935001\n",
      "grad:tensor([-0.0087,  0.0492])\n",
      "params:tensor([  5.3166, -17.0155])\n",
      "Epoch 2415, Loss 2.934973\n",
      "grad:tensor([-0.0087,  0.0491])\n",
      "params:tensor([  5.3167, -17.0160])\n",
      "Epoch 2416, Loss 2.934949\n",
      "grad:tensor([-0.0087,  0.0491])\n",
      "params:tensor([  5.3168, -17.0165])\n",
      "Epoch 2417, Loss 2.934925\n",
      "grad:tensor([-0.0086,  0.0490])\n",
      "params:tensor([  5.3169, -17.0170])\n",
      "Epoch 2418, Loss 2.934899\n",
      "grad:tensor([-0.0086,  0.0489])\n",
      "params:tensor([  5.3170, -17.0175])\n",
      "Epoch 2419, Loss 2.934876\n",
      "grad:tensor([-0.0086,  0.0488])\n",
      "params:tensor([  5.3171, -17.0180])\n",
      "Epoch 2420, Loss 2.934853\n",
      "grad:tensor([-0.0086,  0.0487])\n",
      "params:tensor([  5.3171, -17.0185])\n",
      "Epoch 2421, Loss 2.934826\n",
      "grad:tensor([-0.0086,  0.0486])\n",
      "params:tensor([  5.3172, -17.0189])\n",
      "Epoch 2422, Loss 2.934802\n",
      "grad:tensor([-0.0086,  0.0486])\n",
      "params:tensor([  5.3173, -17.0194])\n",
      "Epoch 2423, Loss 2.934777\n",
      "grad:tensor([-0.0086,  0.0485])\n",
      "params:tensor([  5.3174, -17.0199])\n",
      "Epoch 2424, Loss 2.934753\n",
      "grad:tensor([-0.0086,  0.0484])\n",
      "params:tensor([  5.3175, -17.0204])\n",
      "Epoch 2425, Loss 2.934730\n",
      "grad:tensor([-0.0086,  0.0483])\n",
      "params:tensor([  5.3176, -17.0209])\n",
      "Epoch 2426, Loss 2.934705\n",
      "grad:tensor([-0.0085,  0.0482])\n",
      "params:tensor([  5.3177, -17.0214])\n",
      "Epoch 2427, Loss 2.934681\n",
      "grad:tensor([-0.0085,  0.0481])\n",
      "params:tensor([  5.3177, -17.0219])\n",
      "Epoch 2428, Loss 2.934658\n",
      "grad:tensor([-0.0085,  0.0481])\n",
      "params:tensor([  5.3178, -17.0223])\n",
      "Epoch 2429, Loss 2.934635\n",
      "grad:tensor([-0.0085,  0.0480])\n",
      "params:tensor([  5.3179, -17.0228])\n",
      "Epoch 2430, Loss 2.934609\n",
      "grad:tensor([-0.0085,  0.0479])\n",
      "params:tensor([  5.3180, -17.0233])\n",
      "Epoch 2431, Loss 2.934585\n",
      "grad:tensor([-0.0084,  0.0478])\n",
      "params:tensor([  5.3181, -17.0238])\n",
      "Epoch 2432, Loss 2.934563\n",
      "grad:tensor([-0.0084,  0.0477])\n",
      "params:tensor([  5.3182, -17.0242])\n",
      "Epoch 2433, Loss 2.934541\n",
      "grad:tensor([-0.0084,  0.0477])\n",
      "params:tensor([  5.3182, -17.0247])\n",
      "Epoch 2434, Loss 2.934516\n",
      "grad:tensor([-0.0084,  0.0476])\n",
      "params:tensor([  5.3183, -17.0252])\n",
      "Epoch 2435, Loss 2.934493\n",
      "grad:tensor([-0.0084,  0.0475])\n",
      "params:tensor([  5.3184, -17.0257])\n",
      "Epoch 2436, Loss 2.934469\n",
      "grad:tensor([-0.0084,  0.0474])\n",
      "params:tensor([  5.3185, -17.0261])\n",
      "Epoch 2437, Loss 2.934446\n",
      "grad:tensor([-0.0084,  0.0473])\n",
      "params:tensor([  5.3186, -17.0266])\n",
      "Epoch 2438, Loss 2.934423\n",
      "grad:tensor([-0.0083,  0.0473])\n",
      "params:tensor([  5.3187, -17.0271])\n",
      "Epoch 2439, Loss 2.934400\n",
      "grad:tensor([-0.0083,  0.0472])\n",
      "params:tensor([  5.3187, -17.0276])\n",
      "Epoch 2440, Loss 2.934377\n",
      "grad:tensor([-0.0083,  0.0471])\n",
      "params:tensor([  5.3188, -17.0280])\n",
      "Epoch 2441, Loss 2.934355\n",
      "grad:tensor([-0.0083,  0.0470])\n",
      "params:tensor([  5.3189, -17.0285])\n",
      "Epoch 2442, Loss 2.934331\n",
      "grad:tensor([-0.0083,  0.0469])\n",
      "params:tensor([  5.3190, -17.0290])\n",
      "Epoch 2443, Loss 2.934309\n",
      "grad:tensor([-0.0083,  0.0469])\n",
      "params:tensor([  5.3191, -17.0294])\n",
      "Epoch 2444, Loss 2.934287\n",
      "grad:tensor([-0.0083,  0.0468])\n",
      "params:tensor([  5.3192, -17.0299])\n",
      "Epoch 2445, Loss 2.934264\n",
      "grad:tensor([-0.0083,  0.0467])\n",
      "params:tensor([  5.3192, -17.0304])\n",
      "Epoch 2446, Loss 2.934242\n",
      "grad:tensor([-0.0083,  0.0466])\n",
      "params:tensor([  5.3193, -17.0308])\n",
      "Epoch 2447, Loss 2.934219\n",
      "grad:tensor([-0.0082,  0.0465])\n",
      "params:tensor([  5.3194, -17.0313])\n",
      "Epoch 2448, Loss 2.934198\n",
      "grad:tensor([-0.0082,  0.0465])\n",
      "params:tensor([  5.3195, -17.0318])\n",
      "Epoch 2449, Loss 2.934175\n",
      "grad:tensor([-0.0082,  0.0464])\n",
      "params:tensor([  5.3196, -17.0322])\n",
      "Epoch 2450, Loss 2.934151\n",
      "grad:tensor([-0.0082,  0.0463])\n",
      "params:tensor([  5.3197, -17.0327])\n",
      "Epoch 2451, Loss 2.934129\n",
      "grad:tensor([-0.0082,  0.0462])\n",
      "params:tensor([  5.3197, -17.0332])\n",
      "Epoch 2452, Loss 2.934108\n",
      "grad:tensor([-0.0082,  0.0461])\n",
      "params:tensor([  5.3198, -17.0336])\n",
      "Epoch 2453, Loss 2.934084\n",
      "grad:tensor([-0.0081,  0.0461])\n",
      "params:tensor([  5.3199, -17.0341])\n",
      "Epoch 2454, Loss 2.934065\n",
      "grad:tensor([-0.0081,  0.0460])\n",
      "params:tensor([  5.3200, -17.0345])\n",
      "Epoch 2455, Loss 2.934043\n",
      "grad:tensor([-0.0081,  0.0459])\n",
      "params:tensor([  5.3201, -17.0350])\n",
      "Epoch 2456, Loss 2.934020\n",
      "grad:tensor([-0.0081,  0.0458])\n",
      "params:tensor([  5.3201, -17.0355])\n",
      "Epoch 2457, Loss 2.934000\n",
      "grad:tensor([-0.0081,  0.0457])\n",
      "params:tensor([  5.3202, -17.0359])\n",
      "Epoch 2458, Loss 2.933978\n",
      "grad:tensor([-0.0081,  0.0457])\n",
      "params:tensor([  5.3203, -17.0364])\n",
      "Epoch 2459, Loss 2.933956\n",
      "grad:tensor([-0.0080,  0.0456])\n",
      "params:tensor([  5.3204, -17.0368])\n",
      "Epoch 2460, Loss 2.933935\n",
      "grad:tensor([-0.0080,  0.0455])\n",
      "params:tensor([  5.3205, -17.0373])\n",
      "Epoch 2461, Loss 2.933914\n",
      "grad:tensor([-0.0080,  0.0454])\n",
      "params:tensor([  5.3205, -17.0377])\n",
      "Epoch 2462, Loss 2.933893\n",
      "grad:tensor([-0.0080,  0.0454])\n",
      "params:tensor([  5.3206, -17.0382])\n",
      "Epoch 2463, Loss 2.933871\n",
      "grad:tensor([-0.0080,  0.0453])\n",
      "params:tensor([  5.3207, -17.0386])\n",
      "Epoch 2464, Loss 2.933849\n",
      "grad:tensor([-0.0080,  0.0452])\n",
      "params:tensor([  5.3208, -17.0391])\n",
      "Epoch 2465, Loss 2.933828\n",
      "grad:tensor([-0.0080,  0.0451])\n",
      "params:tensor([  5.3209, -17.0396])\n",
      "Epoch 2466, Loss 2.933807\n",
      "grad:tensor([-0.0080,  0.0451])\n",
      "params:tensor([  5.3209, -17.0400])\n",
      "Epoch 2467, Loss 2.933787\n",
      "grad:tensor([-0.0079,  0.0450])\n",
      "params:tensor([  5.3210, -17.0405])\n",
      "Epoch 2468, Loss 2.933767\n",
      "grad:tensor([-0.0079,  0.0449])\n",
      "params:tensor([  5.3211, -17.0409])\n",
      "Epoch 2469, Loss 2.933746\n",
      "grad:tensor([-0.0079,  0.0448])\n",
      "params:tensor([  5.3212, -17.0413])\n",
      "Epoch 2470, Loss 2.933723\n",
      "grad:tensor([-0.0079,  0.0448])\n",
      "params:tensor([  5.3213, -17.0418])\n",
      "Epoch 2471, Loss 2.933704\n",
      "grad:tensor([-0.0079,  0.0447])\n",
      "params:tensor([  5.3213, -17.0422])\n",
      "Epoch 2472, Loss 2.933682\n",
      "grad:tensor([-0.0079,  0.0446])\n",
      "params:tensor([  5.3214, -17.0427])\n",
      "Epoch 2473, Loss 2.933662\n",
      "grad:tensor([-0.0079,  0.0445])\n",
      "params:tensor([  5.3215, -17.0431])\n",
      "Epoch 2474, Loss 2.933643\n",
      "grad:tensor([-0.0079,  0.0444])\n",
      "params:tensor([  5.3216, -17.0436])\n",
      "Epoch 2475, Loss 2.933622\n",
      "grad:tensor([-0.0078,  0.0444])\n",
      "params:tensor([  5.3217, -17.0440])\n",
      "Epoch 2476, Loss 2.933602\n",
      "grad:tensor([-0.0078,  0.0443])\n",
      "params:tensor([  5.3217, -17.0445])\n",
      "Epoch 2477, Loss 2.933583\n",
      "grad:tensor([-0.0078,  0.0442])\n",
      "params:tensor([  5.3218, -17.0449])\n",
      "Epoch 2478, Loss 2.933561\n",
      "grad:tensor([-0.0078,  0.0441])\n",
      "params:tensor([  5.3219, -17.0453])\n",
      "Epoch 2479, Loss 2.933541\n",
      "grad:tensor([-0.0078,  0.0441])\n",
      "params:tensor([  5.3220, -17.0458])\n",
      "Epoch 2480, Loss 2.933521\n",
      "grad:tensor([-0.0078,  0.0440])\n",
      "params:tensor([  5.3220, -17.0462])\n",
      "Epoch 2481, Loss 2.933501\n",
      "grad:tensor([-0.0078,  0.0439])\n",
      "params:tensor([  5.3221, -17.0467])\n",
      "Epoch 2482, Loss 2.933480\n",
      "grad:tensor([-0.0077,  0.0438])\n",
      "params:tensor([  5.3222, -17.0471])\n",
      "Epoch 2483, Loss 2.933463\n",
      "grad:tensor([-0.0077,  0.0438])\n",
      "params:tensor([  5.3223, -17.0475])\n",
      "Epoch 2484, Loss 2.933442\n",
      "grad:tensor([-0.0077,  0.0437])\n",
      "params:tensor([  5.3224, -17.0480])\n",
      "Epoch 2485, Loss 2.933422\n",
      "grad:tensor([-0.0077,  0.0436])\n",
      "params:tensor([  5.3224, -17.0484])\n",
      "Epoch 2486, Loss 2.933403\n",
      "grad:tensor([-0.0077,  0.0436])\n",
      "params:tensor([  5.3225, -17.0489])\n",
      "Epoch 2487, Loss 2.933382\n",
      "grad:tensor([-0.0077,  0.0435])\n",
      "params:tensor([  5.3226, -17.0493])\n",
      "Epoch 2488, Loss 2.933365\n",
      "grad:tensor([-0.0077,  0.0434])\n",
      "params:tensor([  5.3227, -17.0497])\n",
      "Epoch 2489, Loss 2.933345\n",
      "grad:tensor([-0.0077,  0.0433])\n",
      "params:tensor([  5.3227, -17.0502])\n",
      "Epoch 2490, Loss 2.933325\n",
      "grad:tensor([-0.0076,  0.0433])\n",
      "params:tensor([  5.3228, -17.0506])\n",
      "Epoch 2491, Loss 2.933306\n",
      "grad:tensor([-0.0076,  0.0432])\n",
      "params:tensor([  5.3229, -17.0510])\n",
      "Epoch 2492, Loss 2.933287\n",
      "grad:tensor([-0.0076,  0.0431])\n",
      "params:tensor([  5.3230, -17.0515])\n",
      "Epoch 2493, Loss 2.933266\n",
      "grad:tensor([-0.0076,  0.0430])\n",
      "params:tensor([  5.3230, -17.0519])\n",
      "Epoch 2494, Loss 2.933249\n",
      "grad:tensor([-0.0076,  0.0430])\n",
      "params:tensor([  5.3231, -17.0523])\n",
      "Epoch 2495, Loss 2.933229\n",
      "grad:tensor([-0.0076,  0.0429])\n",
      "params:tensor([  5.3232, -17.0527])\n",
      "Epoch 2496, Loss 2.933209\n",
      "grad:tensor([-0.0076,  0.0428])\n",
      "params:tensor([  5.3233, -17.0532])\n",
      "Epoch 2497, Loss 2.933190\n",
      "grad:tensor([-0.0075,  0.0427])\n",
      "params:tensor([  5.3233, -17.0536])\n",
      "Epoch 2498, Loss 2.933172\n",
      "grad:tensor([-0.0075,  0.0427])\n",
      "params:tensor([  5.3234, -17.0540])\n",
      "Epoch 2499, Loss 2.933154\n",
      "grad:tensor([-0.0075,  0.0426])\n",
      "params:tensor([  5.3235, -17.0544])\n",
      "Epoch 2500, Loss 2.933134\n",
      "grad:tensor([-0.0075,  0.0425])\n",
      "params:tensor([  5.3236, -17.0549])\n",
      "Epoch 2501, Loss 2.933116\n",
      "grad:tensor([-0.0075,  0.0425])\n",
      "params:tensor([  5.3236, -17.0553])\n",
      "Epoch 2502, Loss 2.933097\n",
      "grad:tensor([-0.0075,  0.0424])\n",
      "params:tensor([  5.3237, -17.0557])\n",
      "Epoch 2503, Loss 2.933079\n",
      "grad:tensor([-0.0075,  0.0423])\n",
      "params:tensor([  5.3238, -17.0561])\n",
      "Epoch 2504, Loss 2.933060\n",
      "grad:tensor([-0.0075,  0.0422])\n",
      "params:tensor([  5.3239, -17.0566])\n",
      "Epoch 2505, Loss 2.933043\n",
      "grad:tensor([-0.0074,  0.0422])\n",
      "params:tensor([  5.3239, -17.0570])\n",
      "Epoch 2506, Loss 2.933025\n",
      "grad:tensor([-0.0074,  0.0421])\n",
      "params:tensor([  5.3240, -17.0574])\n",
      "Epoch 2507, Loss 2.933007\n",
      "grad:tensor([-0.0074,  0.0420])\n",
      "params:tensor([  5.3241, -17.0578])\n",
      "Epoch 2508, Loss 2.932988\n",
      "grad:tensor([-0.0074,  0.0420])\n",
      "params:tensor([  5.3242, -17.0582])\n",
      "Epoch 2509, Loss 2.932970\n",
      "grad:tensor([-0.0074,  0.0419])\n",
      "params:tensor([  5.3242, -17.0587])\n",
      "Epoch 2510, Loss 2.932953\n",
      "grad:tensor([-0.0074,  0.0418])\n",
      "params:tensor([  5.3243, -17.0591])\n",
      "Epoch 2511, Loss 2.932932\n",
      "grad:tensor([-0.0074,  0.0417])\n",
      "params:tensor([  5.3244, -17.0595])\n",
      "Epoch 2512, Loss 2.932915\n",
      "grad:tensor([-0.0073,  0.0417])\n",
      "params:tensor([  5.3245, -17.0599])\n",
      "Epoch 2513, Loss 2.932898\n",
      "grad:tensor([-0.0073,  0.0416])\n",
      "params:tensor([  5.3245, -17.0603])\n",
      "Epoch 2514, Loss 2.932880\n",
      "grad:tensor([-0.0073,  0.0415])\n",
      "params:tensor([  5.3246, -17.0608])\n",
      "Epoch 2515, Loss 2.932862\n",
      "grad:tensor([-0.0073,  0.0415])\n",
      "params:tensor([  5.3247, -17.0612])\n",
      "Epoch 2516, Loss 2.932846\n",
      "grad:tensor([-0.0073,  0.0414])\n",
      "params:tensor([  5.3248, -17.0616])\n",
      "Epoch 2517, Loss 2.932826\n",
      "grad:tensor([-0.0073,  0.0413])\n",
      "params:tensor([  5.3248, -17.0620])\n",
      "Epoch 2518, Loss 2.932810\n",
      "grad:tensor([-0.0073,  0.0412])\n",
      "params:tensor([  5.3249, -17.0624])\n",
      "Epoch 2519, Loss 2.932790\n",
      "grad:tensor([-0.0073,  0.0412])\n",
      "params:tensor([  5.3250, -17.0628])\n",
      "Epoch 2520, Loss 2.932774\n",
      "grad:tensor([-0.0073,  0.0411])\n",
      "params:tensor([  5.3250, -17.0632])\n",
      "Epoch 2521, Loss 2.932758\n",
      "grad:tensor([-0.0073,  0.0410])\n",
      "params:tensor([  5.3251, -17.0636])\n",
      "Epoch 2522, Loss 2.932739\n",
      "grad:tensor([-0.0073,  0.0410])\n",
      "params:tensor([  5.3252, -17.0640])\n",
      "Epoch 2523, Loss 2.932723\n",
      "grad:tensor([-0.0072,  0.0409])\n",
      "params:tensor([  5.3253, -17.0645])\n",
      "Epoch 2524, Loss 2.932706\n",
      "grad:tensor([-0.0072,  0.0408])\n",
      "params:tensor([  5.3253, -17.0649])\n",
      "Epoch 2525, Loss 2.932689\n",
      "grad:tensor([-0.0072,  0.0408])\n",
      "params:tensor([  5.3254, -17.0653])\n",
      "Epoch 2526, Loss 2.932671\n",
      "grad:tensor([-0.0072,  0.0407])\n",
      "params:tensor([  5.3255, -17.0657])\n",
      "Epoch 2527, Loss 2.932654\n",
      "grad:tensor([-0.0072,  0.0406])\n",
      "params:tensor([  5.3256, -17.0661])\n",
      "Epoch 2528, Loss 2.932637\n",
      "grad:tensor([-0.0072,  0.0405])\n",
      "params:tensor([  5.3256, -17.0665])\n",
      "Epoch 2529, Loss 2.932619\n",
      "grad:tensor([-0.0072,  0.0405])\n",
      "params:tensor([  5.3257, -17.0669])\n",
      "Epoch 2530, Loss 2.932603\n",
      "grad:tensor([-0.0071,  0.0404])\n",
      "params:tensor([  5.3258, -17.0673])\n",
      "Epoch 2531, Loss 2.932585\n",
      "grad:tensor([-0.0071,  0.0403])\n",
      "params:tensor([  5.3258, -17.0677])\n",
      "Epoch 2532, Loss 2.932569\n",
      "grad:tensor([-0.0071,  0.0403])\n",
      "params:tensor([  5.3259, -17.0681])\n",
      "Epoch 2533, Loss 2.932553\n",
      "grad:tensor([-0.0071,  0.0402])\n",
      "params:tensor([  5.3260, -17.0685])\n",
      "Epoch 2534, Loss 2.932535\n",
      "grad:tensor([-0.0071,  0.0401])\n",
      "params:tensor([  5.3261, -17.0689])\n",
      "Epoch 2535, Loss 2.932520\n",
      "grad:tensor([-0.0071,  0.0401])\n",
      "params:tensor([  5.3261, -17.0693])\n",
      "Epoch 2536, Loss 2.932502\n",
      "grad:tensor([-0.0071,  0.0400])\n",
      "params:tensor([  5.3262, -17.0697])\n",
      "Epoch 2537, Loss 2.932487\n",
      "grad:tensor([-0.0071,  0.0399])\n",
      "params:tensor([  5.3263, -17.0701])\n",
      "Epoch 2538, Loss 2.932469\n",
      "grad:tensor([-0.0070,  0.0399])\n",
      "params:tensor([  5.3263, -17.0705])\n",
      "Epoch 2539, Loss 2.932455\n",
      "grad:tensor([-0.0070,  0.0398])\n",
      "params:tensor([  5.3264, -17.0709])\n",
      "Epoch 2540, Loss 2.932438\n",
      "grad:tensor([-0.0070,  0.0397])\n",
      "params:tensor([  5.3265, -17.0713])\n",
      "Epoch 2541, Loss 2.932421\n",
      "grad:tensor([-0.0070,  0.0397])\n",
      "params:tensor([  5.3265, -17.0717])\n",
      "Epoch 2542, Loss 2.932404\n",
      "grad:tensor([-0.0070,  0.0396])\n",
      "params:tensor([  5.3266, -17.0721])\n",
      "Epoch 2543, Loss 2.932387\n",
      "grad:tensor([-0.0070,  0.0395])\n",
      "params:tensor([  5.3267, -17.0725])\n",
      "Epoch 2544, Loss 2.932371\n",
      "grad:tensor([-0.0070,  0.0395])\n",
      "params:tensor([  5.3268, -17.0729])\n",
      "Epoch 2545, Loss 2.932358\n",
      "grad:tensor([-0.0070,  0.0394])\n",
      "params:tensor([  5.3268, -17.0733])\n",
      "Epoch 2546, Loss 2.932340\n",
      "grad:tensor([-0.0069,  0.0393])\n",
      "params:tensor([  5.3269, -17.0737])\n",
      "Epoch 2547, Loss 2.932324\n",
      "grad:tensor([-0.0069,  0.0393])\n",
      "params:tensor([  5.3270, -17.0741])\n",
      "Epoch 2548, Loss 2.932310\n",
      "grad:tensor([-0.0069,  0.0392])\n",
      "params:tensor([  5.3270, -17.0745])\n",
      "Epoch 2549, Loss 2.932293\n",
      "grad:tensor([-0.0069,  0.0391])\n",
      "params:tensor([  5.3271, -17.0749])\n",
      "Epoch 2550, Loss 2.932277\n",
      "grad:tensor([-0.0069,  0.0391])\n",
      "params:tensor([  5.3272, -17.0752])\n",
      "Epoch 2551, Loss 2.932261\n",
      "grad:tensor([-0.0069,  0.0390])\n",
      "params:tensor([  5.3272, -17.0756])\n",
      "Epoch 2552, Loss 2.932246\n",
      "grad:tensor([-0.0069,  0.0389])\n",
      "params:tensor([  5.3273, -17.0760])\n",
      "Epoch 2553, Loss 2.932229\n",
      "grad:tensor([-0.0069,  0.0389])\n",
      "params:tensor([  5.3274, -17.0764])\n",
      "Epoch 2554, Loss 2.932215\n",
      "grad:tensor([-0.0069,  0.0388])\n",
      "params:tensor([  5.3274, -17.0768])\n",
      "Epoch 2555, Loss 2.932198\n",
      "grad:tensor([-0.0068,  0.0387])\n",
      "params:tensor([  5.3275, -17.0772])\n",
      "Epoch 2556, Loss 2.932183\n",
      "grad:tensor([-0.0068,  0.0387])\n",
      "params:tensor([  5.3276, -17.0776])\n",
      "Epoch 2557, Loss 2.932167\n",
      "grad:tensor([-0.0068,  0.0386])\n",
      "params:tensor([  5.3276, -17.0780])\n",
      "Epoch 2558, Loss 2.932153\n",
      "grad:tensor([-0.0068,  0.0385])\n",
      "params:tensor([  5.3277, -17.0783])\n",
      "Epoch 2559, Loss 2.932137\n",
      "grad:tensor([-0.0068,  0.0385])\n",
      "params:tensor([  5.3278, -17.0787])\n",
      "Epoch 2560, Loss 2.932122\n",
      "grad:tensor([-0.0068,  0.0384])\n",
      "params:tensor([  5.3279, -17.0791])\n",
      "Epoch 2561, Loss 2.932107\n",
      "grad:tensor([-0.0068,  0.0383])\n",
      "params:tensor([  5.3279, -17.0795])\n",
      "Epoch 2562, Loss 2.932092\n",
      "grad:tensor([-0.0068,  0.0383])\n",
      "params:tensor([  5.3280, -17.0799])\n",
      "Epoch 2563, Loss 2.932076\n",
      "grad:tensor([-0.0067,  0.0382])\n",
      "params:tensor([  5.3281, -17.0803])\n",
      "Epoch 2564, Loss 2.932061\n",
      "grad:tensor([-0.0067,  0.0381])\n",
      "params:tensor([  5.3281, -17.0806])\n",
      "Epoch 2565, Loss 2.932047\n",
      "grad:tensor([-0.0067,  0.0381])\n",
      "params:tensor([  5.3282, -17.0810])\n",
      "Epoch 2566, Loss 2.932031\n",
      "grad:tensor([-0.0067,  0.0380])\n",
      "params:tensor([  5.3283, -17.0814])\n",
      "Epoch 2567, Loss 2.932017\n",
      "grad:tensor([-0.0067,  0.0379])\n",
      "params:tensor([  5.3283, -17.0818])\n",
      "Epoch 2568, Loss 2.932002\n",
      "grad:tensor([-0.0067,  0.0379])\n",
      "params:tensor([  5.3284, -17.0822])\n",
      "Epoch 2569, Loss 2.931986\n",
      "grad:tensor([-0.0067,  0.0378])\n",
      "params:tensor([  5.3285, -17.0825])\n",
      "Epoch 2570, Loss 2.931972\n",
      "grad:tensor([-0.0067,  0.0378])\n",
      "params:tensor([  5.3285, -17.0829])\n",
      "Epoch 2571, Loss 2.931957\n",
      "grad:tensor([-0.0067,  0.0377])\n",
      "params:tensor([  5.3286, -17.0833])\n",
      "Epoch 2572, Loss 2.931941\n",
      "grad:tensor([-0.0067,  0.0376])\n",
      "params:tensor([  5.3287, -17.0837])\n",
      "Epoch 2573, Loss 2.931929\n",
      "grad:tensor([-0.0066,  0.0376])\n",
      "params:tensor([  5.3287, -17.0840])\n",
      "Epoch 2574, Loss 2.931914\n",
      "grad:tensor([-0.0066,  0.0375])\n",
      "params:tensor([  5.3288, -17.0844])\n",
      "Epoch 2575, Loss 2.931900\n",
      "grad:tensor([-0.0066,  0.0374])\n",
      "params:tensor([  5.3289, -17.0848])\n",
      "Epoch 2576, Loss 2.931885\n",
      "grad:tensor([-0.0066,  0.0374])\n",
      "params:tensor([  5.3289, -17.0852])\n",
      "Epoch 2577, Loss 2.931870\n",
      "grad:tensor([-0.0066,  0.0373])\n",
      "params:tensor([  5.3290, -17.0855])\n",
      "Epoch 2578, Loss 2.931855\n",
      "grad:tensor([-0.0066,  0.0372])\n",
      "params:tensor([  5.3291, -17.0859])\n",
      "Epoch 2579, Loss 2.931842\n",
      "grad:tensor([-0.0066,  0.0372])\n",
      "params:tensor([  5.3291, -17.0863])\n",
      "Epoch 2580, Loss 2.931828\n",
      "grad:tensor([-0.0066,  0.0371])\n",
      "params:tensor([  5.3292, -17.0867])\n",
      "Epoch 2581, Loss 2.931813\n",
      "grad:tensor([-0.0065,  0.0371])\n",
      "params:tensor([  5.3293, -17.0870])\n",
      "Epoch 2582, Loss 2.931799\n",
      "grad:tensor([-0.0065,  0.0370])\n",
      "params:tensor([  5.3293, -17.0874])\n",
      "Epoch 2583, Loss 2.931786\n",
      "grad:tensor([-0.0065,  0.0369])\n",
      "params:tensor([  5.3294, -17.0878])\n",
      "Epoch 2584, Loss 2.931771\n",
      "grad:tensor([-0.0065,  0.0369])\n",
      "params:tensor([  5.3294, -17.0881])\n",
      "Epoch 2585, Loss 2.931759\n",
      "grad:tensor([-0.0065,  0.0368])\n",
      "params:tensor([  5.3295, -17.0885])\n",
      "Epoch 2586, Loss 2.931742\n",
      "grad:tensor([-0.0065,  0.0367])\n",
      "params:tensor([  5.3296, -17.0889])\n",
      "Epoch 2587, Loss 2.931729\n",
      "grad:tensor([-0.0065,  0.0367])\n",
      "params:tensor([  5.3296, -17.0892])\n",
      "Epoch 2588, Loss 2.931717\n",
      "grad:tensor([-0.0065,  0.0366])\n",
      "params:tensor([  5.3297, -17.0896])\n",
      "Epoch 2589, Loss 2.931701\n",
      "grad:tensor([-0.0065,  0.0366])\n",
      "params:tensor([  5.3298, -17.0900])\n",
      "Epoch 2590, Loss 2.931687\n",
      "grad:tensor([-0.0065,  0.0365])\n",
      "params:tensor([  5.3298, -17.0903])\n",
      "Epoch 2591, Loss 2.931674\n",
      "grad:tensor([-0.0064,  0.0364])\n",
      "params:tensor([  5.3299, -17.0907])\n",
      "Epoch 2592, Loss 2.931660\n",
      "grad:tensor([-0.0064,  0.0364])\n",
      "params:tensor([  5.3300, -17.0911])\n",
      "Epoch 2593, Loss 2.931648\n",
      "grad:tensor([-0.0064,  0.0363])\n",
      "params:tensor([  5.3300, -17.0914])\n",
      "Epoch 2594, Loss 2.931632\n",
      "grad:tensor([-0.0064,  0.0362])\n",
      "params:tensor([  5.3301, -17.0918])\n",
      "Epoch 2595, Loss 2.931619\n",
      "grad:tensor([-0.0064,  0.0362])\n",
      "params:tensor([  5.3302, -17.0921])\n",
      "Epoch 2596, Loss 2.931606\n",
      "grad:tensor([-0.0064,  0.0361])\n",
      "params:tensor([  5.3302, -17.0925])\n",
      "Epoch 2597, Loss 2.931593\n",
      "grad:tensor([-0.0064,  0.0361])\n",
      "params:tensor([  5.3303, -17.0929])\n",
      "Epoch 2598, Loss 2.931580\n",
      "grad:tensor([-0.0064,  0.0360])\n",
      "params:tensor([  5.3303, -17.0932])\n",
      "Epoch 2599, Loss 2.931566\n",
      "grad:tensor([-0.0064,  0.0359])\n",
      "params:tensor([  5.3304, -17.0936])\n",
      "Epoch 2600, Loss 2.931554\n",
      "grad:tensor([-0.0064,  0.0359])\n",
      "params:tensor([  5.3305, -17.0939])\n",
      "Epoch 2601, Loss 2.931538\n",
      "grad:tensor([-0.0063,  0.0358])\n",
      "params:tensor([  5.3305, -17.0943])\n",
      "Epoch 2602, Loss 2.931526\n",
      "grad:tensor([-0.0063,  0.0358])\n",
      "params:tensor([  5.3306, -17.0947])\n",
      "Epoch 2603, Loss 2.931512\n",
      "grad:tensor([-0.0063,  0.0357])\n",
      "params:tensor([  5.3307, -17.0950])\n",
      "Epoch 2604, Loss 2.931499\n",
      "grad:tensor([-0.0063,  0.0356])\n",
      "params:tensor([  5.3307, -17.0954])\n",
      "Epoch 2605, Loss 2.931488\n",
      "grad:tensor([-0.0063,  0.0356])\n",
      "params:tensor([  5.3308, -17.0957])\n",
      "Epoch 2606, Loss 2.931474\n",
      "grad:tensor([-0.0063,  0.0355])\n",
      "params:tensor([  5.3309, -17.0961])\n",
      "Epoch 2607, Loss 2.931462\n",
      "grad:tensor([-0.0062,  0.0355])\n",
      "params:tensor([  5.3309, -17.0964])\n",
      "Epoch 2608, Loss 2.931448\n",
      "grad:tensor([-0.0062,  0.0354])\n",
      "params:tensor([  5.3310, -17.0968])\n",
      "Epoch 2609, Loss 2.931436\n",
      "grad:tensor([-0.0062,  0.0353])\n",
      "params:tensor([  5.3310, -17.0971])\n",
      "Epoch 2610, Loss 2.931423\n",
      "grad:tensor([-0.0062,  0.0353])\n",
      "params:tensor([  5.3311, -17.0975])\n",
      "Epoch 2611, Loss 2.931411\n",
      "grad:tensor([-0.0062,  0.0352])\n",
      "params:tensor([  5.3312, -17.0979])\n",
      "Epoch 2612, Loss 2.931397\n",
      "grad:tensor([-0.0062,  0.0352])\n",
      "params:tensor([  5.3312, -17.0982])\n",
      "Epoch 2613, Loss 2.931384\n",
      "grad:tensor([-0.0062,  0.0351])\n",
      "params:tensor([  5.3313, -17.0986])\n",
      "Epoch 2614, Loss 2.931371\n",
      "grad:tensor([-0.0062,  0.0350])\n",
      "params:tensor([  5.3313, -17.0989])\n",
      "Epoch 2615, Loss 2.931358\n",
      "grad:tensor([-0.0062,  0.0350])\n",
      "params:tensor([  5.3314, -17.0993])\n",
      "Epoch 2616, Loss 2.931346\n",
      "grad:tensor([-0.0062,  0.0349])\n",
      "params:tensor([  5.3315, -17.0996])\n",
      "Epoch 2617, Loss 2.931335\n",
      "grad:tensor([-0.0062,  0.0349])\n",
      "params:tensor([  5.3315, -17.1000])\n",
      "Epoch 2618, Loss 2.931322\n",
      "grad:tensor([-0.0062,  0.0348])\n",
      "params:tensor([  5.3316, -17.1003])\n",
      "Epoch 2619, Loss 2.931308\n",
      "grad:tensor([-0.0061,  0.0347])\n",
      "params:tensor([  5.3317, -17.1006])\n",
      "Epoch 2620, Loss 2.931296\n",
      "grad:tensor([-0.0061,  0.0347])\n",
      "params:tensor([  5.3317, -17.1010])\n",
      "Epoch 2621, Loss 2.931282\n",
      "grad:tensor([-0.0061,  0.0346])\n",
      "params:tensor([  5.3318, -17.1013])\n",
      "Epoch 2622, Loss 2.931272\n",
      "grad:tensor([-0.0061,  0.0346])\n",
      "params:tensor([  5.3318, -17.1017])\n",
      "Epoch 2623, Loss 2.931258\n",
      "grad:tensor([-0.0061,  0.0345])\n",
      "params:tensor([  5.3319, -17.1020])\n",
      "Epoch 2624, Loss 2.931245\n",
      "grad:tensor([-0.0061,  0.0344])\n",
      "params:tensor([  5.3320, -17.1024])\n",
      "Epoch 2625, Loss 2.931234\n",
      "grad:tensor([-0.0061,  0.0344])\n",
      "params:tensor([  5.3320, -17.1027])\n",
      "Epoch 2626, Loss 2.931222\n",
      "grad:tensor([-0.0061,  0.0343])\n",
      "params:tensor([  5.3321, -17.1031])\n",
      "Epoch 2627, Loss 2.931211\n",
      "grad:tensor([-0.0060,  0.0343])\n",
      "params:tensor([  5.3321, -17.1034])\n",
      "Epoch 2628, Loss 2.931196\n",
      "grad:tensor([-0.0060,  0.0342])\n",
      "params:tensor([  5.3322, -17.1038])\n",
      "Epoch 2629, Loss 2.931185\n",
      "grad:tensor([-0.0060,  0.0342])\n",
      "params:tensor([  5.3323, -17.1041])\n",
      "Epoch 2630, Loss 2.931173\n",
      "grad:tensor([-0.0060,  0.0341])\n",
      "params:tensor([  5.3323, -17.1044])\n",
      "Epoch 2631, Loss 2.931162\n",
      "grad:tensor([-0.0060,  0.0340])\n",
      "params:tensor([  5.3324, -17.1048])\n",
      "Epoch 2632, Loss 2.931149\n",
      "grad:tensor([-0.0060,  0.0340])\n",
      "params:tensor([  5.3324, -17.1051])\n",
      "Epoch 2633, Loss 2.931138\n",
      "grad:tensor([-0.0060,  0.0339])\n",
      "params:tensor([  5.3325, -17.1055])\n",
      "Epoch 2634, Loss 2.931126\n",
      "grad:tensor([-0.0060,  0.0339])\n",
      "params:tensor([  5.3326, -17.1058])\n",
      "Epoch 2635, Loss 2.931114\n",
      "grad:tensor([-0.0060,  0.0338])\n",
      "params:tensor([  5.3326, -17.1061])\n",
      "Epoch 2636, Loss 2.931101\n",
      "grad:tensor([-0.0060,  0.0337])\n",
      "params:tensor([  5.3327, -17.1065])\n",
      "Epoch 2637, Loss 2.931090\n",
      "grad:tensor([-0.0059,  0.0337])\n",
      "params:tensor([  5.3327, -17.1068])\n",
      "Epoch 2638, Loss 2.931079\n",
      "grad:tensor([-0.0059,  0.0336])\n",
      "params:tensor([  5.3328, -17.1071])\n",
      "Epoch 2639, Loss 2.931067\n",
      "grad:tensor([-0.0059,  0.0336])\n",
      "params:tensor([  5.3329, -17.1075])\n",
      "Epoch 2640, Loss 2.931054\n",
      "grad:tensor([-0.0059,  0.0335])\n",
      "params:tensor([  5.3329, -17.1078])\n",
      "Epoch 2641, Loss 2.931044\n",
      "grad:tensor([-0.0059,  0.0335])\n",
      "params:tensor([  5.3330, -17.1081])\n",
      "Epoch 2642, Loss 2.931034\n",
      "grad:tensor([-0.0059,  0.0334])\n",
      "params:tensor([  5.3330, -17.1085])\n",
      "Epoch 2643, Loss 2.931021\n",
      "grad:tensor([-0.0059,  0.0333])\n",
      "params:tensor([  5.3331, -17.1088])\n",
      "Epoch 2644, Loss 2.931010\n",
      "grad:tensor([-0.0059,  0.0333])\n",
      "params:tensor([  5.3332, -17.1091])\n",
      "Epoch 2645, Loss 2.930999\n",
      "grad:tensor([-0.0059,  0.0332])\n",
      "params:tensor([  5.3332, -17.1095])\n",
      "Epoch 2646, Loss 2.930987\n",
      "grad:tensor([-0.0059,  0.0332])\n",
      "params:tensor([  5.3333, -17.1098])\n",
      "Epoch 2647, Loss 2.930976\n",
      "grad:tensor([-0.0059,  0.0331])\n",
      "params:tensor([  5.3333, -17.1101])\n",
      "Epoch 2648, Loss 2.930964\n",
      "grad:tensor([-0.0059,  0.0331])\n",
      "params:tensor([  5.3334, -17.1105])\n",
      "Epoch 2649, Loss 2.930953\n",
      "grad:tensor([-0.0058,  0.0330])\n",
      "params:tensor([  5.3335, -17.1108])\n",
      "Epoch 2650, Loss 2.930941\n",
      "grad:tensor([-0.0058,  0.0330])\n",
      "params:tensor([  5.3335, -17.1111])\n",
      "Epoch 2651, Loss 2.930932\n",
      "grad:tensor([-0.0058,  0.0329])\n",
      "params:tensor([  5.3336, -17.1115])\n",
      "Epoch 2652, Loss 2.930921\n",
      "grad:tensor([-0.0058,  0.0328])\n",
      "params:tensor([  5.3336, -17.1118])\n",
      "Epoch 2653, Loss 2.930908\n",
      "grad:tensor([-0.0058,  0.0328])\n",
      "params:tensor([  5.3337, -17.1121])\n",
      "Epoch 2654, Loss 2.930899\n",
      "grad:tensor([-0.0058,  0.0327])\n",
      "params:tensor([  5.3337, -17.1124])\n",
      "Epoch 2655, Loss 2.930885\n",
      "grad:tensor([-0.0058,  0.0327])\n",
      "params:tensor([  5.3338, -17.1128])\n",
      "Epoch 2656, Loss 2.930876\n",
      "grad:tensor([-0.0058,  0.0326])\n",
      "params:tensor([  5.3339, -17.1131])\n",
      "Epoch 2657, Loss 2.930863\n",
      "grad:tensor([-0.0057,  0.0326])\n",
      "params:tensor([  5.3339, -17.1134])\n",
      "Epoch 2658, Loss 2.930854\n",
      "grad:tensor([-0.0057,  0.0325])\n",
      "params:tensor([  5.3340, -17.1137])\n",
      "Epoch 2659, Loss 2.930841\n",
      "grad:tensor([-0.0057,  0.0325])\n",
      "params:tensor([  5.3340, -17.1141])\n",
      "Epoch 2660, Loss 2.930833\n",
      "grad:tensor([-0.0057,  0.0324])\n",
      "params:tensor([  5.3341, -17.1144])\n",
      "Epoch 2661, Loss 2.930821\n",
      "grad:tensor([-0.0057,  0.0323])\n",
      "params:tensor([  5.3341, -17.1147])\n",
      "Epoch 2662, Loss 2.930811\n",
      "grad:tensor([-0.0057,  0.0323])\n",
      "params:tensor([  5.3342, -17.1150])\n",
      "Epoch 2663, Loss 2.930801\n",
      "grad:tensor([-0.0057,  0.0322])\n",
      "params:tensor([  5.3343, -17.1154])\n",
      "Epoch 2664, Loss 2.930788\n",
      "grad:tensor([-0.0057,  0.0322])\n",
      "params:tensor([  5.3343, -17.1157])\n",
      "Epoch 2665, Loss 2.930778\n",
      "grad:tensor([-0.0057,  0.0321])\n",
      "params:tensor([  5.3344, -17.1160])\n",
      "Epoch 2666, Loss 2.930767\n",
      "grad:tensor([-0.0057,  0.0321])\n",
      "params:tensor([  5.3344, -17.1163])\n",
      "Epoch 2667, Loss 2.930757\n",
      "grad:tensor([-0.0057,  0.0320])\n",
      "params:tensor([  5.3345, -17.1166])\n",
      "Epoch 2668, Loss 2.930746\n",
      "grad:tensor([-0.0056,  0.0320])\n",
      "params:tensor([  5.3345, -17.1170])\n",
      "Epoch 2669, Loss 2.930736\n",
      "grad:tensor([-0.0056,  0.0319])\n",
      "params:tensor([  5.3346, -17.1173])\n",
      "Epoch 2670, Loss 2.930724\n",
      "grad:tensor([-0.0056,  0.0319])\n",
      "params:tensor([  5.3347, -17.1176])\n",
      "Epoch 2671, Loss 2.930715\n",
      "grad:tensor([-0.0056,  0.0318])\n",
      "params:tensor([  5.3347, -17.1179])\n",
      "Epoch 2672, Loss 2.930704\n",
      "grad:tensor([-0.0056,  0.0317])\n",
      "params:tensor([  5.3348, -17.1182])\n",
      "Epoch 2673, Loss 2.930694\n",
      "grad:tensor([-0.0056,  0.0317])\n",
      "params:tensor([  5.3348, -17.1186])\n",
      "Epoch 2674, Loss 2.930685\n",
      "grad:tensor([-0.0056,  0.0316])\n",
      "params:tensor([  5.3349, -17.1189])\n",
      "Epoch 2675, Loss 2.930674\n",
      "grad:tensor([-0.0056,  0.0316])\n",
      "params:tensor([  5.3349, -17.1192])\n",
      "Epoch 2676, Loss 2.930663\n",
      "grad:tensor([-0.0056,  0.0315])\n",
      "params:tensor([  5.3350, -17.1195])\n",
      "Epoch 2677, Loss 2.930654\n",
      "grad:tensor([-0.0056,  0.0315])\n",
      "params:tensor([  5.3350, -17.1198])\n",
      "Epoch 2678, Loss 2.930644\n",
      "grad:tensor([-0.0055,  0.0314])\n",
      "params:tensor([  5.3351, -17.1201])\n",
      "Epoch 2679, Loss 2.930631\n",
      "grad:tensor([-0.0055,  0.0314])\n",
      "params:tensor([  5.3352, -17.1204])\n",
      "Epoch 2680, Loss 2.930621\n",
      "grad:tensor([-0.0055,  0.0313])\n",
      "params:tensor([  5.3352, -17.1208])\n",
      "Epoch 2681, Loss 2.930613\n",
      "grad:tensor([-0.0055,  0.0313])\n",
      "params:tensor([  5.3353, -17.1211])\n",
      "Epoch 2682, Loss 2.930603\n",
      "grad:tensor([-0.0055,  0.0312])\n",
      "params:tensor([  5.3353, -17.1214])\n",
      "Epoch 2683, Loss 2.930593\n",
      "grad:tensor([-0.0055,  0.0312])\n",
      "params:tensor([  5.3354, -17.1217])\n",
      "Epoch 2684, Loss 2.930582\n",
      "grad:tensor([-0.0055,  0.0311])\n",
      "params:tensor([  5.3354, -17.1220])\n",
      "Epoch 2685, Loss 2.930571\n",
      "grad:tensor([-0.0055,  0.0310])\n",
      "params:tensor([  5.3355, -17.1223])\n",
      "Epoch 2686, Loss 2.930562\n",
      "grad:tensor([-0.0055,  0.0310])\n",
      "params:tensor([  5.3355, -17.1226])\n",
      "Epoch 2687, Loss 2.930552\n",
      "grad:tensor([-0.0055,  0.0309])\n",
      "params:tensor([  5.3356, -17.1229])\n",
      "Epoch 2688, Loss 2.930543\n",
      "grad:tensor([-0.0055,  0.0309])\n",
      "params:tensor([  5.3356, -17.1232])\n",
      "Epoch 2689, Loss 2.930534\n",
      "grad:tensor([-0.0055,  0.0308])\n",
      "params:tensor([  5.3357, -17.1236])\n",
      "Epoch 2690, Loss 2.930523\n",
      "grad:tensor([-0.0054,  0.0308])\n",
      "params:tensor([  5.3358, -17.1239])\n",
      "Epoch 2691, Loss 2.930514\n",
      "grad:tensor([-0.0054,  0.0307])\n",
      "params:tensor([  5.3358, -17.1242])\n",
      "Epoch 2692, Loss 2.930502\n",
      "grad:tensor([-0.0054,  0.0307])\n",
      "params:tensor([  5.3359, -17.1245])\n",
      "Epoch 2693, Loss 2.930493\n",
      "grad:tensor([-0.0054,  0.0306])\n",
      "params:tensor([  5.3359, -17.1248])\n",
      "Epoch 2694, Loss 2.930482\n",
      "grad:tensor([-0.0054,  0.0306])\n",
      "params:tensor([  5.3360, -17.1251])\n",
      "Epoch 2695, Loss 2.930474\n",
      "grad:tensor([-0.0054,  0.0305])\n",
      "params:tensor([  5.3360, -17.1254])\n",
      "Epoch 2696, Loss 2.930464\n",
      "grad:tensor([-0.0054,  0.0305])\n",
      "params:tensor([  5.3361, -17.1257])\n",
      "Epoch 2697, Loss 2.930454\n",
      "grad:tensor([-0.0054,  0.0304])\n",
      "params:tensor([  5.3361, -17.1260])\n",
      "Epoch 2698, Loss 2.930445\n",
      "grad:tensor([-0.0054,  0.0304])\n",
      "params:tensor([  5.3362, -17.1263])\n",
      "Epoch 2699, Loss 2.930436\n",
      "grad:tensor([-0.0054,  0.0303])\n",
      "params:tensor([  5.3362, -17.1266])\n",
      "Epoch 2700, Loss 2.930426\n",
      "grad:tensor([-0.0054,  0.0303])\n",
      "params:tensor([  5.3363, -17.1269])\n",
      "Epoch 2701, Loss 2.930416\n",
      "grad:tensor([-0.0054,  0.0302])\n",
      "params:tensor([  5.3364, -17.1272])\n",
      "Epoch 2702, Loss 2.930408\n",
      "grad:tensor([-0.0053,  0.0302])\n",
      "params:tensor([  5.3364, -17.1275])\n",
      "Epoch 2703, Loss 2.930398\n",
      "grad:tensor([-0.0053,  0.0301])\n",
      "params:tensor([  5.3365, -17.1278])\n",
      "Epoch 2704, Loss 2.930388\n",
      "grad:tensor([-0.0053,  0.0301])\n",
      "params:tensor([  5.3365, -17.1281])\n",
      "Epoch 2705, Loss 2.930380\n",
      "grad:tensor([-0.0053,  0.0300])\n",
      "params:tensor([  5.3366, -17.1284])\n",
      "Epoch 2706, Loss 2.930370\n",
      "grad:tensor([-0.0053,  0.0300])\n",
      "params:tensor([  5.3366, -17.1287])\n",
      "Epoch 2707, Loss 2.930360\n",
      "grad:tensor([-0.0053,  0.0299])\n",
      "params:tensor([  5.3367, -17.1290])\n",
      "Epoch 2708, Loss 2.930353\n",
      "grad:tensor([-0.0053,  0.0299])\n",
      "params:tensor([  5.3367, -17.1293])\n",
      "Epoch 2709, Loss 2.930342\n",
      "grad:tensor([-0.0053,  0.0298])\n",
      "params:tensor([  5.3368, -17.1296])\n",
      "Epoch 2710, Loss 2.930335\n",
      "grad:tensor([-0.0053,  0.0298])\n",
      "params:tensor([  5.3368, -17.1299])\n",
      "Epoch 2711, Loss 2.930325\n",
      "grad:tensor([-0.0053,  0.0297])\n",
      "params:tensor([  5.3369, -17.1302])\n",
      "Epoch 2712, Loss 2.930315\n",
      "grad:tensor([-0.0053,  0.0297])\n",
      "params:tensor([  5.3369, -17.1305])\n",
      "Epoch 2713, Loss 2.930306\n",
      "grad:tensor([-0.0052,  0.0296])\n",
      "params:tensor([  5.3370, -17.1308])\n",
      "Epoch 2714, Loss 2.930298\n",
      "grad:tensor([-0.0052,  0.0296])\n",
      "params:tensor([  5.3370, -17.1311])\n",
      "Epoch 2715, Loss 2.930288\n",
      "grad:tensor([-0.0052,  0.0295])\n",
      "params:tensor([  5.3371, -17.1314])\n",
      "Epoch 2716, Loss 2.930279\n",
      "grad:tensor([-0.0052,  0.0295])\n",
      "params:tensor([  5.3371, -17.1317])\n",
      "Epoch 2717, Loss 2.930270\n",
      "grad:tensor([-0.0052,  0.0294])\n",
      "params:tensor([  5.3372, -17.1320])\n",
      "Epoch 2718, Loss 2.930262\n",
      "grad:tensor([-0.0052,  0.0294])\n",
      "params:tensor([  5.3372, -17.1323])\n",
      "Epoch 2719, Loss 2.930254\n",
      "grad:tensor([-0.0052,  0.0293])\n",
      "params:tensor([  5.3373, -17.1326])\n",
      "Epoch 2720, Loss 2.930244\n",
      "grad:tensor([-0.0052,  0.0293])\n",
      "params:tensor([  5.3373, -17.1329])\n",
      "Epoch 2721, Loss 2.930235\n",
      "grad:tensor([-0.0052,  0.0292])\n",
      "params:tensor([  5.3374, -17.1332])\n",
      "Epoch 2722, Loss 2.930226\n",
      "grad:tensor([-0.0052,  0.0292])\n",
      "params:tensor([  5.3375, -17.1334])\n",
      "Epoch 2723, Loss 2.930218\n",
      "grad:tensor([-0.0051,  0.0291])\n",
      "params:tensor([  5.3375, -17.1337])\n",
      "Epoch 2724, Loss 2.930209\n",
      "grad:tensor([-0.0051,  0.0291])\n",
      "params:tensor([  5.3376, -17.1340])\n",
      "Epoch 2725, Loss 2.930201\n",
      "grad:tensor([-0.0051,  0.0290])\n",
      "params:tensor([  5.3376, -17.1343])\n",
      "Epoch 2726, Loss 2.930190\n",
      "grad:tensor([-0.0051,  0.0290])\n",
      "params:tensor([  5.3377, -17.1346])\n",
      "Epoch 2727, Loss 2.930183\n",
      "grad:tensor([-0.0051,  0.0289])\n",
      "params:tensor([  5.3377, -17.1349])\n",
      "Epoch 2728, Loss 2.930173\n",
      "grad:tensor([-0.0051,  0.0289])\n",
      "params:tensor([  5.3378, -17.1352])\n",
      "Epoch 2729, Loss 2.930166\n",
      "grad:tensor([-0.0051,  0.0288])\n",
      "params:tensor([  5.3378, -17.1355])\n",
      "Epoch 2730, Loss 2.930156\n",
      "grad:tensor([-0.0051,  0.0288])\n",
      "params:tensor([  5.3379, -17.1358])\n",
      "Epoch 2731, Loss 2.930149\n",
      "grad:tensor([-0.0051,  0.0287])\n",
      "params:tensor([  5.3379, -17.1360])\n",
      "Epoch 2732, Loss 2.930139\n",
      "grad:tensor([-0.0051,  0.0287])\n",
      "params:tensor([  5.3380, -17.1363])\n",
      "Epoch 2733, Loss 2.930131\n",
      "grad:tensor([-0.0050,  0.0286])\n",
      "params:tensor([  5.3380, -17.1366])\n",
      "Epoch 2734, Loss 2.930123\n",
      "grad:tensor([-0.0050,  0.0286])\n",
      "params:tensor([  5.3381, -17.1369])\n",
      "Epoch 2735, Loss 2.930113\n",
      "grad:tensor([-0.0050,  0.0285])\n",
      "params:tensor([  5.3381, -17.1372])\n",
      "Epoch 2736, Loss 2.930107\n",
      "grad:tensor([-0.0051,  0.0285])\n",
      "params:tensor([  5.3382, -17.1375])\n",
      "Epoch 2737, Loss 2.930099\n",
      "grad:tensor([-0.0050,  0.0284])\n",
      "params:tensor([  5.3382, -17.1378])\n",
      "Epoch 2738, Loss 2.930090\n",
      "grad:tensor([-0.0050,  0.0284])\n",
      "params:tensor([  5.3383, -17.1380])\n",
      "Epoch 2739, Loss 2.930081\n",
      "grad:tensor([-0.0050,  0.0283])\n",
      "params:tensor([  5.3383, -17.1383])\n",
      "Epoch 2740, Loss 2.930073\n",
      "grad:tensor([-0.0050,  0.0283])\n",
      "params:tensor([  5.3384, -17.1386])\n",
      "Epoch 2741, Loss 2.930064\n",
      "grad:tensor([-0.0050,  0.0282])\n",
      "params:tensor([  5.3384, -17.1389])\n",
      "Epoch 2742, Loss 2.930056\n",
      "grad:tensor([-0.0050,  0.0282])\n",
      "params:tensor([  5.3385, -17.1392])\n",
      "Epoch 2743, Loss 2.930048\n",
      "grad:tensor([-0.0050,  0.0281])\n",
      "params:tensor([  5.3385, -17.1395])\n",
      "Epoch 2744, Loss 2.930041\n",
      "grad:tensor([-0.0050,  0.0281])\n",
      "params:tensor([  5.3386, -17.1397])\n",
      "Epoch 2745, Loss 2.930032\n",
      "grad:tensor([-0.0050,  0.0280])\n",
      "params:tensor([  5.3386, -17.1400])\n",
      "Epoch 2746, Loss 2.930022\n",
      "grad:tensor([-0.0050,  0.0280])\n",
      "params:tensor([  5.3387, -17.1403])\n",
      "Epoch 2747, Loss 2.930016\n",
      "grad:tensor([-0.0049,  0.0279])\n",
      "params:tensor([  5.3387, -17.1406])\n",
      "Epoch 2748, Loss 2.930008\n",
      "grad:tensor([-0.0049,  0.0279])\n",
      "params:tensor([  5.3388, -17.1409])\n",
      "Epoch 2749, Loss 2.930000\n",
      "grad:tensor([-0.0049,  0.0279])\n",
      "params:tensor([  5.3388, -17.1411])\n",
      "Epoch 2750, Loss 2.929992\n",
      "grad:tensor([-0.0049,  0.0278])\n",
      "params:tensor([  5.3389, -17.1414])\n",
      "Epoch 2751, Loss 2.929983\n",
      "grad:tensor([-0.0049,  0.0278])\n",
      "params:tensor([  5.3389, -17.1417])\n",
      "Epoch 2752, Loss 2.929975\n",
      "grad:tensor([-0.0049,  0.0277])\n",
      "params:tensor([  5.3390, -17.1420])\n",
      "Epoch 2753, Loss 2.929968\n",
      "grad:tensor([-0.0049,  0.0277])\n",
      "params:tensor([  5.3390, -17.1422])\n",
      "Epoch 2754, Loss 2.929960\n",
      "grad:tensor([-0.0049,  0.0276])\n",
      "params:tensor([  5.3391, -17.1425])\n",
      "Epoch 2755, Loss 2.929953\n",
      "grad:tensor([-0.0049,  0.0276])\n",
      "params:tensor([  5.3391, -17.1428])\n",
      "Epoch 2756, Loss 2.929945\n",
      "grad:tensor([-0.0049,  0.0275])\n",
      "params:tensor([  5.3392, -17.1431])\n",
      "Epoch 2757, Loss 2.929936\n",
      "grad:tensor([-0.0049,  0.0275])\n",
      "params:tensor([  5.3392, -17.1433])\n",
      "Epoch 2758, Loss 2.929929\n",
      "grad:tensor([-0.0049,  0.0274])\n",
      "params:tensor([  5.3392, -17.1436])\n",
      "Epoch 2759, Loss 2.929921\n",
      "grad:tensor([-0.0048,  0.0274])\n",
      "params:tensor([  5.3393, -17.1439])\n",
      "Epoch 2760, Loss 2.929914\n",
      "grad:tensor([-0.0049,  0.0273])\n",
      "params:tensor([  5.3393, -17.1442])\n",
      "Epoch 2761, Loss 2.929905\n",
      "grad:tensor([-0.0048,  0.0273])\n",
      "params:tensor([  5.3394, -17.1444])\n",
      "Epoch 2762, Loss 2.929896\n",
      "grad:tensor([-0.0048,  0.0272])\n",
      "params:tensor([  5.3394, -17.1447])\n",
      "Epoch 2763, Loss 2.929891\n",
      "grad:tensor([-0.0048,  0.0272])\n",
      "params:tensor([  5.3395, -17.1450])\n",
      "Epoch 2764, Loss 2.929882\n",
      "grad:tensor([-0.0048,  0.0271])\n",
      "params:tensor([  5.3395, -17.1453])\n",
      "Epoch 2765, Loss 2.929875\n",
      "grad:tensor([-0.0048,  0.0271])\n",
      "params:tensor([  5.3396, -17.1455])\n",
      "Epoch 2766, Loss 2.929868\n",
      "grad:tensor([-0.0048,  0.0271])\n",
      "params:tensor([  5.3396, -17.1458])\n",
      "Epoch 2767, Loss 2.929859\n",
      "grad:tensor([-0.0048,  0.0270])\n",
      "params:tensor([  5.3397, -17.1461])\n",
      "Epoch 2768, Loss 2.929852\n",
      "grad:tensor([-0.0048,  0.0270])\n",
      "params:tensor([  5.3397, -17.1463])\n",
      "Epoch 2769, Loss 2.929845\n",
      "grad:tensor([-0.0048,  0.0269])\n",
      "params:tensor([  5.3398, -17.1466])\n",
      "Epoch 2770, Loss 2.929838\n",
      "grad:tensor([-0.0047,  0.0269])\n",
      "params:tensor([  5.3398, -17.1469])\n",
      "Epoch 2771, Loss 2.929830\n",
      "grad:tensor([-0.0047,  0.0268])\n",
      "params:tensor([  5.3399, -17.1471])\n",
      "Epoch 2772, Loss 2.929822\n",
      "grad:tensor([-0.0047,  0.0268])\n",
      "params:tensor([  5.3399, -17.1474])\n",
      "Epoch 2773, Loss 2.929816\n",
      "grad:tensor([-0.0047,  0.0267])\n",
      "params:tensor([  5.3400, -17.1477])\n",
      "Epoch 2774, Loss 2.929807\n",
      "grad:tensor([-0.0047,  0.0267])\n",
      "params:tensor([  5.3400, -17.1479])\n",
      "Epoch 2775, Loss 2.929800\n",
      "grad:tensor([-0.0047,  0.0266])\n",
      "params:tensor([  5.3401, -17.1482])\n",
      "Epoch 2776, Loss 2.929794\n",
      "grad:tensor([-0.0047,  0.0266])\n",
      "params:tensor([  5.3401, -17.1485])\n",
      "Epoch 2777, Loss 2.929786\n",
      "grad:tensor([-0.0047,  0.0266])\n",
      "params:tensor([  5.3402, -17.1487])\n",
      "Epoch 2778, Loss 2.929778\n",
      "grad:tensor([-0.0047,  0.0265])\n",
      "params:tensor([  5.3402, -17.1490])\n",
      "Epoch 2779, Loss 2.929771\n",
      "grad:tensor([-0.0047,  0.0265])\n",
      "params:tensor([  5.3402, -17.1493])\n",
      "Epoch 2780, Loss 2.929765\n",
      "grad:tensor([-0.0047,  0.0264])\n",
      "params:tensor([  5.3403, -17.1495])\n",
      "Epoch 2781, Loss 2.929757\n",
      "grad:tensor([-0.0047,  0.0264])\n",
      "params:tensor([  5.3403, -17.1498])\n",
      "Epoch 2782, Loss 2.929750\n",
      "grad:tensor([-0.0046,  0.0263])\n",
      "params:tensor([  5.3404, -17.1501])\n",
      "Epoch 2783, Loss 2.929743\n",
      "grad:tensor([-0.0046,  0.0263])\n",
      "params:tensor([  5.3404, -17.1503])\n",
      "Epoch 2784, Loss 2.929735\n",
      "grad:tensor([-0.0046,  0.0262])\n",
      "params:tensor([  5.3405, -17.1506])\n",
      "Epoch 2785, Loss 2.929729\n",
      "grad:tensor([-0.0047,  0.0262])\n",
      "params:tensor([  5.3405, -17.1508])\n",
      "Epoch 2786, Loss 2.929722\n",
      "grad:tensor([-0.0046,  0.0262])\n",
      "params:tensor([  5.3406, -17.1511])\n",
      "Epoch 2787, Loss 2.929714\n",
      "grad:tensor([-0.0046,  0.0261])\n",
      "params:tensor([  5.3406, -17.1514])\n",
      "Epoch 2788, Loss 2.929707\n",
      "grad:tensor([-0.0046,  0.0261])\n",
      "params:tensor([  5.3407, -17.1516])\n",
      "Epoch 2789, Loss 2.929701\n",
      "grad:tensor([-0.0046,  0.0260])\n",
      "params:tensor([  5.3407, -17.1519])\n",
      "Epoch 2790, Loss 2.929692\n",
      "grad:tensor([-0.0046,  0.0260])\n",
      "params:tensor([  5.3408, -17.1522])\n",
      "Epoch 2791, Loss 2.929685\n",
      "grad:tensor([-0.0046,  0.0259])\n",
      "params:tensor([  5.3408, -17.1524])\n",
      "Epoch 2792, Loss 2.929681\n",
      "grad:tensor([-0.0046,  0.0259])\n",
      "params:tensor([  5.3408, -17.1527])\n",
      "Epoch 2793, Loss 2.929672\n",
      "grad:tensor([-0.0046,  0.0258])\n",
      "params:tensor([  5.3409, -17.1529])\n",
      "Epoch 2794, Loss 2.929666\n",
      "grad:tensor([-0.0046,  0.0258])\n",
      "params:tensor([  5.3409, -17.1532])\n",
      "Epoch 2795, Loss 2.929659\n",
      "grad:tensor([-0.0045,  0.0258])\n",
      "params:tensor([  5.3410, -17.1534])\n",
      "Epoch 2796, Loss 2.929653\n",
      "grad:tensor([-0.0045,  0.0257])\n",
      "params:tensor([  5.3410, -17.1537])\n",
      "Epoch 2797, Loss 2.929646\n",
      "grad:tensor([-0.0045,  0.0257])\n",
      "params:tensor([  5.3411, -17.1540])\n",
      "Epoch 2798, Loss 2.929638\n",
      "grad:tensor([-0.0045,  0.0256])\n",
      "params:tensor([  5.3411, -17.1542])\n",
      "Epoch 2799, Loss 2.929632\n",
      "grad:tensor([-0.0045,  0.0256])\n",
      "params:tensor([  5.3412, -17.1545])\n",
      "Epoch 2800, Loss 2.929626\n",
      "grad:tensor([-0.0045,  0.0255])\n",
      "params:tensor([  5.3412, -17.1547])\n",
      "Epoch 2801, Loss 2.929620\n",
      "grad:tensor([-0.0045,  0.0255])\n",
      "params:tensor([  5.3413, -17.1550])\n",
      "Epoch 2802, Loss 2.929611\n",
      "grad:tensor([-0.0045,  0.0254])\n",
      "params:tensor([  5.3413, -17.1552])\n",
      "Epoch 2803, Loss 2.929605\n",
      "grad:tensor([-0.0045,  0.0254])\n",
      "params:tensor([  5.3413, -17.1555])\n",
      "Epoch 2804, Loss 2.929600\n",
      "grad:tensor([-0.0045,  0.0254])\n",
      "params:tensor([  5.3414, -17.1557])\n",
      "Epoch 2805, Loss 2.929592\n",
      "grad:tensor([-0.0045,  0.0253])\n",
      "params:tensor([  5.3414, -17.1560])\n",
      "Epoch 2806, Loss 2.929586\n",
      "grad:tensor([-0.0045,  0.0253])\n",
      "params:tensor([  5.3415, -17.1562])\n",
      "Epoch 2807, Loss 2.929579\n",
      "grad:tensor([-0.0045,  0.0252])\n",
      "params:tensor([  5.3415, -17.1565])\n",
      "Epoch 2808, Loss 2.929572\n",
      "grad:tensor([-0.0044,  0.0252])\n",
      "params:tensor([  5.3416, -17.1568])\n",
      "Epoch 2809, Loss 2.929566\n",
      "grad:tensor([-0.0044,  0.0251])\n",
      "params:tensor([  5.3416, -17.1570])\n",
      "Epoch 2810, Loss 2.929559\n",
      "grad:tensor([-0.0044,  0.0251])\n",
      "params:tensor([  5.3417, -17.1573])\n",
      "Epoch 2811, Loss 2.929551\n",
      "grad:tensor([-0.0044,  0.0251])\n",
      "params:tensor([  5.3417, -17.1575])\n",
      "Epoch 2812, Loss 2.929545\n",
      "grad:tensor([-0.0044,  0.0250])\n",
      "params:tensor([  5.3417, -17.1578])\n",
      "Epoch 2813, Loss 2.929540\n",
      "grad:tensor([-0.0044,  0.0250])\n",
      "params:tensor([  5.3418, -17.1580])\n",
      "Epoch 2814, Loss 2.929533\n",
      "grad:tensor([-0.0044,  0.0249])\n",
      "params:tensor([  5.3418, -17.1583])\n",
      "Epoch 2815, Loss 2.929528\n",
      "grad:tensor([-0.0044,  0.0249])\n",
      "params:tensor([  5.3419, -17.1585])\n",
      "Epoch 2816, Loss 2.929521\n",
      "grad:tensor([-0.0044,  0.0249])\n",
      "params:tensor([  5.3419, -17.1588])\n",
      "Epoch 2817, Loss 2.929513\n",
      "grad:tensor([-0.0044,  0.0248])\n",
      "params:tensor([  5.3420, -17.1590])\n",
      "Epoch 2818, Loss 2.929507\n",
      "grad:tensor([-0.0043,  0.0248])\n",
      "params:tensor([  5.3420, -17.1592])\n",
      "Epoch 2819, Loss 2.929501\n",
      "grad:tensor([-0.0044,  0.0247])\n",
      "params:tensor([  5.3421, -17.1595])\n",
      "Epoch 2820, Loss 2.929496\n",
      "grad:tensor([-0.0044,  0.0247])\n",
      "params:tensor([  5.3421, -17.1597])\n",
      "Epoch 2821, Loss 2.929489\n",
      "grad:tensor([-0.0044,  0.0246])\n",
      "params:tensor([  5.3421, -17.1600])\n",
      "Epoch 2822, Loss 2.929482\n",
      "grad:tensor([-0.0043,  0.0246])\n",
      "params:tensor([  5.3422, -17.1602])\n",
      "Epoch 2823, Loss 2.929476\n",
      "grad:tensor([-0.0043,  0.0246])\n",
      "params:tensor([  5.3422, -17.1605])\n",
      "Epoch 2824, Loss 2.929471\n",
      "grad:tensor([-0.0043,  0.0245])\n",
      "params:tensor([  5.3423, -17.1607])\n",
      "Epoch 2825, Loss 2.929463\n",
      "grad:tensor([-0.0043,  0.0245])\n",
      "params:tensor([  5.3423, -17.1610])\n",
      "Epoch 2826, Loss 2.929458\n",
      "grad:tensor([-0.0043,  0.0244])\n",
      "params:tensor([  5.3424, -17.1612])\n",
      "Epoch 2827, Loss 2.929452\n",
      "grad:tensor([-0.0043,  0.0244])\n",
      "params:tensor([  5.3424, -17.1615])\n",
      "Epoch 2828, Loss 2.929445\n",
      "grad:tensor([-0.0043,  0.0243])\n",
      "params:tensor([  5.3424, -17.1617])\n",
      "Epoch 2829, Loss 2.929439\n",
      "grad:tensor([-0.0043,  0.0243])\n",
      "params:tensor([  5.3425, -17.1619])\n",
      "Epoch 2830, Loss 2.929433\n",
      "grad:tensor([-0.0043,  0.0243])\n",
      "params:tensor([  5.3425, -17.1622])\n",
      "Epoch 2831, Loss 2.929427\n",
      "grad:tensor([-0.0043,  0.0242])\n",
      "params:tensor([  5.3426, -17.1624])\n",
      "Epoch 2832, Loss 2.929421\n",
      "grad:tensor([-0.0043,  0.0242])\n",
      "params:tensor([  5.3426, -17.1627])\n",
      "Epoch 2833, Loss 2.929415\n",
      "grad:tensor([-0.0043,  0.0241])\n",
      "params:tensor([  5.3427, -17.1629])\n",
      "Epoch 2834, Loss 2.929409\n",
      "grad:tensor([-0.0043,  0.0241])\n",
      "params:tensor([  5.3427, -17.1632])\n",
      "Epoch 2835, Loss 2.929404\n",
      "grad:tensor([-0.0043,  0.0241])\n",
      "params:tensor([  5.3427, -17.1634])\n",
      "Epoch 2836, Loss 2.929396\n",
      "grad:tensor([-0.0042,  0.0240])\n",
      "params:tensor([  5.3428, -17.1636])\n",
      "Epoch 2837, Loss 2.929391\n",
      "grad:tensor([-0.0042,  0.0240])\n",
      "params:tensor([  5.3428, -17.1639])\n",
      "Epoch 2838, Loss 2.929383\n",
      "grad:tensor([-0.0042,  0.0239])\n",
      "params:tensor([  5.3429, -17.1641])\n",
      "Epoch 2839, Loss 2.929380\n",
      "grad:tensor([-0.0042,  0.0239])\n",
      "params:tensor([  5.3429, -17.1644])\n",
      "Epoch 2840, Loss 2.929373\n",
      "grad:tensor([-0.0042,  0.0239])\n",
      "params:tensor([  5.3430, -17.1646])\n",
      "Epoch 2841, Loss 2.929368\n",
      "grad:tensor([-0.0042,  0.0238])\n",
      "params:tensor([  5.3430, -17.1648])\n",
      "Epoch 2842, Loss 2.929361\n",
      "grad:tensor([-0.0042,  0.0238])\n",
      "params:tensor([  5.3430, -17.1651])\n",
      "Epoch 2843, Loss 2.929356\n",
      "grad:tensor([-0.0042,  0.0237])\n",
      "params:tensor([  5.3431, -17.1653])\n",
      "Epoch 2844, Loss 2.929351\n",
      "grad:tensor([-0.0042,  0.0237])\n",
      "params:tensor([  5.3431, -17.1655])\n",
      "Epoch 2845, Loss 2.929344\n",
      "grad:tensor([-0.0042,  0.0237])\n",
      "params:tensor([  5.3432, -17.1658])\n",
      "Epoch 2846, Loss 2.929338\n",
      "grad:tensor([-0.0042,  0.0236])\n",
      "params:tensor([  5.3432, -17.1660])\n",
      "Epoch 2847, Loss 2.929332\n",
      "grad:tensor([-0.0042,  0.0236])\n",
      "params:tensor([  5.3432, -17.1662])\n",
      "Epoch 2848, Loss 2.929328\n",
      "grad:tensor([-0.0042,  0.0235])\n",
      "params:tensor([  5.3433, -17.1665])\n",
      "Epoch 2849, Loss 2.929321\n",
      "grad:tensor([-0.0041,  0.0235])\n",
      "params:tensor([  5.3433, -17.1667])\n",
      "Epoch 2850, Loss 2.929316\n",
      "grad:tensor([-0.0041,  0.0235])\n",
      "params:tensor([  5.3434, -17.1670])\n",
      "Epoch 2851, Loss 2.929309\n",
      "grad:tensor([-0.0041,  0.0234])\n",
      "params:tensor([  5.3434, -17.1672])\n",
      "Epoch 2852, Loss 2.929304\n",
      "grad:tensor([-0.0041,  0.0234])\n",
      "params:tensor([  5.3435, -17.1674])\n",
      "Epoch 2853, Loss 2.929300\n",
      "grad:tensor([-0.0041,  0.0233])\n",
      "params:tensor([  5.3435, -17.1677])\n",
      "Epoch 2854, Loss 2.929293\n",
      "grad:tensor([-0.0041,  0.0233])\n",
      "params:tensor([  5.3435, -17.1679])\n",
      "Epoch 2855, Loss 2.929288\n",
      "grad:tensor([-0.0041,  0.0233])\n",
      "params:tensor([  5.3436, -17.1681])\n",
      "Epoch 2856, Loss 2.929282\n",
      "grad:tensor([-0.0041,  0.0232])\n",
      "params:tensor([  5.3436, -17.1684])\n",
      "Epoch 2857, Loss 2.929277\n",
      "grad:tensor([-0.0041,  0.0232])\n",
      "params:tensor([  5.3437, -17.1686])\n",
      "Epoch 2858, Loss 2.929271\n",
      "grad:tensor([-0.0041,  0.0231])\n",
      "params:tensor([  5.3437, -17.1688])\n",
      "Epoch 2859, Loss 2.929266\n",
      "grad:tensor([-0.0041,  0.0231])\n",
      "params:tensor([  5.3437, -17.1690])\n",
      "Epoch 2860, Loss 2.929260\n",
      "grad:tensor([-0.0041,  0.0231])\n",
      "params:tensor([  5.3438, -17.1693])\n",
      "Epoch 2861, Loss 2.929255\n",
      "grad:tensor([-0.0041,  0.0230])\n",
      "params:tensor([  5.3438, -17.1695])\n",
      "Epoch 2862, Loss 2.929250\n",
      "grad:tensor([-0.0041,  0.0230])\n",
      "params:tensor([  5.3439, -17.1697])\n",
      "Epoch 2863, Loss 2.929244\n",
      "grad:tensor([-0.0040,  0.0229])\n",
      "params:tensor([  5.3439, -17.1700])\n",
      "Epoch 2864, Loss 2.929238\n",
      "grad:tensor([-0.0040,  0.0229])\n",
      "params:tensor([  5.3439, -17.1702])\n",
      "Epoch 2865, Loss 2.929234\n",
      "grad:tensor([-0.0040,  0.0229])\n",
      "params:tensor([  5.3440, -17.1704])\n",
      "Epoch 2866, Loss 2.929228\n",
      "grad:tensor([-0.0040,  0.0228])\n",
      "params:tensor([  5.3440, -17.1707])\n",
      "Epoch 2867, Loss 2.929222\n",
      "grad:tensor([-0.0040,  0.0228])\n",
      "params:tensor([  5.3441, -17.1709])\n",
      "Epoch 2868, Loss 2.929217\n",
      "grad:tensor([-0.0040,  0.0227])\n",
      "params:tensor([  5.3441, -17.1711])\n",
      "Epoch 2869, Loss 2.929211\n",
      "grad:tensor([-0.0040,  0.0227])\n",
      "params:tensor([  5.3441, -17.1713])\n",
      "Epoch 2870, Loss 2.929208\n",
      "grad:tensor([-0.0040,  0.0227])\n",
      "params:tensor([  5.3442, -17.1716])\n",
      "Epoch 2871, Loss 2.929201\n",
      "grad:tensor([-0.0040,  0.0226])\n",
      "params:tensor([  5.3442, -17.1718])\n",
      "Epoch 2872, Loss 2.929195\n",
      "grad:tensor([-0.0040,  0.0226])\n",
      "params:tensor([  5.3443, -17.1720])\n",
      "Epoch 2873, Loss 2.929191\n",
      "grad:tensor([-0.0040,  0.0226])\n",
      "params:tensor([  5.3443, -17.1722])\n",
      "Epoch 2874, Loss 2.929185\n",
      "grad:tensor([-0.0040,  0.0225])\n",
      "params:tensor([  5.3443, -17.1725])\n",
      "Epoch 2875, Loss 2.929180\n",
      "grad:tensor([-0.0040,  0.0225])\n",
      "params:tensor([  5.3444, -17.1727])\n",
      "Epoch 2876, Loss 2.929175\n",
      "grad:tensor([-0.0040,  0.0224])\n",
      "params:tensor([  5.3444, -17.1729])\n",
      "Epoch 2877, Loss 2.929170\n",
      "grad:tensor([-0.0040,  0.0224])\n",
      "params:tensor([  5.3445, -17.1731])\n",
      "Epoch 2878, Loss 2.929165\n",
      "grad:tensor([-0.0040,  0.0224])\n",
      "params:tensor([  5.3445, -17.1734])\n",
      "Epoch 2879, Loss 2.929160\n",
      "grad:tensor([-0.0039,  0.0223])\n",
      "params:tensor([  5.3445, -17.1736])\n",
      "Epoch 2880, Loss 2.929155\n",
      "grad:tensor([-0.0039,  0.0223])\n",
      "params:tensor([  5.3446, -17.1738])\n",
      "Epoch 2881, Loss 2.929149\n",
      "grad:tensor([-0.0039,  0.0223])\n",
      "params:tensor([  5.3446, -17.1740])\n",
      "Epoch 2882, Loss 2.929143\n",
      "grad:tensor([-0.0039,  0.0222])\n",
      "params:tensor([  5.3447, -17.1742])\n",
      "Epoch 2883, Loss 2.929139\n",
      "grad:tensor([-0.0039,  0.0222])\n",
      "params:tensor([  5.3447, -17.1745])\n",
      "Epoch 2884, Loss 2.929133\n",
      "grad:tensor([-0.0039,  0.0221])\n",
      "params:tensor([  5.3447, -17.1747])\n",
      "Epoch 2885, Loss 2.929128\n",
      "grad:tensor([-0.0039,  0.0221])\n",
      "params:tensor([  5.3448, -17.1749])\n",
      "Epoch 2886, Loss 2.929122\n",
      "grad:tensor([-0.0039,  0.0221])\n",
      "params:tensor([  5.3448, -17.1751])\n",
      "Epoch 2887, Loss 2.929119\n",
      "grad:tensor([-0.0039,  0.0220])\n",
      "params:tensor([  5.3449, -17.1754])\n",
      "Epoch 2888, Loss 2.929113\n",
      "grad:tensor([-0.0039,  0.0220])\n",
      "params:tensor([  5.3449, -17.1756])\n",
      "Epoch 2889, Loss 2.929108\n",
      "grad:tensor([-0.0039,  0.0220])\n",
      "params:tensor([  5.3449, -17.1758])\n",
      "Epoch 2890, Loss 2.929104\n",
      "grad:tensor([-0.0039,  0.0219])\n",
      "params:tensor([  5.3450, -17.1760])\n",
      "Epoch 2891, Loss 2.929099\n",
      "grad:tensor([-0.0039,  0.0219])\n",
      "params:tensor([  5.3450, -17.1762])\n",
      "Epoch 2892, Loss 2.929093\n",
      "grad:tensor([-0.0039,  0.0218])\n",
      "params:tensor([  5.3450, -17.1764])\n",
      "Epoch 2893, Loss 2.929088\n",
      "grad:tensor([-0.0039,  0.0218])\n",
      "params:tensor([  5.3451, -17.1767])\n",
      "Epoch 2894, Loss 2.929083\n",
      "grad:tensor([-0.0038,  0.0218])\n",
      "params:tensor([  5.3451, -17.1769])\n",
      "Epoch 2895, Loss 2.929079\n",
      "grad:tensor([-0.0038,  0.0217])\n",
      "params:tensor([  5.3452, -17.1771])\n",
      "Epoch 2896, Loss 2.929074\n",
      "grad:tensor([-0.0038,  0.0217])\n",
      "params:tensor([  5.3452, -17.1773])\n",
      "Epoch 2897, Loss 2.929069\n",
      "grad:tensor([-0.0038,  0.0217])\n",
      "params:tensor([  5.3452, -17.1775])\n",
      "Epoch 2898, Loss 2.929065\n",
      "grad:tensor([-0.0038,  0.0216])\n",
      "params:tensor([  5.3453, -17.1777])\n",
      "Epoch 2899, Loss 2.929058\n",
      "grad:tensor([-0.0038,  0.0216])\n",
      "params:tensor([  5.3453, -17.1780])\n",
      "Epoch 2900, Loss 2.929054\n",
      "grad:tensor([-0.0038,  0.0215])\n",
      "params:tensor([  5.3454, -17.1782])\n",
      "Epoch 2901, Loss 2.929050\n",
      "grad:tensor([-0.0038,  0.0215])\n",
      "params:tensor([  5.3454, -17.1784])\n",
      "Epoch 2902, Loss 2.929044\n",
      "grad:tensor([-0.0038,  0.0215])\n",
      "params:tensor([  5.3454, -17.1786])\n",
      "Epoch 2903, Loss 2.929041\n",
      "grad:tensor([-0.0038,  0.0214])\n",
      "params:tensor([  5.3455, -17.1788])\n",
      "Epoch 2904, Loss 2.929036\n",
      "grad:tensor([-0.0038,  0.0214])\n",
      "params:tensor([  5.3455, -17.1790])\n",
      "Epoch 2905, Loss 2.929031\n",
      "grad:tensor([-0.0038,  0.0214])\n",
      "params:tensor([  5.3455, -17.1793])\n",
      "Epoch 2906, Loss 2.929025\n",
      "grad:tensor([-0.0038,  0.0213])\n",
      "params:tensor([  5.3456, -17.1795])\n",
      "Epoch 2907, Loss 2.929021\n",
      "grad:tensor([-0.0038,  0.0213])\n",
      "params:tensor([  5.3456, -17.1797])\n",
      "Epoch 2908, Loss 2.929017\n",
      "grad:tensor([-0.0037,  0.0213])\n",
      "params:tensor([  5.3457, -17.1799])\n",
      "Epoch 2909, Loss 2.929012\n",
      "grad:tensor([-0.0037,  0.0212])\n",
      "params:tensor([  5.3457, -17.1801])\n",
      "Epoch 2910, Loss 2.929007\n",
      "grad:tensor([-0.0037,  0.0212])\n",
      "params:tensor([  5.3457, -17.1803])\n",
      "Epoch 2911, Loss 2.929003\n",
      "grad:tensor([-0.0037,  0.0211])\n",
      "params:tensor([  5.3458, -17.1805])\n",
      "Epoch 2912, Loss 2.928999\n",
      "grad:tensor([-0.0037,  0.0211])\n",
      "params:tensor([  5.3458, -17.1807])\n",
      "Epoch 2913, Loss 2.928993\n",
      "grad:tensor([-0.0037,  0.0211])\n",
      "params:tensor([  5.3458, -17.1809])\n",
      "Epoch 2914, Loss 2.928989\n",
      "grad:tensor([-0.0037,  0.0210])\n",
      "params:tensor([  5.3459, -17.1812])\n",
      "Epoch 2915, Loss 2.928985\n",
      "grad:tensor([-0.0037,  0.0210])\n",
      "params:tensor([  5.3459, -17.1814])\n",
      "Epoch 2916, Loss 2.928980\n",
      "grad:tensor([-0.0037,  0.0210])\n",
      "params:tensor([  5.3460, -17.1816])\n",
      "Epoch 2917, Loss 2.928976\n",
      "grad:tensor([-0.0037,  0.0209])\n",
      "params:tensor([  5.3460, -17.1818])\n",
      "Epoch 2918, Loss 2.928971\n",
      "grad:tensor([-0.0037,  0.0209])\n",
      "params:tensor([  5.3460, -17.1820])\n",
      "Epoch 2919, Loss 2.928967\n",
      "grad:tensor([-0.0037,  0.0209])\n",
      "params:tensor([  5.3461, -17.1822])\n",
      "Epoch 2920, Loss 2.928962\n",
      "grad:tensor([-0.0037,  0.0208])\n",
      "params:tensor([  5.3461, -17.1824])\n",
      "Epoch 2921, Loss 2.928958\n",
      "grad:tensor([-0.0037,  0.0208])\n",
      "params:tensor([  5.3461, -17.1826])\n",
      "Epoch 2922, Loss 2.928953\n",
      "grad:tensor([-0.0037,  0.0208])\n",
      "params:tensor([  5.3462, -17.1828])\n",
      "Epoch 2923, Loss 2.928947\n",
      "grad:tensor([-0.0036,  0.0207])\n",
      "params:tensor([  5.3462, -17.1830])\n",
      "Epoch 2924, Loss 2.928943\n",
      "grad:tensor([-0.0037,  0.0207])\n",
      "params:tensor([  5.3462, -17.1832])\n",
      "Epoch 2925, Loss 2.928940\n",
      "grad:tensor([-0.0036,  0.0206])\n",
      "params:tensor([  5.3463, -17.1834])\n",
      "Epoch 2926, Loss 2.928935\n",
      "grad:tensor([-0.0036,  0.0206])\n",
      "params:tensor([  5.3463, -17.1837])\n",
      "Epoch 2927, Loss 2.928932\n",
      "grad:tensor([-0.0036,  0.0206])\n",
      "params:tensor([  5.3464, -17.1839])\n",
      "Epoch 2928, Loss 2.928926\n",
      "grad:tensor([-0.0036,  0.0205])\n",
      "params:tensor([  5.3464, -17.1841])\n",
      "Epoch 2929, Loss 2.928923\n",
      "grad:tensor([-0.0036,  0.0205])\n",
      "params:tensor([  5.3464, -17.1843])\n",
      "Epoch 2930, Loss 2.928919\n",
      "grad:tensor([-0.0036,  0.0205])\n",
      "params:tensor([  5.3465, -17.1845])\n",
      "Epoch 2931, Loss 2.928913\n",
      "grad:tensor([-0.0036,  0.0204])\n",
      "params:tensor([  5.3465, -17.1847])\n",
      "Epoch 2932, Loss 2.928909\n",
      "grad:tensor([-0.0036,  0.0204])\n",
      "params:tensor([  5.3465, -17.1849])\n",
      "Epoch 2933, Loss 2.928904\n",
      "grad:tensor([-0.0036,  0.0204])\n",
      "params:tensor([  5.3466, -17.1851])\n",
      "Epoch 2934, Loss 2.928902\n",
      "grad:tensor([-0.0036,  0.0203])\n",
      "params:tensor([  5.3466, -17.1853])\n",
      "Epoch 2935, Loss 2.928897\n",
      "grad:tensor([-0.0036,  0.0203])\n",
      "params:tensor([  5.3466, -17.1855])\n",
      "Epoch 2936, Loss 2.928893\n",
      "grad:tensor([-0.0036,  0.0203])\n",
      "params:tensor([  5.3467, -17.1857])\n",
      "Epoch 2937, Loss 2.928887\n",
      "grad:tensor([-0.0036,  0.0202])\n",
      "params:tensor([  5.3467, -17.1859])\n",
      "Epoch 2938, Loss 2.928883\n",
      "grad:tensor([-0.0035,  0.0202])\n",
      "params:tensor([  5.3468, -17.1861])\n",
      "Epoch 2939, Loss 2.928880\n",
      "grad:tensor([-0.0036,  0.0202])\n",
      "params:tensor([  5.3468, -17.1863])\n",
      "Epoch 2940, Loss 2.928878\n",
      "grad:tensor([-0.0036,  0.0201])\n",
      "params:tensor([  5.3468, -17.1865])\n",
      "Epoch 2941, Loss 2.928871\n",
      "grad:tensor([-0.0035,  0.0201])\n",
      "params:tensor([  5.3469, -17.1867])\n",
      "Epoch 2942, Loss 2.928867\n",
      "grad:tensor([-0.0035,  0.0201])\n",
      "params:tensor([  5.3469, -17.1869])\n",
      "Epoch 2943, Loss 2.928864\n",
      "grad:tensor([-0.0035,  0.0200])\n",
      "params:tensor([  5.3469, -17.1871])\n",
      "Epoch 2944, Loss 2.928860\n",
      "grad:tensor([-0.0035,  0.0200])\n",
      "params:tensor([  5.3470, -17.1873])\n",
      "Epoch 2945, Loss 2.928855\n",
      "grad:tensor([-0.0035,  0.0200])\n",
      "params:tensor([  5.3470, -17.1875])\n",
      "Epoch 2946, Loss 2.928850\n",
      "grad:tensor([-0.0035,  0.0199])\n",
      "params:tensor([  5.3470, -17.1877])\n",
      "Epoch 2947, Loss 2.928845\n",
      "grad:tensor([-0.0035,  0.0199])\n",
      "params:tensor([  5.3471, -17.1879])\n",
      "Epoch 2948, Loss 2.928843\n",
      "grad:tensor([-0.0035,  0.0199])\n",
      "params:tensor([  5.3471, -17.1881])\n",
      "Epoch 2949, Loss 2.928838\n",
      "grad:tensor([-0.0035,  0.0198])\n",
      "params:tensor([  5.3471, -17.1883])\n",
      "Epoch 2950, Loss 2.928833\n",
      "grad:tensor([-0.0035,  0.0198])\n",
      "params:tensor([  5.3472, -17.1885])\n",
      "Epoch 2951, Loss 2.928830\n",
      "grad:tensor([-0.0035,  0.0198])\n",
      "params:tensor([  5.3472, -17.1887])\n",
      "Epoch 2952, Loss 2.928826\n",
      "grad:tensor([-0.0035,  0.0197])\n",
      "params:tensor([  5.3472, -17.1889])\n",
      "Epoch 2953, Loss 2.928823\n",
      "grad:tensor([-0.0035,  0.0197])\n",
      "params:tensor([  5.3473, -17.1891])\n",
      "Epoch 2954, Loss 2.928818\n",
      "grad:tensor([-0.0035,  0.0197])\n",
      "params:tensor([  5.3473, -17.1893])\n",
      "Epoch 2955, Loss 2.928816\n",
      "grad:tensor([-0.0035,  0.0196])\n",
      "params:tensor([  5.3474, -17.1895])\n",
      "Epoch 2956, Loss 2.928811\n",
      "grad:tensor([-0.0035,  0.0196])\n",
      "params:tensor([  5.3474, -17.1897])\n",
      "Epoch 2957, Loss 2.928805\n",
      "grad:tensor([-0.0034,  0.0196])\n",
      "params:tensor([  5.3474, -17.1899])\n",
      "Epoch 2958, Loss 2.928802\n",
      "grad:tensor([-0.0035,  0.0195])\n",
      "params:tensor([  5.3475, -17.1901])\n",
      "Epoch 2959, Loss 2.928799\n",
      "grad:tensor([-0.0034,  0.0195])\n",
      "params:tensor([  5.3475, -17.1903])\n",
      "Epoch 2960, Loss 2.928795\n",
      "grad:tensor([-0.0034,  0.0195])\n",
      "params:tensor([  5.3475, -17.1905])\n",
      "Epoch 2961, Loss 2.928789\n",
      "grad:tensor([-0.0034,  0.0194])\n",
      "params:tensor([  5.3476, -17.1907])\n",
      "Epoch 2962, Loss 2.928789\n",
      "grad:tensor([-0.0034,  0.0194])\n",
      "params:tensor([  5.3476, -17.1908])\n",
      "Epoch 2963, Loss 2.928783\n",
      "grad:tensor([-0.0034,  0.0194])\n",
      "params:tensor([  5.3476, -17.1910])\n",
      "Epoch 2964, Loss 2.928779\n",
      "grad:tensor([-0.0034,  0.0193])\n",
      "params:tensor([  5.3477, -17.1912])\n",
      "Epoch 2965, Loss 2.928775\n",
      "grad:tensor([-0.0034,  0.0193])\n",
      "params:tensor([  5.3477, -17.1914])\n",
      "Epoch 2966, Loss 2.928771\n",
      "grad:tensor([-0.0034,  0.0193])\n",
      "params:tensor([  5.3477, -17.1916])\n",
      "Epoch 2967, Loss 2.928767\n",
      "grad:tensor([-0.0034,  0.0192])\n",
      "params:tensor([  5.3478, -17.1918])\n",
      "Epoch 2968, Loss 2.928765\n",
      "grad:tensor([-0.0034,  0.0192])\n",
      "params:tensor([  5.3478, -17.1920])\n",
      "Epoch 2969, Loss 2.928761\n",
      "grad:tensor([-0.0034,  0.0192])\n",
      "params:tensor([  5.3478, -17.1922])\n",
      "Epoch 2970, Loss 2.928758\n",
      "grad:tensor([-0.0034,  0.0191])\n",
      "params:tensor([  5.3479, -17.1924])\n",
      "Epoch 2971, Loss 2.928752\n",
      "grad:tensor([-0.0034,  0.0191])\n",
      "params:tensor([  5.3479, -17.1926])\n",
      "Epoch 2972, Loss 2.928750\n",
      "grad:tensor([-0.0034,  0.0191])\n",
      "params:tensor([  5.3479, -17.1928])\n",
      "Epoch 2973, Loss 2.928745\n",
      "grad:tensor([-0.0034,  0.0190])\n",
      "params:tensor([  5.3480, -17.1930])\n",
      "Epoch 2974, Loss 2.928741\n",
      "grad:tensor([-0.0034,  0.0190])\n",
      "params:tensor([  5.3480, -17.1931])\n",
      "Epoch 2975, Loss 2.928737\n",
      "grad:tensor([-0.0034,  0.0190])\n",
      "params:tensor([  5.3480, -17.1933])\n",
      "Epoch 2976, Loss 2.928735\n",
      "grad:tensor([-0.0033,  0.0189])\n",
      "params:tensor([  5.3481, -17.1935])\n",
      "Epoch 2977, Loss 2.928730\n",
      "grad:tensor([-0.0033,  0.0189])\n",
      "params:tensor([  5.3481, -17.1937])\n",
      "Epoch 2978, Loss 2.928727\n",
      "grad:tensor([-0.0033,  0.0189])\n",
      "params:tensor([  5.3481, -17.1939])\n",
      "Epoch 2979, Loss 2.928723\n",
      "grad:tensor([-0.0033,  0.0188])\n",
      "params:tensor([  5.3482, -17.1941])\n",
      "Epoch 2980, Loss 2.928719\n",
      "grad:tensor([-0.0033,  0.0188])\n",
      "params:tensor([  5.3482, -17.1943])\n",
      "Epoch 2981, Loss 2.928716\n",
      "grad:tensor([-0.0033,  0.0188])\n",
      "params:tensor([  5.3482, -17.1945])\n",
      "Epoch 2982, Loss 2.928712\n",
      "grad:tensor([-0.0033,  0.0187])\n",
      "params:tensor([  5.3483, -17.1947])\n",
      "Epoch 2983, Loss 2.928708\n",
      "grad:tensor([-0.0033,  0.0187])\n",
      "params:tensor([  5.3483, -17.1948])\n",
      "Epoch 2984, Loss 2.928705\n",
      "grad:tensor([-0.0033,  0.0187])\n",
      "params:tensor([  5.3483, -17.1950])\n",
      "Epoch 2985, Loss 2.928700\n",
      "grad:tensor([-0.0033,  0.0186])\n",
      "params:tensor([  5.3484, -17.1952])\n",
      "Epoch 2986, Loss 2.928698\n",
      "grad:tensor([-0.0033,  0.0186])\n",
      "params:tensor([  5.3484, -17.1954])\n",
      "Epoch 2987, Loss 2.928695\n",
      "grad:tensor([-0.0033,  0.0186])\n",
      "params:tensor([  5.3484, -17.1956])\n",
      "Epoch 2988, Loss 2.928690\n",
      "grad:tensor([-0.0033,  0.0186])\n",
      "params:tensor([  5.3485, -17.1958])\n",
      "Epoch 2989, Loss 2.928687\n",
      "grad:tensor([-0.0033,  0.0185])\n",
      "params:tensor([  5.3485, -17.1960])\n",
      "Epoch 2990, Loss 2.928684\n",
      "grad:tensor([-0.0033,  0.0185])\n",
      "params:tensor([  5.3485, -17.1961])\n",
      "Epoch 2991, Loss 2.928679\n",
      "grad:tensor([-0.0032,  0.0185])\n",
      "params:tensor([  5.3486, -17.1963])\n",
      "Epoch 2992, Loss 2.928677\n",
      "grad:tensor([-0.0033,  0.0184])\n",
      "params:tensor([  5.3486, -17.1965])\n",
      "Epoch 2993, Loss 2.928673\n",
      "grad:tensor([-0.0033,  0.0184])\n",
      "params:tensor([  5.3486, -17.1967])\n",
      "Epoch 2994, Loss 2.928669\n",
      "grad:tensor([-0.0033,  0.0184])\n",
      "params:tensor([  5.3487, -17.1969])\n",
      "Epoch 2995, Loss 2.928666\n",
      "grad:tensor([-0.0032,  0.0183])\n",
      "params:tensor([  5.3487, -17.1971])\n",
      "Epoch 2996, Loss 2.928662\n",
      "grad:tensor([-0.0032,  0.0183])\n",
      "params:tensor([  5.3487, -17.1972])\n",
      "Epoch 2997, Loss 2.928660\n",
      "grad:tensor([-0.0032,  0.0183])\n",
      "params:tensor([  5.3488, -17.1974])\n",
      "Epoch 2998, Loss 2.928656\n",
      "grad:tensor([-0.0032,  0.0182])\n",
      "params:tensor([  5.3488, -17.1976])\n",
      "Epoch 2999, Loss 2.928651\n",
      "grad:tensor([-0.0032,  0.0182])\n",
      "params:tensor([  5.3488, -17.1978])\n",
      "Epoch 3000, Loss 2.928648\n",
      "grad:tensor([-0.0032,  0.0182])\n",
      "params:tensor([  5.3489, -17.1980])\n",
      "Epoch 3001, Loss 2.928646\n",
      "grad:tensor([-0.0032,  0.0181])\n",
      "params:tensor([  5.3489, -17.1982])\n",
      "Epoch 3002, Loss 2.928643\n",
      "grad:tensor([-0.0032,  0.0181])\n",
      "params:tensor([  5.3489, -17.1983])\n",
      "Epoch 3003, Loss 2.928638\n",
      "grad:tensor([-0.0032,  0.0181])\n",
      "params:tensor([  5.3489, -17.1985])\n",
      "Epoch 3004, Loss 2.928635\n",
      "grad:tensor([-0.0032,  0.0181])\n",
      "params:tensor([  5.3490, -17.1987])\n",
      "Epoch 3005, Loss 2.928632\n",
      "grad:tensor([-0.0032,  0.0180])\n",
      "params:tensor([  5.3490, -17.1989])\n",
      "Epoch 3006, Loss 2.928629\n",
      "grad:tensor([-0.0032,  0.0180])\n",
      "params:tensor([  5.3490, -17.1991])\n",
      "Epoch 3007, Loss 2.928625\n",
      "grad:tensor([-0.0032,  0.0180])\n",
      "params:tensor([  5.3491, -17.1992])\n",
      "Epoch 3008, Loss 2.928621\n",
      "grad:tensor([-0.0032,  0.0179])\n",
      "params:tensor([  5.3491, -17.1994])\n",
      "Epoch 3009, Loss 2.928617\n",
      "grad:tensor([-0.0032,  0.0179])\n",
      "params:tensor([  5.3491, -17.1996])\n",
      "Epoch 3010, Loss 2.928616\n",
      "grad:tensor([-0.0032,  0.0179])\n",
      "params:tensor([  5.3492, -17.1998])\n",
      "Epoch 3011, Loss 2.928612\n",
      "grad:tensor([-0.0032,  0.0178])\n",
      "params:tensor([  5.3492, -17.2000])\n",
      "Epoch 3012, Loss 2.928608\n",
      "grad:tensor([-0.0032,  0.0178])\n",
      "params:tensor([  5.3492, -17.2001])\n",
      "Epoch 3013, Loss 2.928604\n",
      "grad:tensor([-0.0031,  0.0178])\n",
      "params:tensor([  5.3493, -17.2003])\n",
      "Epoch 3014, Loss 2.928601\n",
      "grad:tensor([-0.0031,  0.0177])\n",
      "params:tensor([  5.3493, -17.2005])\n",
      "Epoch 3015, Loss 2.928599\n",
      "grad:tensor([-0.0031,  0.0177])\n",
      "params:tensor([  5.3493, -17.2007])\n",
      "Epoch 3016, Loss 2.928596\n",
      "grad:tensor([-0.0031,  0.0177])\n",
      "params:tensor([  5.3494, -17.2008])\n",
      "Epoch 3017, Loss 2.928592\n",
      "grad:tensor([-0.0031,  0.0177])\n",
      "params:tensor([  5.3494, -17.2010])\n",
      "Epoch 3018, Loss 2.928588\n",
      "grad:tensor([-0.0031,  0.0176])\n",
      "params:tensor([  5.3494, -17.2012])\n",
      "Epoch 3019, Loss 2.928586\n",
      "grad:tensor([-0.0031,  0.0176])\n",
      "params:tensor([  5.3495, -17.2014])\n",
      "Epoch 3020, Loss 2.928583\n",
      "grad:tensor([-0.0031,  0.0176])\n",
      "params:tensor([  5.3495, -17.2015])\n",
      "Epoch 3021, Loss 2.928580\n",
      "grad:tensor([-0.0031,  0.0175])\n",
      "params:tensor([  5.3495, -17.2017])\n",
      "Epoch 3022, Loss 2.928576\n",
      "grad:tensor([-0.0031,  0.0175])\n",
      "params:tensor([  5.3495, -17.2019])\n",
      "Epoch 3023, Loss 2.928574\n",
      "grad:tensor([-0.0031,  0.0175])\n",
      "params:tensor([  5.3496, -17.2021])\n",
      "Epoch 3024, Loss 2.928570\n",
      "grad:tensor([-0.0031,  0.0175])\n",
      "params:tensor([  5.3496, -17.2022])\n",
      "Epoch 3025, Loss 2.928567\n",
      "grad:tensor([-0.0031,  0.0174])\n",
      "params:tensor([  5.3496, -17.2024])\n",
      "Epoch 3026, Loss 2.928564\n",
      "grad:tensor([-0.0031,  0.0174])\n",
      "params:tensor([  5.3497, -17.2026])\n",
      "Epoch 3027, Loss 2.928561\n",
      "grad:tensor([-0.0030,  0.0174])\n",
      "params:tensor([  5.3497, -17.2028])\n",
      "Epoch 3028, Loss 2.928557\n",
      "grad:tensor([-0.0031,  0.0173])\n",
      "params:tensor([  5.3497, -17.2029])\n",
      "Epoch 3029, Loss 2.928555\n",
      "grad:tensor([-0.0031,  0.0173])\n",
      "params:tensor([  5.3498, -17.2031])\n",
      "Epoch 3030, Loss 2.928551\n",
      "grad:tensor([-0.0031,  0.0173])\n",
      "params:tensor([  5.3498, -17.2033])\n",
      "Epoch 3031, Loss 2.928548\n",
      "grad:tensor([-0.0031,  0.0172])\n",
      "params:tensor([  5.3498, -17.2035])\n",
      "Epoch 3032, Loss 2.928545\n",
      "grad:tensor([-0.0030,  0.0172])\n",
      "params:tensor([  5.3498, -17.2036])\n",
      "Epoch 3033, Loss 2.928543\n",
      "grad:tensor([-0.0030,  0.0172])\n",
      "params:tensor([  5.3499, -17.2038])\n",
      "Epoch 3034, Loss 2.928539\n",
      "grad:tensor([-0.0030,  0.0172])\n",
      "params:tensor([  5.3499, -17.2040])\n",
      "Epoch 3035, Loss 2.928536\n",
      "grad:tensor([-0.0030,  0.0171])\n",
      "params:tensor([  5.3499, -17.2041])\n",
      "Epoch 3036, Loss 2.928532\n",
      "grad:tensor([-0.0030,  0.0171])\n",
      "params:tensor([  5.3500, -17.2043])\n",
      "Epoch 3037, Loss 2.928531\n",
      "grad:tensor([-0.0030,  0.0171])\n",
      "params:tensor([  5.3500, -17.2045])\n",
      "Epoch 3038, Loss 2.928528\n",
      "grad:tensor([-0.0030,  0.0170])\n",
      "params:tensor([  5.3500, -17.2047])\n",
      "Epoch 3039, Loss 2.928524\n",
      "grad:tensor([-0.0030,  0.0170])\n",
      "params:tensor([  5.3501, -17.2048])\n",
      "Epoch 3040, Loss 2.928521\n",
      "grad:tensor([-0.0030,  0.0170])\n",
      "params:tensor([  5.3501, -17.2050])\n",
      "Epoch 3041, Loss 2.928519\n",
      "grad:tensor([-0.0030,  0.0170])\n",
      "params:tensor([  5.3501, -17.2052])\n",
      "Epoch 3042, Loss 2.928514\n",
      "grad:tensor([-0.0030,  0.0169])\n",
      "params:tensor([  5.3502, -17.2053])\n",
      "Epoch 3043, Loss 2.928512\n",
      "grad:tensor([-0.0030,  0.0169])\n",
      "params:tensor([  5.3502, -17.2055])\n",
      "Epoch 3044, Loss 2.928509\n",
      "grad:tensor([-0.0030,  0.0169])\n",
      "params:tensor([  5.3502, -17.2057])\n",
      "Epoch 3045, Loss 2.928505\n",
      "grad:tensor([-0.0030,  0.0168])\n",
      "params:tensor([  5.3502, -17.2058])\n",
      "Epoch 3046, Loss 2.928503\n",
      "grad:tensor([-0.0030,  0.0168])\n",
      "params:tensor([  5.3503, -17.2060])\n",
      "Epoch 3047, Loss 2.928500\n",
      "grad:tensor([-0.0030,  0.0168])\n",
      "params:tensor([  5.3503, -17.2062])\n",
      "Epoch 3048, Loss 2.928498\n",
      "grad:tensor([-0.0030,  0.0168])\n",
      "params:tensor([  5.3503, -17.2063])\n",
      "Epoch 3049, Loss 2.928495\n",
      "grad:tensor([-0.0030,  0.0167])\n",
      "params:tensor([  5.3504, -17.2065])\n",
      "Epoch 3050, Loss 2.928491\n",
      "grad:tensor([-0.0030,  0.0167])\n",
      "params:tensor([  5.3504, -17.2067])\n",
      "Epoch 3051, Loss 2.928489\n",
      "grad:tensor([-0.0030,  0.0167])\n",
      "params:tensor([  5.3504, -17.2068])\n",
      "Epoch 3052, Loss 2.928486\n",
      "grad:tensor([-0.0029,  0.0166])\n",
      "params:tensor([  5.3504, -17.2070])\n",
      "Epoch 3053, Loss 2.928484\n",
      "grad:tensor([-0.0029,  0.0166])\n",
      "params:tensor([  5.3505, -17.2072])\n",
      "Epoch 3054, Loss 2.928481\n",
      "grad:tensor([-0.0029,  0.0166])\n",
      "params:tensor([  5.3505, -17.2073])\n",
      "Epoch 3055, Loss 2.928477\n",
      "grad:tensor([-0.0029,  0.0165])\n",
      "params:tensor([  5.3505, -17.2075])\n",
      "Epoch 3056, Loss 2.928474\n",
      "grad:tensor([-0.0029,  0.0165])\n",
      "params:tensor([  5.3506, -17.2077])\n",
      "Epoch 3057, Loss 2.928472\n",
      "grad:tensor([-0.0029,  0.0165])\n",
      "params:tensor([  5.3506, -17.2078])\n",
      "Epoch 3058, Loss 2.928469\n",
      "grad:tensor([-0.0029,  0.0165])\n",
      "params:tensor([  5.3506, -17.2080])\n",
      "Epoch 3059, Loss 2.928468\n",
      "grad:tensor([-0.0029,  0.0164])\n",
      "params:tensor([  5.3507, -17.2082])\n",
      "Epoch 3060, Loss 2.928463\n",
      "grad:tensor([-0.0029,  0.0164])\n",
      "params:tensor([  5.3507, -17.2083])\n",
      "Epoch 3061, Loss 2.928460\n",
      "grad:tensor([-0.0029,  0.0164])\n",
      "params:tensor([  5.3507, -17.2085])\n",
      "Epoch 3062, Loss 2.928458\n",
      "grad:tensor([-0.0029,  0.0164])\n",
      "params:tensor([  5.3507, -17.2087])\n",
      "Epoch 3063, Loss 2.928456\n",
      "grad:tensor([-0.0029,  0.0163])\n",
      "params:tensor([  5.3508, -17.2088])\n",
      "Epoch 3064, Loss 2.928452\n",
      "grad:tensor([-0.0029,  0.0163])\n",
      "params:tensor([  5.3508, -17.2090])\n",
      "Epoch 3065, Loss 2.928449\n",
      "grad:tensor([-0.0029,  0.0163])\n",
      "params:tensor([  5.3508, -17.2091])\n",
      "Epoch 3066, Loss 2.928447\n",
      "grad:tensor([-0.0029,  0.0162])\n",
      "params:tensor([  5.3509, -17.2093])\n",
      "Epoch 3067, Loss 2.928443\n",
      "grad:tensor([-0.0029,  0.0162])\n",
      "params:tensor([  5.3509, -17.2095])\n",
      "Epoch 3068, Loss 2.928444\n",
      "grad:tensor([-0.0029,  0.0162])\n",
      "params:tensor([  5.3509, -17.2096])\n",
      "Epoch 3069, Loss 2.928440\n",
      "grad:tensor([-0.0029,  0.0162])\n",
      "params:tensor([  5.3509, -17.2098])\n",
      "Epoch 3070, Loss 2.928435\n",
      "grad:tensor([-0.0029,  0.0161])\n",
      "params:tensor([  5.3510, -17.2100])\n",
      "Epoch 3071, Loss 2.928435\n",
      "grad:tensor([-0.0029,  0.0161])\n",
      "params:tensor([  5.3510, -17.2101])\n",
      "Epoch 3072, Loss 2.928430\n",
      "grad:tensor([-0.0028,  0.0161])\n",
      "params:tensor([  5.3510, -17.2103])\n",
      "Epoch 3073, Loss 2.928428\n",
      "grad:tensor([-0.0028,  0.0161])\n",
      "params:tensor([  5.3511, -17.2104])\n",
      "Epoch 3074, Loss 2.928426\n",
      "grad:tensor([-0.0028,  0.0160])\n",
      "params:tensor([  5.3511, -17.2106])\n",
      "Epoch 3075, Loss 2.928423\n",
      "grad:tensor([-0.0028,  0.0160])\n",
      "params:tensor([  5.3511, -17.2108])\n",
      "Epoch 3076, Loss 2.928421\n",
      "grad:tensor([-0.0028,  0.0160])\n",
      "params:tensor([  5.3511, -17.2109])\n",
      "Epoch 3077, Loss 2.928417\n",
      "grad:tensor([-0.0028,  0.0159])\n",
      "params:tensor([  5.3512, -17.2111])\n",
      "Epoch 3078, Loss 2.928416\n",
      "grad:tensor([-0.0028,  0.0159])\n",
      "params:tensor([  5.3512, -17.2112])\n",
      "Epoch 3079, Loss 2.928411\n",
      "grad:tensor([-0.0028,  0.0159])\n",
      "params:tensor([  5.3512, -17.2114])\n",
      "Epoch 3080, Loss 2.928410\n",
      "grad:tensor([-0.0028,  0.0159])\n",
      "params:tensor([  5.3512, -17.2116])\n",
      "Epoch 3081, Loss 2.928407\n",
      "grad:tensor([-0.0028,  0.0158])\n",
      "params:tensor([  5.3513, -17.2117])\n",
      "Epoch 3082, Loss 2.928404\n",
      "grad:tensor([-0.0028,  0.0158])\n",
      "params:tensor([  5.3513, -17.2119])\n",
      "Epoch 3083, Loss 2.928402\n",
      "grad:tensor([-0.0028,  0.0158])\n",
      "params:tensor([  5.3513, -17.2120])\n",
      "Epoch 3084, Loss 2.928399\n",
      "grad:tensor([-0.0028,  0.0158])\n",
      "params:tensor([  5.3514, -17.2122])\n",
      "Epoch 3085, Loss 2.928396\n",
      "grad:tensor([-0.0028,  0.0157])\n",
      "params:tensor([  5.3514, -17.2123])\n",
      "Epoch 3086, Loss 2.928395\n",
      "grad:tensor([-0.0028,  0.0157])\n",
      "params:tensor([  5.3514, -17.2125])\n",
      "Epoch 3087, Loss 2.928392\n",
      "grad:tensor([-0.0027,  0.0157])\n",
      "params:tensor([  5.3514, -17.2127])\n",
      "Epoch 3088, Loss 2.928389\n",
      "grad:tensor([-0.0027,  0.0157])\n",
      "params:tensor([  5.3515, -17.2128])\n",
      "Epoch 3089, Loss 2.928386\n",
      "grad:tensor([-0.0027,  0.0156])\n",
      "params:tensor([  5.3515, -17.2130])\n",
      "Epoch 3090, Loss 2.928383\n",
      "grad:tensor([-0.0028,  0.0156])\n",
      "params:tensor([  5.3515, -17.2131])\n",
      "Epoch 3091, Loss 2.928382\n",
      "grad:tensor([-0.0028,  0.0156])\n",
      "params:tensor([  5.3516, -17.2133])\n",
      "Epoch 3092, Loss 2.928379\n",
      "grad:tensor([-0.0027,  0.0155])\n",
      "params:tensor([  5.3516, -17.2134])\n",
      "Epoch 3093, Loss 2.928378\n",
      "grad:tensor([-0.0027,  0.0155])\n",
      "params:tensor([  5.3516, -17.2136])\n",
      "Epoch 3094, Loss 2.928375\n",
      "grad:tensor([-0.0027,  0.0155])\n",
      "params:tensor([  5.3516, -17.2137])\n",
      "Epoch 3095, Loss 2.928372\n",
      "grad:tensor([-0.0027,  0.0155])\n",
      "params:tensor([  5.3517, -17.2139])\n",
      "Epoch 3096, Loss 2.928370\n",
      "grad:tensor([-0.0027,  0.0154])\n",
      "params:tensor([  5.3517, -17.2141])\n",
      "Epoch 3097, Loss 2.928368\n",
      "grad:tensor([-0.0027,  0.0154])\n",
      "params:tensor([  5.3517, -17.2142])\n",
      "Epoch 3098, Loss 2.928364\n",
      "grad:tensor([-0.0027,  0.0154])\n",
      "params:tensor([  5.3517, -17.2144])\n",
      "Epoch 3099, Loss 2.928362\n",
      "grad:tensor([-0.0027,  0.0154])\n",
      "params:tensor([  5.3518, -17.2145])\n",
      "Epoch 3100, Loss 2.928361\n",
      "grad:tensor([-0.0027,  0.0153])\n",
      "params:tensor([  5.3518, -17.2147])\n",
      "Epoch 3101, Loss 2.928356\n",
      "grad:tensor([-0.0027,  0.0153])\n",
      "params:tensor([  5.3518, -17.2148])\n",
      "Epoch 3102, Loss 2.928355\n",
      "grad:tensor([-0.0027,  0.0153])\n",
      "params:tensor([  5.3519, -17.2150])\n",
      "Epoch 3103, Loss 2.928353\n",
      "grad:tensor([-0.0027,  0.0153])\n",
      "params:tensor([  5.3519, -17.2151])\n",
      "Epoch 3104, Loss 2.928349\n",
      "grad:tensor([-0.0027,  0.0152])\n",
      "params:tensor([  5.3519, -17.2153])\n",
      "Epoch 3105, Loss 2.928348\n",
      "grad:tensor([-0.0027,  0.0152])\n",
      "params:tensor([  5.3519, -17.2154])\n",
      "Epoch 3106, Loss 2.928345\n",
      "grad:tensor([-0.0027,  0.0152])\n",
      "params:tensor([  5.3520, -17.2156])\n",
      "Epoch 3107, Loss 2.928343\n",
      "grad:tensor([-0.0027,  0.0152])\n",
      "params:tensor([  5.3520, -17.2157])\n",
      "Epoch 3108, Loss 2.928340\n",
      "grad:tensor([-0.0027,  0.0151])\n",
      "params:tensor([  5.3520, -17.2159])\n",
      "Epoch 3109, Loss 2.928339\n",
      "grad:tensor([-0.0027,  0.0151])\n",
      "params:tensor([  5.3520, -17.2160])\n",
      "Epoch 3110, Loss 2.928337\n",
      "grad:tensor([-0.0027,  0.0151])\n",
      "params:tensor([  5.3521, -17.2162])\n",
      "Epoch 3111, Loss 2.928333\n",
      "grad:tensor([-0.0027,  0.0151])\n",
      "params:tensor([  5.3521, -17.2163])\n",
      "Epoch 3112, Loss 2.928332\n",
      "grad:tensor([-0.0027,  0.0150])\n",
      "params:tensor([  5.3521, -17.2165])\n",
      "Epoch 3113, Loss 2.928328\n",
      "grad:tensor([-0.0026,  0.0150])\n",
      "params:tensor([  5.3521, -17.2166])\n",
      "Epoch 3114, Loss 2.928329\n",
      "grad:tensor([-0.0027,  0.0150])\n",
      "params:tensor([  5.3522, -17.2168])\n",
      "Epoch 3115, Loss 2.928324\n",
      "grad:tensor([-0.0026,  0.0149])\n",
      "params:tensor([  5.3522, -17.2169])\n",
      "Epoch 3116, Loss 2.928323\n",
      "grad:tensor([-0.0026,  0.0149])\n",
      "params:tensor([  5.3522, -17.2171])\n",
      "Epoch 3117, Loss 2.928320\n",
      "grad:tensor([-0.0026,  0.0149])\n",
      "params:tensor([  5.3523, -17.2172])\n",
      "Epoch 3118, Loss 2.928319\n",
      "grad:tensor([-0.0026,  0.0149])\n",
      "params:tensor([  5.3523, -17.2174])\n",
      "Epoch 3119, Loss 2.928315\n",
      "grad:tensor([-0.0026,  0.0148])\n",
      "params:tensor([  5.3523, -17.2175])\n",
      "Epoch 3120, Loss 2.928313\n",
      "grad:tensor([-0.0026,  0.0148])\n",
      "params:tensor([  5.3523, -17.2177])\n",
      "Epoch 3121, Loss 2.928310\n",
      "grad:tensor([-0.0026,  0.0148])\n",
      "params:tensor([  5.3524, -17.2178])\n",
      "Epoch 3122, Loss 2.928308\n",
      "grad:tensor([-0.0026,  0.0148])\n",
      "params:tensor([  5.3524, -17.2180])\n",
      "Epoch 3123, Loss 2.928306\n",
      "grad:tensor([-0.0026,  0.0147])\n",
      "params:tensor([  5.3524, -17.2181])\n",
      "Epoch 3124, Loss 2.928304\n",
      "grad:tensor([-0.0026,  0.0147])\n",
      "params:tensor([  5.3524, -17.2183])\n",
      "Epoch 3125, Loss 2.928303\n",
      "grad:tensor([-0.0026,  0.0147])\n",
      "params:tensor([  5.3525, -17.2184])\n",
      "Epoch 3126, Loss 2.928299\n",
      "grad:tensor([-0.0026,  0.0147])\n",
      "params:tensor([  5.3525, -17.2186])\n",
      "Epoch 3127, Loss 2.928296\n",
      "grad:tensor([-0.0026,  0.0146])\n",
      "params:tensor([  5.3525, -17.2187])\n",
      "Epoch 3128, Loss 2.928295\n",
      "grad:tensor([-0.0026,  0.0146])\n",
      "params:tensor([  5.3525, -17.2189])\n",
      "Epoch 3129, Loss 2.928293\n",
      "grad:tensor([-0.0026,  0.0146])\n",
      "params:tensor([  5.3526, -17.2190])\n",
      "Epoch 3130, Loss 2.928291\n",
      "grad:tensor([-0.0026,  0.0146])\n",
      "params:tensor([  5.3526, -17.2192])\n",
      "Epoch 3131, Loss 2.928288\n",
      "grad:tensor([-0.0026,  0.0145])\n",
      "params:tensor([  5.3526, -17.2193])\n",
      "Epoch 3132, Loss 2.928287\n",
      "grad:tensor([-0.0026,  0.0145])\n",
      "params:tensor([  5.3526, -17.2194])\n",
      "Epoch 3133, Loss 2.928285\n",
      "grad:tensor([-0.0025,  0.0145])\n",
      "params:tensor([  5.3527, -17.2196])\n",
      "Epoch 3134, Loss 2.928282\n",
      "grad:tensor([-0.0026,  0.0145])\n",
      "params:tensor([  5.3527, -17.2197])\n",
      "Epoch 3135, Loss 2.928280\n",
      "grad:tensor([-0.0026,  0.0144])\n",
      "params:tensor([  5.3527, -17.2199])\n",
      "Epoch 3136, Loss 2.928276\n",
      "grad:tensor([-0.0025,  0.0144])\n",
      "params:tensor([  5.3527, -17.2200])\n",
      "Epoch 3137, Loss 2.928275\n",
      "grad:tensor([-0.0026,  0.0144])\n",
      "params:tensor([  5.3528, -17.2202])\n",
      "Epoch 3138, Loss 2.928273\n",
      "grad:tensor([-0.0025,  0.0144])\n",
      "params:tensor([  5.3528, -17.2203])\n",
      "Epoch 3139, Loss 2.928271\n",
      "grad:tensor([-0.0025,  0.0144])\n",
      "params:tensor([  5.3528, -17.2205])\n",
      "Epoch 3140, Loss 2.928268\n",
      "grad:tensor([-0.0025,  0.0143])\n",
      "params:tensor([  5.3528, -17.2206])\n",
      "Epoch 3141, Loss 2.928267\n",
      "grad:tensor([-0.0025,  0.0143])\n",
      "params:tensor([  5.3529, -17.2207])\n",
      "Epoch 3142, Loss 2.928264\n",
      "grad:tensor([-0.0025,  0.0143])\n",
      "params:tensor([  5.3529, -17.2209])\n",
      "Epoch 3143, Loss 2.928263\n",
      "grad:tensor([-0.0025,  0.0143])\n",
      "params:tensor([  5.3529, -17.2210])\n",
      "Epoch 3144, Loss 2.928260\n",
      "grad:tensor([-0.0025,  0.0142])\n",
      "params:tensor([  5.3529, -17.2212])\n",
      "Epoch 3145, Loss 2.928259\n",
      "grad:tensor([-0.0025,  0.0142])\n",
      "params:tensor([  5.3530, -17.2213])\n",
      "Epoch 3146, Loss 2.928256\n",
      "grad:tensor([-0.0025,  0.0142])\n",
      "params:tensor([  5.3530, -17.2214])\n",
      "Epoch 3147, Loss 2.928255\n",
      "grad:tensor([-0.0025,  0.0142])\n",
      "params:tensor([  5.3530, -17.2216])\n",
      "Epoch 3148, Loss 2.928252\n",
      "grad:tensor([-0.0025,  0.0141])\n",
      "params:tensor([  5.3530, -17.2217])\n",
      "Epoch 3149, Loss 2.928250\n",
      "grad:tensor([-0.0025,  0.0141])\n",
      "params:tensor([  5.3531, -17.2219])\n",
      "Epoch 3150, Loss 2.928249\n",
      "grad:tensor([-0.0025,  0.0141])\n",
      "params:tensor([  5.3531, -17.2220])\n",
      "Epoch 3151, Loss 2.928246\n",
      "grad:tensor([-0.0025,  0.0141])\n",
      "params:tensor([  5.3531, -17.2222])\n",
      "Epoch 3152, Loss 2.928245\n",
      "grad:tensor([-0.0025,  0.0140])\n",
      "params:tensor([  5.3531, -17.2223])\n",
      "Epoch 3153, Loss 2.928242\n",
      "grad:tensor([-0.0025,  0.0140])\n",
      "params:tensor([  5.3532, -17.2224])\n",
      "Epoch 3154, Loss 2.928239\n",
      "grad:tensor([-0.0025,  0.0140])\n",
      "params:tensor([  5.3532, -17.2226])\n",
      "Epoch 3155, Loss 2.928236\n",
      "grad:tensor([-0.0025,  0.0140])\n",
      "params:tensor([  5.3532, -17.2227])\n",
      "Epoch 3156, Loss 2.928236\n",
      "grad:tensor([-0.0024,  0.0139])\n",
      "params:tensor([  5.3532, -17.2229])\n",
      "Epoch 3157, Loss 2.928233\n",
      "grad:tensor([-0.0025,  0.0139])\n",
      "params:tensor([  5.3533, -17.2230])\n",
      "Epoch 3158, Loss 2.928231\n",
      "grad:tensor([-0.0024,  0.0139])\n",
      "params:tensor([  5.3533, -17.2231])\n",
      "Epoch 3159, Loss 2.928230\n",
      "grad:tensor([-0.0025,  0.0139])\n",
      "params:tensor([  5.3533, -17.2233])\n",
      "Epoch 3160, Loss 2.928227\n",
      "grad:tensor([-0.0024,  0.0138])\n",
      "params:tensor([  5.3533, -17.2234])\n",
      "Epoch 3161, Loss 2.928226\n",
      "grad:tensor([-0.0025,  0.0138])\n",
      "params:tensor([  5.3534, -17.2235])\n",
      "Epoch 3162, Loss 2.928225\n",
      "grad:tensor([-0.0024,  0.0138])\n",
      "params:tensor([  5.3534, -17.2237])\n",
      "Epoch 3163, Loss 2.928222\n",
      "grad:tensor([-0.0024,  0.0138])\n",
      "params:tensor([  5.3534, -17.2238])\n",
      "Epoch 3164, Loss 2.928219\n",
      "grad:tensor([-0.0024,  0.0138])\n",
      "params:tensor([  5.3534, -17.2240])\n",
      "Epoch 3165, Loss 2.928218\n",
      "grad:tensor([-0.0024,  0.0137])\n",
      "params:tensor([  5.3535, -17.2241])\n",
      "Epoch 3166, Loss 2.928216\n",
      "grad:tensor([-0.0024,  0.0137])\n",
      "params:tensor([  5.3535, -17.2242])\n",
      "Epoch 3167, Loss 2.928215\n",
      "grad:tensor([-0.0024,  0.0137])\n",
      "params:tensor([  5.3535, -17.2244])\n",
      "Epoch 3168, Loss 2.928212\n",
      "grad:tensor([-0.0024,  0.0137])\n",
      "params:tensor([  5.3535, -17.2245])\n",
      "Epoch 3169, Loss 2.928211\n",
      "grad:tensor([-0.0024,  0.0136])\n",
      "params:tensor([  5.3536, -17.2246])\n",
      "Epoch 3170, Loss 2.928209\n",
      "grad:tensor([-0.0024,  0.0136])\n",
      "params:tensor([  5.3536, -17.2248])\n",
      "Epoch 3171, Loss 2.928206\n",
      "grad:tensor([-0.0024,  0.0136])\n",
      "params:tensor([  5.3536, -17.2249])\n",
      "Epoch 3172, Loss 2.928205\n",
      "grad:tensor([-0.0024,  0.0136])\n",
      "params:tensor([  5.3536, -17.2250])\n",
      "Epoch 3173, Loss 2.928204\n",
      "grad:tensor([-0.0024,  0.0135])\n",
      "params:tensor([  5.3537, -17.2252])\n",
      "Epoch 3174, Loss 2.928202\n",
      "grad:tensor([-0.0024,  0.0135])\n",
      "params:tensor([  5.3537, -17.2253])\n",
      "Epoch 3175, Loss 2.928200\n",
      "grad:tensor([-0.0024,  0.0135])\n",
      "params:tensor([  5.3537, -17.2255])\n",
      "Epoch 3176, Loss 2.928196\n",
      "grad:tensor([-0.0024,  0.0135])\n",
      "params:tensor([  5.3537, -17.2256])\n",
      "Epoch 3177, Loss 2.928195\n",
      "grad:tensor([-0.0024,  0.0134])\n",
      "params:tensor([  5.3538, -17.2257])\n",
      "Epoch 3178, Loss 2.928195\n",
      "grad:tensor([-0.0024,  0.0134])\n",
      "params:tensor([  5.3538, -17.2259])\n",
      "Epoch 3179, Loss 2.928191\n",
      "grad:tensor([-0.0024,  0.0134])\n",
      "params:tensor([  5.3538, -17.2260])\n",
      "Epoch 3180, Loss 2.928190\n",
      "grad:tensor([-0.0024,  0.0134])\n",
      "params:tensor([  5.3538, -17.2261])\n",
      "Epoch 3181, Loss 2.928188\n",
      "grad:tensor([-0.0023,  0.0134])\n",
      "params:tensor([  5.3538, -17.2263])\n",
      "Epoch 3182, Loss 2.928186\n",
      "grad:tensor([-0.0023,  0.0133])\n",
      "params:tensor([  5.3539, -17.2264])\n",
      "Epoch 3183, Loss 2.928185\n",
      "grad:tensor([-0.0024,  0.0133])\n",
      "params:tensor([  5.3539, -17.2265])\n",
      "Epoch 3184, Loss 2.928184\n",
      "grad:tensor([-0.0023,  0.0133])\n",
      "params:tensor([  5.3539, -17.2267])\n",
      "Epoch 3185, Loss 2.928182\n",
      "grad:tensor([-0.0024,  0.0133])\n",
      "params:tensor([  5.3539, -17.2268])\n",
      "Epoch 3186, Loss 2.928180\n",
      "grad:tensor([-0.0024,  0.0132])\n",
      "params:tensor([  5.3540, -17.2269])\n",
      "Epoch 3187, Loss 2.928178\n",
      "grad:tensor([-0.0023,  0.0132])\n",
      "params:tensor([  5.3540, -17.2271])\n",
      "Epoch 3188, Loss 2.928175\n",
      "grad:tensor([-0.0023,  0.0132])\n",
      "params:tensor([  5.3540, -17.2272])\n",
      "Epoch 3189, Loss 2.928172\n",
      "grad:tensor([-0.0023,  0.0132])\n",
      "params:tensor([  5.3540, -17.2273])\n",
      "Epoch 3190, Loss 2.928171\n",
      "grad:tensor([-0.0023,  0.0132])\n",
      "params:tensor([  5.3541, -17.2275])\n",
      "Epoch 3191, Loss 2.928170\n",
      "grad:tensor([-0.0023,  0.0131])\n",
      "params:tensor([  5.3541, -17.2276])\n",
      "Epoch 3192, Loss 2.928169\n",
      "grad:tensor([-0.0023,  0.0131])\n",
      "params:tensor([  5.3541, -17.2277])\n",
      "Epoch 3193, Loss 2.928167\n",
      "grad:tensor([-0.0023,  0.0131])\n",
      "params:tensor([  5.3541, -17.2278])\n",
      "Epoch 3194, Loss 2.928164\n",
      "grad:tensor([-0.0023,  0.0131])\n",
      "params:tensor([  5.3542, -17.2280])\n",
      "Epoch 3195, Loss 2.928163\n",
      "grad:tensor([-0.0023,  0.0130])\n",
      "params:tensor([  5.3542, -17.2281])\n",
      "Epoch 3196, Loss 2.928162\n",
      "grad:tensor([-0.0023,  0.0130])\n",
      "params:tensor([  5.3542, -17.2282])\n",
      "Epoch 3197, Loss 2.928160\n",
      "grad:tensor([-0.0023,  0.0130])\n",
      "params:tensor([  5.3542, -17.2284])\n",
      "Epoch 3198, Loss 2.928158\n",
      "grad:tensor([-0.0023,  0.0130])\n",
      "params:tensor([  5.3542, -17.2285])\n",
      "Epoch 3199, Loss 2.928157\n",
      "grad:tensor([-0.0023,  0.0130])\n",
      "params:tensor([  5.3543, -17.2286])\n",
      "Epoch 3200, Loss 2.928154\n",
      "grad:tensor([-0.0023,  0.0129])\n",
      "params:tensor([  5.3543, -17.2288])\n",
      "Epoch 3201, Loss 2.928152\n",
      "grad:tensor([-0.0023,  0.0129])\n",
      "params:tensor([  5.3543, -17.2289])\n",
      "Epoch 3202, Loss 2.928149\n",
      "grad:tensor([-0.0023,  0.0129])\n",
      "params:tensor([  5.3543, -17.2290])\n",
      "Epoch 3203, Loss 2.928150\n",
      "grad:tensor([-0.0023,  0.0129])\n",
      "params:tensor([  5.3544, -17.2291])\n",
      "Epoch 3204, Loss 2.928147\n",
      "grad:tensor([-0.0022,  0.0129])\n",
      "params:tensor([  5.3544, -17.2293])\n",
      "Epoch 3205, Loss 2.928146\n",
      "grad:tensor([-0.0023,  0.0128])\n",
      "params:tensor([  5.3544, -17.2294])\n",
      "Epoch 3206, Loss 2.928144\n",
      "grad:tensor([-0.0023,  0.0128])\n",
      "params:tensor([  5.3544, -17.2295])\n",
      "Epoch 3207, Loss 2.928142\n",
      "grad:tensor([-0.0023,  0.0128])\n",
      "params:tensor([  5.3544, -17.2297])\n",
      "Epoch 3208, Loss 2.928140\n",
      "grad:tensor([-0.0022,  0.0128])\n",
      "params:tensor([  5.3545, -17.2298])\n",
      "Epoch 3209, Loss 2.928138\n",
      "grad:tensor([-0.0022,  0.0127])\n",
      "params:tensor([  5.3545, -17.2299])\n",
      "Epoch 3210, Loss 2.928137\n",
      "grad:tensor([-0.0023,  0.0127])\n",
      "params:tensor([  5.3545, -17.2300])\n",
      "Epoch 3211, Loss 2.928135\n",
      "grad:tensor([-0.0023,  0.0127])\n",
      "params:tensor([  5.3545, -17.2302])\n",
      "Epoch 3212, Loss 2.928135\n",
      "grad:tensor([-0.0023,  0.0127])\n",
      "params:tensor([  5.3546, -17.2303])\n",
      "Epoch 3213, Loss 2.928133\n",
      "grad:tensor([-0.0022,  0.0127])\n",
      "params:tensor([  5.3546, -17.2304])\n",
      "Epoch 3214, Loss 2.928131\n",
      "grad:tensor([-0.0022,  0.0126])\n",
      "params:tensor([  5.3546, -17.2305])\n",
      "Epoch 3215, Loss 2.928130\n",
      "grad:tensor([-0.0022,  0.0126])\n",
      "params:tensor([  5.3546, -17.2307])\n",
      "Epoch 3216, Loss 2.928126\n",
      "grad:tensor([-0.0022,  0.0126])\n",
      "params:tensor([  5.3546, -17.2308])\n",
      "Epoch 3217, Loss 2.928125\n",
      "grad:tensor([-0.0022,  0.0126])\n",
      "params:tensor([  5.3547, -17.2309])\n",
      "Epoch 3218, Loss 2.928124\n",
      "grad:tensor([-0.0022,  0.0125])\n",
      "params:tensor([  5.3547, -17.2310])\n",
      "Epoch 3219, Loss 2.928121\n",
      "grad:tensor([-0.0022,  0.0125])\n",
      "params:tensor([  5.3547, -17.2312])\n",
      "Epoch 3220, Loss 2.928121\n",
      "grad:tensor([-0.0022,  0.0125])\n",
      "params:tensor([  5.3547, -17.2313])\n",
      "Epoch 3221, Loss 2.928120\n",
      "grad:tensor([-0.0022,  0.0125])\n",
      "params:tensor([  5.3548, -17.2314])\n",
      "Epoch 3222, Loss 2.928118\n",
      "grad:tensor([-0.0022,  0.0125])\n",
      "params:tensor([  5.3548, -17.2315])\n",
      "Epoch 3223, Loss 2.928117\n",
      "grad:tensor([-0.0022,  0.0124])\n",
      "params:tensor([  5.3548, -17.2317])\n",
      "Epoch 3224, Loss 2.928115\n",
      "grad:tensor([-0.0022,  0.0124])\n",
      "params:tensor([  5.3548, -17.2318])\n",
      "Epoch 3225, Loss 2.928113\n",
      "grad:tensor([-0.0022,  0.0124])\n",
      "params:tensor([  5.3548, -17.2319])\n",
      "Epoch 3226, Loss 2.928110\n",
      "grad:tensor([-0.0022,  0.0124])\n",
      "params:tensor([  5.3549, -17.2320])\n",
      "Epoch 3227, Loss 2.928109\n",
      "grad:tensor([-0.0022,  0.0124])\n",
      "params:tensor([  5.3549, -17.2322])\n",
      "Epoch 3228, Loss 2.928108\n",
      "grad:tensor([-0.0022,  0.0123])\n",
      "params:tensor([  5.3549, -17.2323])\n",
      "Epoch 3229, Loss 2.928105\n",
      "grad:tensor([-0.0022,  0.0123])\n",
      "params:tensor([  5.3549, -17.2324])\n",
      "Epoch 3230, Loss 2.928105\n",
      "grad:tensor([-0.0022,  0.0123])\n",
      "params:tensor([  5.3550, -17.2325])\n",
      "Epoch 3231, Loss 2.928104\n",
      "grad:tensor([-0.0022,  0.0123])\n",
      "params:tensor([  5.3550, -17.2327])\n",
      "Epoch 3232, Loss 2.928102\n",
      "grad:tensor([-0.0021,  0.0123])\n",
      "params:tensor([  5.3550, -17.2328])\n",
      "Epoch 3233, Loss 2.928101\n",
      "grad:tensor([-0.0022,  0.0122])\n",
      "params:tensor([  5.3550, -17.2329])\n",
      "Epoch 3234, Loss 2.928098\n",
      "grad:tensor([-0.0022,  0.0122])\n",
      "params:tensor([  5.3550, -17.2330])\n",
      "Epoch 3235, Loss 2.928097\n",
      "grad:tensor([-0.0022,  0.0122])\n",
      "params:tensor([  5.3551, -17.2331])\n",
      "Epoch 3236, Loss 2.928095\n",
      "grad:tensor([-0.0022,  0.0122])\n",
      "params:tensor([  5.3551, -17.2333])\n",
      "Epoch 3237, Loss 2.928094\n",
      "grad:tensor([-0.0022,  0.0121])\n",
      "params:tensor([  5.3551, -17.2334])\n",
      "Epoch 3238, Loss 2.928093\n",
      "grad:tensor([-0.0022,  0.0121])\n",
      "params:tensor([  5.3551, -17.2335])\n",
      "Epoch 3239, Loss 2.928091\n",
      "grad:tensor([-0.0022,  0.0121])\n",
      "params:tensor([  5.3551, -17.2336])\n",
      "Epoch 3240, Loss 2.928090\n",
      "grad:tensor([-0.0021,  0.0121])\n",
      "params:tensor([  5.3552, -17.2338])\n",
      "Epoch 3241, Loss 2.928088\n",
      "grad:tensor([-0.0021,  0.0121])\n",
      "params:tensor([  5.3552, -17.2339])\n",
      "Epoch 3242, Loss 2.928086\n",
      "grad:tensor([-0.0021,  0.0120])\n",
      "params:tensor([  5.3552, -17.2340])\n",
      "Epoch 3243, Loss 2.928085\n",
      "grad:tensor([-0.0021,  0.0120])\n",
      "params:tensor([  5.3552, -17.2341])\n",
      "Epoch 3244, Loss 2.928084\n",
      "grad:tensor([-0.0021,  0.0120])\n",
      "params:tensor([  5.3553, -17.2342])\n",
      "Epoch 3245, Loss 2.928082\n",
      "grad:tensor([-0.0021,  0.0120])\n",
      "params:tensor([  5.3553, -17.2344])\n",
      "Epoch 3246, Loss 2.928080\n",
      "grad:tensor([-0.0021,  0.0120])\n",
      "params:tensor([  5.3553, -17.2345])\n",
      "Epoch 3247, Loss 2.928079\n",
      "grad:tensor([-0.0021,  0.0119])\n",
      "params:tensor([  5.3553, -17.2346])\n",
      "Epoch 3248, Loss 2.928076\n",
      "grad:tensor([-0.0021,  0.0119])\n",
      "params:tensor([  5.3553, -17.2347])\n",
      "Epoch 3249, Loss 2.928077\n",
      "grad:tensor([-0.0021,  0.0119])\n",
      "params:tensor([  5.3554, -17.2348])\n",
      "Epoch 3250, Loss 2.928075\n",
      "grad:tensor([-0.0021,  0.0119])\n",
      "params:tensor([  5.3554, -17.2350])\n",
      "Epoch 3251, Loss 2.928072\n",
      "grad:tensor([-0.0021,  0.0119])\n",
      "params:tensor([  5.3554, -17.2351])\n",
      "Epoch 3252, Loss 2.928072\n",
      "grad:tensor([-0.0021,  0.0118])\n",
      "params:tensor([  5.3554, -17.2352])\n",
      "Epoch 3253, Loss 2.928071\n",
      "grad:tensor([-0.0021,  0.0118])\n",
      "params:tensor([  5.3554, -17.2353])\n",
      "Epoch 3254, Loss 2.928068\n",
      "grad:tensor([-0.0021,  0.0118])\n",
      "params:tensor([  5.3555, -17.2354])\n",
      "Epoch 3255, Loss 2.928069\n",
      "grad:tensor([-0.0021,  0.0118])\n",
      "params:tensor([  5.3555, -17.2355])\n",
      "Epoch 3256, Loss 2.928066\n",
      "grad:tensor([-0.0021,  0.0118])\n",
      "params:tensor([  5.3555, -17.2357])\n",
      "Epoch 3257, Loss 2.928065\n",
      "grad:tensor([-0.0021,  0.0117])\n",
      "params:tensor([  5.3555, -17.2358])\n",
      "Epoch 3258, Loss 2.928064\n",
      "grad:tensor([-0.0021,  0.0117])\n",
      "params:tensor([  5.3555, -17.2359])\n",
      "Epoch 3259, Loss 2.928061\n",
      "grad:tensor([-0.0021,  0.0117])\n",
      "params:tensor([  5.3556, -17.2360])\n",
      "Epoch 3260, Loss 2.928060\n",
      "grad:tensor([-0.0021,  0.0117])\n",
      "params:tensor([  5.3556, -17.2361])\n",
      "Epoch 3261, Loss 2.928057\n",
      "grad:tensor([-0.0021,  0.0117])\n",
      "params:tensor([  5.3556, -17.2362])\n",
      "Epoch 3262, Loss 2.928058\n",
      "grad:tensor([-0.0021,  0.0116])\n",
      "params:tensor([  5.3556, -17.2364])\n",
      "Epoch 3263, Loss 2.928056\n",
      "grad:tensor([-0.0021,  0.0116])\n",
      "params:tensor([  5.3557, -17.2365])\n",
      "Epoch 3264, Loss 2.928055\n",
      "grad:tensor([-0.0021,  0.0116])\n",
      "params:tensor([  5.3557, -17.2366])\n",
      "Epoch 3265, Loss 2.928052\n",
      "grad:tensor([-0.0021,  0.0116])\n",
      "params:tensor([  5.3557, -17.2367])\n",
      "Epoch 3266, Loss 2.928053\n",
      "grad:tensor([-0.0021,  0.0116])\n",
      "params:tensor([  5.3557, -17.2368])\n",
      "Epoch 3267, Loss 2.928051\n",
      "grad:tensor([-0.0021,  0.0115])\n",
      "params:tensor([  5.3557, -17.2369])\n",
      "Epoch 3268, Loss 2.928050\n",
      "grad:tensor([-0.0021,  0.0115])\n",
      "params:tensor([  5.3558, -17.2371])\n",
      "Epoch 3269, Loss 2.928047\n",
      "grad:tensor([-0.0020,  0.0115])\n",
      "params:tensor([  5.3558, -17.2372])\n",
      "Epoch 3270, Loss 2.928046\n",
      "grad:tensor([-0.0020,  0.0115])\n",
      "params:tensor([  5.3558, -17.2373])\n",
      "Epoch 3271, Loss 2.928046\n",
      "grad:tensor([-0.0020,  0.0115])\n",
      "params:tensor([  5.3558, -17.2374])\n",
      "Epoch 3272, Loss 2.928044\n",
      "grad:tensor([-0.0020,  0.0115])\n",
      "params:tensor([  5.3558, -17.2375])\n",
      "Epoch 3273, Loss 2.928042\n",
      "grad:tensor([-0.0020,  0.0114])\n",
      "params:tensor([  5.3559, -17.2376])\n",
      "Epoch 3274, Loss 2.928040\n",
      "grad:tensor([-0.0020,  0.0114])\n",
      "params:tensor([  5.3559, -17.2377])\n",
      "Epoch 3275, Loss 2.928040\n",
      "grad:tensor([-0.0020,  0.0114])\n",
      "params:tensor([  5.3559, -17.2379])\n",
      "Epoch 3276, Loss 2.928036\n",
      "grad:tensor([-0.0020,  0.0114])\n",
      "params:tensor([  5.3559, -17.2380])\n",
      "Epoch 3277, Loss 2.928036\n",
      "grad:tensor([-0.0020,  0.0113])\n",
      "params:tensor([  5.3559, -17.2381])\n",
      "Epoch 3278, Loss 2.928037\n",
      "grad:tensor([-0.0020,  0.0113])\n",
      "params:tensor([  5.3560, -17.2382])\n",
      "Epoch 3279, Loss 2.928034\n",
      "grad:tensor([-0.0020,  0.0113])\n",
      "params:tensor([  5.3560, -17.2383])\n",
      "Epoch 3280, Loss 2.928034\n",
      "grad:tensor([-0.0020,  0.0113])\n",
      "params:tensor([  5.3560, -17.2384])\n",
      "Epoch 3281, Loss 2.928031\n",
      "grad:tensor([-0.0020,  0.0113])\n",
      "params:tensor([  5.3560, -17.2385])\n",
      "Epoch 3282, Loss 2.928032\n",
      "grad:tensor([-0.0020,  0.0113])\n",
      "params:tensor([  5.3560, -17.2386])\n",
      "Epoch 3283, Loss 2.928028\n",
      "grad:tensor([-0.0020,  0.0112])\n",
      "params:tensor([  5.3561, -17.2388])\n",
      "Epoch 3284, Loss 2.928027\n",
      "grad:tensor([-0.0020,  0.0112])\n",
      "params:tensor([  5.3561, -17.2389])\n",
      "Epoch 3285, Loss 2.928026\n",
      "grad:tensor([-0.0020,  0.0112])\n",
      "params:tensor([  5.3561, -17.2390])\n",
      "Epoch 3286, Loss 2.928025\n",
      "grad:tensor([-0.0020,  0.0112])\n",
      "params:tensor([  5.3561, -17.2391])\n",
      "Epoch 3287, Loss 2.928024\n",
      "grad:tensor([-0.0020,  0.0112])\n",
      "params:tensor([  5.3561, -17.2392])\n",
      "Epoch 3288, Loss 2.928022\n",
      "grad:tensor([-0.0020,  0.0111])\n",
      "params:tensor([  5.3562, -17.2393])\n",
      "Epoch 3289, Loss 2.928023\n",
      "grad:tensor([-0.0020,  0.0111])\n",
      "params:tensor([  5.3562, -17.2394])\n",
      "Epoch 3290, Loss 2.928021\n",
      "grad:tensor([-0.0020,  0.0111])\n",
      "params:tensor([  5.3562, -17.2395])\n",
      "Epoch 3291, Loss 2.928019\n",
      "grad:tensor([-0.0020,  0.0111])\n",
      "params:tensor([  5.3562, -17.2397])\n",
      "Epoch 3292, Loss 2.928018\n",
      "grad:tensor([-0.0020,  0.0111])\n",
      "params:tensor([  5.3562, -17.2398])\n",
      "Epoch 3293, Loss 2.928017\n",
      "grad:tensor([-0.0020,  0.0110])\n",
      "params:tensor([  5.3563, -17.2399])\n",
      "Epoch 3294, Loss 2.928015\n",
      "grad:tensor([-0.0020,  0.0110])\n",
      "params:tensor([  5.3563, -17.2400])\n",
      "Epoch 3295, Loss 2.928013\n",
      "grad:tensor([-0.0020,  0.0110])\n",
      "params:tensor([  5.3563, -17.2401])\n",
      "Epoch 3296, Loss 2.928013\n",
      "grad:tensor([-0.0019,  0.0110])\n",
      "params:tensor([  5.3563, -17.2402])\n",
      "Epoch 3297, Loss 2.928011\n",
      "grad:tensor([-0.0019,  0.0110])\n",
      "params:tensor([  5.3563, -17.2403])\n",
      "Epoch 3298, Loss 2.928009\n",
      "grad:tensor([-0.0019,  0.0110])\n",
      "params:tensor([  5.3563, -17.2404])\n",
      "Epoch 3299, Loss 2.928008\n",
      "grad:tensor([-0.0019,  0.0109])\n",
      "params:tensor([  5.3564, -17.2405])\n",
      "Epoch 3300, Loss 2.928006\n",
      "grad:tensor([-0.0019,  0.0109])\n",
      "params:tensor([  5.3564, -17.2406])\n",
      "Epoch 3301, Loss 2.928007\n",
      "grad:tensor([-0.0019,  0.0109])\n",
      "params:tensor([  5.3564, -17.2407])\n",
      "Epoch 3302, Loss 2.928007\n",
      "grad:tensor([-0.0019,  0.0109])\n",
      "params:tensor([  5.3564, -17.2409])\n",
      "Epoch 3303, Loss 2.928004\n",
      "grad:tensor([-0.0019,  0.0109])\n",
      "params:tensor([  5.3564, -17.2410])\n",
      "Epoch 3304, Loss 2.928002\n",
      "grad:tensor([-0.0019,  0.0108])\n",
      "params:tensor([  5.3565, -17.2411])\n",
      "Epoch 3305, Loss 2.928002\n",
      "grad:tensor([-0.0019,  0.0108])\n",
      "params:tensor([  5.3565, -17.2412])\n",
      "Epoch 3306, Loss 2.928000\n",
      "grad:tensor([-0.0019,  0.0108])\n",
      "params:tensor([  5.3565, -17.2413])\n",
      "Epoch 3307, Loss 2.928000\n",
      "grad:tensor([-0.0019,  0.0108])\n",
      "params:tensor([  5.3565, -17.2414])\n",
      "Epoch 3308, Loss 2.927998\n",
      "grad:tensor([-0.0019,  0.0108])\n",
      "params:tensor([  5.3565, -17.2415])\n",
      "Epoch 3309, Loss 2.927995\n",
      "grad:tensor([-0.0019,  0.0107])\n",
      "params:tensor([  5.3566, -17.2416])\n",
      "Epoch 3310, Loss 2.927995\n",
      "grad:tensor([-0.0019,  0.0107])\n",
      "params:tensor([  5.3566, -17.2417])\n",
      "Epoch 3311, Loss 2.927994\n",
      "grad:tensor([-0.0019,  0.0107])\n",
      "params:tensor([  5.3566, -17.2418])\n",
      "Epoch 3312, Loss 2.927994\n",
      "grad:tensor([-0.0019,  0.0107])\n",
      "params:tensor([  5.3566, -17.2419])\n",
      "Epoch 3313, Loss 2.927991\n",
      "grad:tensor([-0.0019,  0.0107])\n",
      "params:tensor([  5.3566, -17.2420])\n",
      "Epoch 3314, Loss 2.927991\n",
      "grad:tensor([-0.0019,  0.0107])\n",
      "params:tensor([  5.3567, -17.2421])\n",
      "Epoch 3315, Loss 2.927990\n",
      "grad:tensor([-0.0019,  0.0106])\n",
      "params:tensor([  5.3567, -17.2423])\n",
      "Epoch 3316, Loss 2.927989\n",
      "grad:tensor([-0.0019,  0.0106])\n",
      "params:tensor([  5.3567, -17.2424])\n",
      "Epoch 3317, Loss 2.927988\n",
      "grad:tensor([-0.0019,  0.0106])\n",
      "params:tensor([  5.3567, -17.2425])\n",
      "Epoch 3318, Loss 2.927986\n",
      "grad:tensor([-0.0019,  0.0106])\n",
      "params:tensor([  5.3567, -17.2426])\n",
      "Epoch 3319, Loss 2.927985\n",
      "grad:tensor([-0.0019,  0.0106])\n",
      "params:tensor([  5.3567, -17.2427])\n",
      "Epoch 3320, Loss 2.927983\n",
      "grad:tensor([-0.0018,  0.0106])\n",
      "params:tensor([  5.3568, -17.2428])\n",
      "Epoch 3321, Loss 2.927983\n",
      "grad:tensor([-0.0018,  0.0105])\n",
      "params:tensor([  5.3568, -17.2429])\n",
      "Epoch 3322, Loss 2.927981\n",
      "grad:tensor([-0.0018,  0.0105])\n",
      "params:tensor([  5.3568, -17.2430])\n",
      "Epoch 3323, Loss 2.927980\n",
      "grad:tensor([-0.0018,  0.0105])\n",
      "params:tensor([  5.3568, -17.2431])\n",
      "Epoch 3324, Loss 2.927979\n",
      "grad:tensor([-0.0018,  0.0105])\n",
      "params:tensor([  5.3568, -17.2432])\n",
      "Epoch 3325, Loss 2.927979\n",
      "grad:tensor([-0.0018,  0.0105])\n",
      "params:tensor([  5.3569, -17.2433])\n",
      "Epoch 3326, Loss 2.927977\n",
      "grad:tensor([-0.0018,  0.0104])\n",
      "params:tensor([  5.3569, -17.2434])\n",
      "Epoch 3327, Loss 2.927975\n",
      "grad:tensor([-0.0019,  0.0104])\n",
      "params:tensor([  5.3569, -17.2435])\n",
      "Epoch 3328, Loss 2.927973\n",
      "grad:tensor([-0.0018,  0.0104])\n",
      "params:tensor([  5.3569, -17.2436])\n",
      "Epoch 3329, Loss 2.927974\n",
      "grad:tensor([-0.0018,  0.0104])\n",
      "params:tensor([  5.3569, -17.2437])\n",
      "Epoch 3330, Loss 2.927974\n",
      "grad:tensor([-0.0018,  0.0104])\n",
      "params:tensor([  5.3570, -17.2438])\n",
      "Epoch 3331, Loss 2.927972\n",
      "grad:tensor([-0.0018,  0.0104])\n",
      "params:tensor([  5.3570, -17.2439])\n",
      "Epoch 3332, Loss 2.927972\n",
      "grad:tensor([-0.0018,  0.0103])\n",
      "params:tensor([  5.3570, -17.2440])\n",
      "Epoch 3333, Loss 2.927969\n",
      "grad:tensor([-0.0018,  0.0103])\n",
      "params:tensor([  5.3570, -17.2441])\n",
      "Epoch 3334, Loss 2.927969\n",
      "grad:tensor([-0.0018,  0.0103])\n",
      "params:tensor([  5.3570, -17.2442])\n",
      "Epoch 3335, Loss 2.927967\n",
      "grad:tensor([-0.0018,  0.0103])\n",
      "params:tensor([  5.3570, -17.2443])\n",
      "Epoch 3336, Loss 2.927967\n",
      "grad:tensor([-0.0018,  0.0103])\n",
      "params:tensor([  5.3571, -17.2444])\n",
      "Epoch 3337, Loss 2.927963\n",
      "grad:tensor([-0.0018,  0.0102])\n",
      "params:tensor([  5.3571, -17.2446])\n",
      "Epoch 3338, Loss 2.927963\n",
      "grad:tensor([-0.0018,  0.0102])\n",
      "params:tensor([  5.3571, -17.2447])\n",
      "Epoch 3339, Loss 2.927962\n",
      "grad:tensor([-0.0018,  0.0102])\n",
      "params:tensor([  5.3571, -17.2448])\n",
      "Epoch 3340, Loss 2.927962\n",
      "grad:tensor([-0.0018,  0.0102])\n",
      "params:tensor([  5.3571, -17.2449])\n",
      "Epoch 3341, Loss 2.927960\n",
      "grad:tensor([-0.0018,  0.0102])\n",
      "params:tensor([  5.3572, -17.2450])\n",
      "Epoch 3342, Loss 2.927960\n",
      "grad:tensor([-0.0018,  0.0102])\n",
      "params:tensor([  5.3572, -17.2451])\n",
      "Epoch 3343, Loss 2.927959\n",
      "grad:tensor([-0.0018,  0.0101])\n",
      "params:tensor([  5.3572, -17.2452])\n",
      "Epoch 3344, Loss 2.927958\n",
      "grad:tensor([-0.0018,  0.0101])\n",
      "params:tensor([  5.3572, -17.2453])\n",
      "Epoch 3345, Loss 2.927956\n",
      "grad:tensor([-0.0018,  0.0101])\n",
      "params:tensor([  5.3572, -17.2454])\n",
      "Epoch 3346, Loss 2.927956\n",
      "grad:tensor([-0.0018,  0.0101])\n",
      "params:tensor([  5.3572, -17.2455])\n",
      "Epoch 3347, Loss 2.927955\n",
      "grad:tensor([-0.0018,  0.0101])\n",
      "params:tensor([  5.3573, -17.2456])\n",
      "Epoch 3348, Loss 2.927953\n",
      "grad:tensor([-0.0018,  0.0101])\n",
      "params:tensor([  5.3573, -17.2457])\n",
      "Epoch 3349, Loss 2.927953\n",
      "grad:tensor([-0.0018,  0.0100])\n",
      "params:tensor([  5.3573, -17.2458])\n",
      "Epoch 3350, Loss 2.927951\n",
      "grad:tensor([-0.0018,  0.0100])\n",
      "params:tensor([  5.3573, -17.2459])\n",
      "Epoch 3351, Loss 2.927950\n",
      "grad:tensor([-0.0018,  0.0100])\n",
      "params:tensor([  5.3573, -17.2460])\n",
      "Epoch 3352, Loss 2.927948\n",
      "grad:tensor([-0.0018,  0.0100])\n",
      "params:tensor([  5.3573, -17.2461])\n",
      "Epoch 3353, Loss 2.927947\n",
      "grad:tensor([-0.0017,  0.0100])\n",
      "params:tensor([  5.3574, -17.2462])\n",
      "Epoch 3354, Loss 2.927948\n",
      "grad:tensor([-0.0018,  0.0100])\n",
      "params:tensor([  5.3574, -17.2463])\n",
      "Epoch 3355, Loss 2.927945\n",
      "grad:tensor([-0.0017,  0.0099])\n",
      "params:tensor([  5.3574, -17.2464])\n",
      "Epoch 3356, Loss 2.927944\n",
      "grad:tensor([-0.0017,  0.0099])\n",
      "params:tensor([  5.3574, -17.2465])\n",
      "Epoch 3357, Loss 2.927943\n",
      "grad:tensor([-0.0018,  0.0099])\n",
      "params:tensor([  5.3574, -17.2466])\n",
      "Epoch 3358, Loss 2.927944\n",
      "grad:tensor([-0.0017,  0.0099])\n",
      "params:tensor([  5.3575, -17.2467])\n",
      "Epoch 3359, Loss 2.927942\n",
      "grad:tensor([-0.0017,  0.0099])\n",
      "params:tensor([  5.3575, -17.2468])\n",
      "Epoch 3360, Loss 2.927941\n",
      "grad:tensor([-0.0018,  0.0099])\n",
      "params:tensor([  5.3575, -17.2469])\n",
      "Epoch 3361, Loss 2.927940\n",
      "grad:tensor([-0.0017,  0.0098])\n",
      "params:tensor([  5.3575, -17.2470])\n",
      "Epoch 3362, Loss 2.927938\n",
      "grad:tensor([-0.0017,  0.0098])\n",
      "params:tensor([  5.3575, -17.2471])\n",
      "Epoch 3363, Loss 2.927938\n",
      "grad:tensor([-0.0018,  0.0098])\n",
      "params:tensor([  5.3575, -17.2472])\n",
      "Epoch 3364, Loss 2.927936\n",
      "grad:tensor([-0.0017,  0.0098])\n",
      "params:tensor([  5.3576, -17.2473])\n",
      "Epoch 3365, Loss 2.927936\n",
      "grad:tensor([-0.0017,  0.0098])\n",
      "params:tensor([  5.3576, -17.2474])\n",
      "Epoch 3366, Loss 2.927937\n",
      "grad:tensor([-0.0017,  0.0098])\n",
      "params:tensor([  5.3576, -17.2474])\n",
      "Epoch 3367, Loss 2.927934\n",
      "grad:tensor([-0.0017,  0.0097])\n",
      "params:tensor([  5.3576, -17.2475])\n",
      "Epoch 3368, Loss 2.927933\n",
      "grad:tensor([-0.0017,  0.0097])\n",
      "params:tensor([  5.3576, -17.2476])\n",
      "Epoch 3369, Loss 2.927932\n",
      "grad:tensor([-0.0017,  0.0097])\n",
      "params:tensor([  5.3576, -17.2477])\n",
      "Epoch 3370, Loss 2.927930\n",
      "grad:tensor([-0.0017,  0.0097])\n",
      "params:tensor([  5.3577, -17.2478])\n",
      "Epoch 3371, Loss 2.927928\n",
      "grad:tensor([-0.0017,  0.0097])\n",
      "params:tensor([  5.3577, -17.2479])\n",
      "Epoch 3372, Loss 2.927931\n",
      "grad:tensor([-0.0017,  0.0097])\n",
      "params:tensor([  5.3577, -17.2480])\n",
      "Epoch 3373, Loss 2.927929\n",
      "grad:tensor([-0.0017,  0.0096])\n",
      "params:tensor([  5.3577, -17.2481])\n",
      "Epoch 3374, Loss 2.927927\n",
      "grad:tensor([-0.0017,  0.0096])\n",
      "params:tensor([  5.3577, -17.2482])\n",
      "Epoch 3375, Loss 2.927926\n",
      "grad:tensor([-0.0017,  0.0096])\n",
      "params:tensor([  5.3577, -17.2483])\n",
      "Epoch 3376, Loss 2.927925\n",
      "grad:tensor([-0.0017,  0.0096])\n",
      "params:tensor([  5.3578, -17.2484])\n",
      "Epoch 3377, Loss 2.927924\n",
      "grad:tensor([-0.0017,  0.0096])\n",
      "params:tensor([  5.3578, -17.2485])\n",
      "Epoch 3378, Loss 2.927923\n",
      "grad:tensor([-0.0017,  0.0096])\n",
      "params:tensor([  5.3578, -17.2486])\n",
      "Epoch 3379, Loss 2.927924\n",
      "grad:tensor([-0.0017,  0.0095])\n",
      "params:tensor([  5.3578, -17.2487])\n",
      "Epoch 3380, Loss 2.927922\n",
      "grad:tensor([-0.0017,  0.0095])\n",
      "params:tensor([  5.3578, -17.2488])\n",
      "Epoch 3381, Loss 2.927922\n",
      "grad:tensor([-0.0017,  0.0095])\n",
      "params:tensor([  5.3578, -17.2489])\n",
      "Epoch 3382, Loss 2.927920\n",
      "grad:tensor([-0.0017,  0.0095])\n",
      "params:tensor([  5.3579, -17.2490])\n",
      "Epoch 3383, Loss 2.927918\n",
      "grad:tensor([-0.0017,  0.0095])\n",
      "params:tensor([  5.3579, -17.2491])\n",
      "Epoch 3384, Loss 2.927917\n",
      "grad:tensor([-0.0017,  0.0095])\n",
      "params:tensor([  5.3579, -17.2492])\n",
      "Epoch 3385, Loss 2.927917\n",
      "grad:tensor([-0.0017,  0.0094])\n",
      "params:tensor([  5.3579, -17.2493])\n",
      "Epoch 3386, Loss 2.927915\n",
      "grad:tensor([-0.0017,  0.0094])\n",
      "params:tensor([  5.3579, -17.2494])\n",
      "Epoch 3387, Loss 2.927915\n",
      "grad:tensor([-0.0017,  0.0094])\n",
      "params:tensor([  5.3579, -17.2495])\n",
      "Epoch 3388, Loss 2.927914\n",
      "grad:tensor([-0.0016,  0.0094])\n",
      "params:tensor([  5.3580, -17.2496])\n",
      "Epoch 3389, Loss 2.927913\n",
      "grad:tensor([-0.0017,  0.0094])\n",
      "params:tensor([  5.3580, -17.2496])\n",
      "Epoch 3390, Loss 2.927911\n",
      "grad:tensor([-0.0016,  0.0094])\n",
      "params:tensor([  5.3580, -17.2497])\n",
      "Epoch 3391, Loss 2.927913\n",
      "grad:tensor([-0.0017,  0.0093])\n",
      "params:tensor([  5.3580, -17.2498])\n",
      "Epoch 3392, Loss 2.927911\n",
      "grad:tensor([-0.0016,  0.0093])\n",
      "params:tensor([  5.3580, -17.2499])\n",
      "Epoch 3393, Loss 2.927910\n",
      "grad:tensor([-0.0016,  0.0093])\n",
      "params:tensor([  5.3580, -17.2500])\n",
      "Epoch 3394, Loss 2.927909\n",
      "grad:tensor([-0.0016,  0.0093])\n",
      "params:tensor([  5.3581, -17.2501])\n",
      "Epoch 3395, Loss 2.927908\n",
      "grad:tensor([-0.0016,  0.0093])\n",
      "params:tensor([  5.3581, -17.2502])\n",
      "Epoch 3396, Loss 2.927907\n",
      "grad:tensor([-0.0017,  0.0093])\n",
      "params:tensor([  5.3581, -17.2503])\n",
      "Epoch 3397, Loss 2.927906\n",
      "grad:tensor([-0.0016,  0.0093])\n",
      "params:tensor([  5.3581, -17.2504])\n",
      "Epoch 3398, Loss 2.927905\n",
      "grad:tensor([-0.0017,  0.0092])\n",
      "params:tensor([  5.3581, -17.2505])\n",
      "Epoch 3399, Loss 2.927905\n",
      "grad:tensor([-0.0016,  0.0092])\n",
      "params:tensor([  5.3581, -17.2506])\n",
      "Epoch 3400, Loss 2.927904\n",
      "grad:tensor([-0.0016,  0.0092])\n",
      "params:tensor([  5.3582, -17.2507])\n",
      "Epoch 3401, Loss 2.927902\n",
      "grad:tensor([-0.0016,  0.0092])\n",
      "params:tensor([  5.3582, -17.2508])\n",
      "Epoch 3402, Loss 2.927902\n",
      "grad:tensor([-0.0016,  0.0092])\n",
      "params:tensor([  5.3582, -17.2509])\n",
      "Epoch 3403, Loss 2.927902\n",
      "grad:tensor([-0.0016,  0.0092])\n",
      "params:tensor([  5.3582, -17.2509])\n",
      "Epoch 3404, Loss 2.927899\n",
      "grad:tensor([-0.0016,  0.0091])\n",
      "params:tensor([  5.3582, -17.2510])\n",
      "Epoch 3405, Loss 2.927899\n",
      "grad:tensor([-0.0016,  0.0091])\n",
      "params:tensor([  5.3582, -17.2511])\n",
      "Epoch 3406, Loss 2.927898\n",
      "grad:tensor([-0.0016,  0.0091])\n",
      "params:tensor([  5.3583, -17.2512])\n",
      "Epoch 3407, Loss 2.927899\n",
      "grad:tensor([-0.0016,  0.0091])\n",
      "params:tensor([  5.3583, -17.2513])\n",
      "Epoch 3408, Loss 2.927896\n",
      "grad:tensor([-0.0016,  0.0091])\n",
      "params:tensor([  5.3583, -17.2514])\n",
      "Epoch 3409, Loss 2.927895\n",
      "grad:tensor([-0.0016,  0.0091])\n",
      "params:tensor([  5.3583, -17.2515])\n",
      "Epoch 3410, Loss 2.927896\n",
      "grad:tensor([-0.0016,  0.0091])\n",
      "params:tensor([  5.3583, -17.2516])\n",
      "Epoch 3411, Loss 2.927894\n",
      "grad:tensor([-0.0016,  0.0090])\n",
      "params:tensor([  5.3583, -17.2517])\n",
      "Epoch 3412, Loss 2.927892\n",
      "grad:tensor([-0.0016,  0.0090])\n",
      "params:tensor([  5.3584, -17.2518])\n",
      "Epoch 3413, Loss 2.927892\n",
      "grad:tensor([-0.0016,  0.0090])\n",
      "params:tensor([  5.3584, -17.2519])\n",
      "Epoch 3414, Loss 2.927891\n",
      "grad:tensor([-0.0016,  0.0090])\n",
      "params:tensor([  5.3584, -17.2519])\n",
      "Epoch 3415, Loss 2.927891\n",
      "grad:tensor([-0.0016,  0.0090])\n",
      "params:tensor([  5.3584, -17.2520])\n",
      "Epoch 3416, Loss 2.927890\n",
      "grad:tensor([-0.0016,  0.0090])\n",
      "params:tensor([  5.3584, -17.2521])\n",
      "Epoch 3417, Loss 2.927891\n",
      "grad:tensor([-0.0016,  0.0089])\n",
      "params:tensor([  5.3584, -17.2522])\n",
      "Epoch 3418, Loss 2.927888\n",
      "grad:tensor([-0.0016,  0.0089])\n",
      "params:tensor([  5.3584, -17.2523])\n",
      "Epoch 3419, Loss 2.927888\n",
      "grad:tensor([-0.0016,  0.0089])\n",
      "params:tensor([  5.3585, -17.2524])\n",
      "Epoch 3420, Loss 2.927886\n",
      "grad:tensor([-0.0016,  0.0089])\n",
      "params:tensor([  5.3585, -17.2525])\n",
      "Epoch 3421, Loss 2.927887\n",
      "grad:tensor([-0.0016,  0.0089])\n",
      "params:tensor([  5.3585, -17.2526])\n",
      "Epoch 3422, Loss 2.927886\n",
      "grad:tensor([-0.0016,  0.0089])\n",
      "params:tensor([  5.3585, -17.2527])\n",
      "Epoch 3423, Loss 2.927884\n",
      "grad:tensor([-0.0016,  0.0089])\n",
      "params:tensor([  5.3585, -17.2527])\n",
      "Epoch 3424, Loss 2.927883\n",
      "grad:tensor([-0.0015,  0.0088])\n",
      "params:tensor([  5.3585, -17.2528])\n",
      "Epoch 3425, Loss 2.927881\n",
      "grad:tensor([-0.0015,  0.0088])\n",
      "params:tensor([  5.3586, -17.2529])\n",
      "Epoch 3426, Loss 2.927881\n",
      "grad:tensor([-0.0016,  0.0088])\n",
      "params:tensor([  5.3586, -17.2530])\n",
      "Epoch 3427, Loss 2.927880\n",
      "grad:tensor([-0.0015,  0.0088])\n",
      "params:tensor([  5.3586, -17.2531])\n",
      "Epoch 3428, Loss 2.927880\n",
      "grad:tensor([-0.0016,  0.0088])\n",
      "params:tensor([  5.3586, -17.2532])\n",
      "Epoch 3429, Loss 2.927879\n",
      "grad:tensor([-0.0015,  0.0088])\n",
      "params:tensor([  5.3586, -17.2533])\n",
      "Epoch 3430, Loss 2.927877\n",
      "grad:tensor([-0.0016,  0.0087])\n",
      "params:tensor([  5.3586, -17.2534])\n",
      "Epoch 3431, Loss 2.927876\n",
      "grad:tensor([-0.0015,  0.0087])\n",
      "params:tensor([  5.3586, -17.2534])\n",
      "Epoch 3432, Loss 2.927876\n",
      "grad:tensor([-0.0015,  0.0087])\n",
      "params:tensor([  5.3587, -17.2535])\n",
      "Epoch 3433, Loss 2.927876\n",
      "grad:tensor([-0.0015,  0.0087])\n",
      "params:tensor([  5.3587, -17.2536])\n",
      "Epoch 3434, Loss 2.927876\n",
      "grad:tensor([-0.0016,  0.0087])\n",
      "params:tensor([  5.3587, -17.2537])\n",
      "Epoch 3435, Loss 2.927876\n",
      "grad:tensor([-0.0016,  0.0087])\n",
      "params:tensor([  5.3587, -17.2538])\n",
      "Epoch 3436, Loss 2.927875\n",
      "grad:tensor([-0.0015,  0.0087])\n",
      "params:tensor([  5.3587, -17.2539])\n",
      "Epoch 3437, Loss 2.927873\n",
      "grad:tensor([-0.0015,  0.0087])\n",
      "params:tensor([  5.3587, -17.2540])\n",
      "Epoch 3438, Loss 2.927872\n",
      "grad:tensor([-0.0015,  0.0086])\n",
      "params:tensor([  5.3588, -17.2541])\n",
      "Epoch 3439, Loss 2.927871\n",
      "grad:tensor([-0.0015,  0.0086])\n",
      "params:tensor([  5.3588, -17.2541])\n",
      "Epoch 3440, Loss 2.927870\n",
      "grad:tensor([-0.0015,  0.0086])\n",
      "params:tensor([  5.3588, -17.2542])\n",
      "Epoch 3441, Loss 2.927871\n",
      "grad:tensor([-0.0015,  0.0086])\n",
      "params:tensor([  5.3588, -17.2543])\n",
      "Epoch 3442, Loss 2.927869\n",
      "grad:tensor([-0.0015,  0.0086])\n",
      "params:tensor([  5.3588, -17.2544])\n",
      "Epoch 3443, Loss 2.927869\n",
      "grad:tensor([-0.0015,  0.0086])\n",
      "params:tensor([  5.3588, -17.2545])\n",
      "Epoch 3444, Loss 2.927867\n",
      "grad:tensor([-0.0015,  0.0085])\n",
      "params:tensor([  5.3588, -17.2546])\n",
      "Epoch 3445, Loss 2.927866\n",
      "grad:tensor([-0.0015,  0.0085])\n",
      "params:tensor([  5.3589, -17.2547])\n",
      "Epoch 3446, Loss 2.927866\n",
      "grad:tensor([-0.0015,  0.0085])\n",
      "params:tensor([  5.3589, -17.2547])\n",
      "Epoch 3447, Loss 2.927866\n",
      "grad:tensor([-0.0015,  0.0085])\n",
      "params:tensor([  5.3589, -17.2548])\n",
      "Epoch 3448, Loss 2.927864\n",
      "grad:tensor([-0.0015,  0.0085])\n",
      "params:tensor([  5.3589, -17.2549])\n",
      "Epoch 3449, Loss 2.927863\n",
      "grad:tensor([-0.0015,  0.0085])\n",
      "params:tensor([  5.3589, -17.2550])\n",
      "Epoch 3450, Loss 2.927863\n",
      "grad:tensor([-0.0015,  0.0085])\n",
      "params:tensor([  5.3589, -17.2551])\n",
      "Epoch 3451, Loss 2.927862\n",
      "grad:tensor([-0.0015,  0.0084])\n",
      "params:tensor([  5.3590, -17.2552])\n",
      "Epoch 3452, Loss 2.927863\n",
      "grad:tensor([-0.0015,  0.0084])\n",
      "params:tensor([  5.3590, -17.2552])\n",
      "Epoch 3453, Loss 2.927860\n",
      "grad:tensor([-0.0015,  0.0084])\n",
      "params:tensor([  5.3590, -17.2553])\n",
      "Epoch 3454, Loss 2.927860\n",
      "grad:tensor([-0.0015,  0.0084])\n",
      "params:tensor([  5.3590, -17.2554])\n",
      "Epoch 3455, Loss 2.927860\n",
      "grad:tensor([-0.0015,  0.0084])\n",
      "params:tensor([  5.3590, -17.2555])\n",
      "Epoch 3456, Loss 2.927859\n",
      "grad:tensor([-0.0015,  0.0084])\n",
      "params:tensor([  5.3590, -17.2556])\n",
      "Epoch 3457, Loss 2.927858\n",
      "grad:tensor([-0.0015,  0.0084])\n",
      "params:tensor([  5.3590, -17.2557])\n",
      "Epoch 3458, Loss 2.927858\n",
      "grad:tensor([-0.0015,  0.0083])\n",
      "params:tensor([  5.3591, -17.2557])\n",
      "Epoch 3459, Loss 2.927856\n",
      "grad:tensor([-0.0015,  0.0083])\n",
      "params:tensor([  5.3591, -17.2558])\n",
      "Epoch 3460, Loss 2.927857\n",
      "grad:tensor([-0.0015,  0.0083])\n",
      "params:tensor([  5.3591, -17.2559])\n",
      "Epoch 3461, Loss 2.927854\n",
      "grad:tensor([-0.0015,  0.0083])\n",
      "params:tensor([  5.3591, -17.2560])\n",
      "Epoch 3462, Loss 2.927855\n",
      "grad:tensor([-0.0015,  0.0083])\n",
      "params:tensor([  5.3591, -17.2561])\n",
      "Epoch 3463, Loss 2.927854\n",
      "grad:tensor([-0.0015,  0.0083])\n",
      "params:tensor([  5.3591, -17.2562])\n",
      "Epoch 3464, Loss 2.927854\n",
      "grad:tensor([-0.0015,  0.0083])\n",
      "params:tensor([  5.3591, -17.2562])\n",
      "Epoch 3465, Loss 2.927851\n",
      "grad:tensor([-0.0014,  0.0082])\n",
      "params:tensor([  5.3592, -17.2563])\n",
      "Epoch 3466, Loss 2.927853\n",
      "grad:tensor([-0.0015,  0.0082])\n",
      "params:tensor([  5.3592, -17.2564])\n",
      "Epoch 3467, Loss 2.927852\n",
      "grad:tensor([-0.0014,  0.0082])\n",
      "params:tensor([  5.3592, -17.2565])\n",
      "Epoch 3468, Loss 2.927850\n",
      "grad:tensor([-0.0014,  0.0082])\n",
      "params:tensor([  5.3592, -17.2566])\n",
      "Epoch 3469, Loss 2.927849\n",
      "grad:tensor([-0.0015,  0.0082])\n",
      "params:tensor([  5.3592, -17.2567])\n",
      "Epoch 3470, Loss 2.927849\n",
      "grad:tensor([-0.0015,  0.0082])\n",
      "params:tensor([  5.3592, -17.2567])\n",
      "Epoch 3471, Loss 2.927848\n",
      "grad:tensor([-0.0014,  0.0082])\n",
      "params:tensor([  5.3592, -17.2568])\n",
      "Epoch 3472, Loss 2.927848\n",
      "grad:tensor([-0.0014,  0.0081])\n",
      "params:tensor([  5.3593, -17.2569])\n",
      "Epoch 3473, Loss 2.927846\n",
      "grad:tensor([-0.0015,  0.0081])\n",
      "params:tensor([  5.3593, -17.2570])\n",
      "Epoch 3474, Loss 2.927846\n",
      "grad:tensor([-0.0015,  0.0081])\n",
      "params:tensor([  5.3593, -17.2571])\n",
      "Epoch 3475, Loss 2.927845\n",
      "grad:tensor([-0.0014,  0.0081])\n",
      "params:tensor([  5.3593, -17.2571])\n",
      "Epoch 3476, Loss 2.927844\n",
      "grad:tensor([-0.0014,  0.0081])\n",
      "params:tensor([  5.3593, -17.2572])\n",
      "Epoch 3477, Loss 2.927844\n",
      "grad:tensor([-0.0014,  0.0081])\n",
      "params:tensor([  5.3593, -17.2573])\n",
      "Epoch 3478, Loss 2.927844\n",
      "grad:tensor([-0.0014,  0.0081])\n",
      "params:tensor([  5.3593, -17.2574])\n",
      "Epoch 3479, Loss 2.927843\n",
      "grad:tensor([-0.0014,  0.0081])\n",
      "params:tensor([  5.3594, -17.2575])\n",
      "Epoch 3480, Loss 2.927843\n",
      "grad:tensor([-0.0014,  0.0080])\n",
      "params:tensor([  5.3594, -17.2575])\n",
      "Epoch 3481, Loss 2.927842\n",
      "grad:tensor([-0.0014,  0.0080])\n",
      "params:tensor([  5.3594, -17.2576])\n",
      "Epoch 3482, Loss 2.927840\n",
      "grad:tensor([-0.0014,  0.0080])\n",
      "params:tensor([  5.3594, -17.2577])\n",
      "Epoch 3483, Loss 2.927842\n",
      "grad:tensor([-0.0014,  0.0080])\n",
      "params:tensor([  5.3594, -17.2578])\n",
      "Epoch 3484, Loss 2.927839\n",
      "grad:tensor([-0.0014,  0.0080])\n",
      "params:tensor([  5.3594, -17.2579])\n",
      "Epoch 3485, Loss 2.927838\n",
      "grad:tensor([-0.0014,  0.0080])\n",
      "params:tensor([  5.3594, -17.2579])\n",
      "Epoch 3486, Loss 2.927839\n",
      "grad:tensor([-0.0014,  0.0080])\n",
      "params:tensor([  5.3595, -17.2580])\n",
      "Epoch 3487, Loss 2.927838\n",
      "grad:tensor([-0.0014,  0.0079])\n",
      "params:tensor([  5.3595, -17.2581])\n",
      "Epoch 3488, Loss 2.927837\n",
      "grad:tensor([-0.0014,  0.0079])\n",
      "params:tensor([  5.3595, -17.2582])\n",
      "Epoch 3489, Loss 2.927835\n",
      "grad:tensor([-0.0014,  0.0079])\n",
      "params:tensor([  5.3595, -17.2583])\n",
      "Epoch 3490, Loss 2.927837\n",
      "grad:tensor([-0.0014,  0.0079])\n",
      "params:tensor([  5.3595, -17.2583])\n",
      "Epoch 3491, Loss 2.927836\n",
      "grad:tensor([-0.0014,  0.0079])\n",
      "params:tensor([  5.3595, -17.2584])\n",
      "Epoch 3492, Loss 2.927835\n",
      "grad:tensor([-0.0014,  0.0079])\n",
      "params:tensor([  5.3595, -17.2585])\n",
      "Epoch 3493, Loss 2.927833\n",
      "grad:tensor([-0.0014,  0.0079])\n",
      "params:tensor([  5.3596, -17.2586])\n",
      "Epoch 3494, Loss 2.927833\n",
      "grad:tensor([-0.0014,  0.0079])\n",
      "params:tensor([  5.3596, -17.2587])\n",
      "Epoch 3495, Loss 2.927833\n",
      "grad:tensor([-0.0014,  0.0078])\n",
      "params:tensor([  5.3596, -17.2587])\n",
      "Epoch 3496, Loss 2.927832\n",
      "grad:tensor([-0.0014,  0.0078])\n",
      "params:tensor([  5.3596, -17.2588])\n",
      "Epoch 3497, Loss 2.927831\n",
      "grad:tensor([-0.0014,  0.0078])\n",
      "params:tensor([  5.3596, -17.2589])\n",
      "Epoch 3498, Loss 2.927830\n",
      "grad:tensor([-0.0014,  0.0078])\n",
      "params:tensor([  5.3596, -17.2590])\n",
      "Epoch 3499, Loss 2.927830\n",
      "grad:tensor([-0.0014,  0.0078])\n",
      "params:tensor([  5.3596, -17.2590])\n",
      "Epoch 3500, Loss 2.927830\n",
      "grad:tensor([-0.0014,  0.0078])\n",
      "params:tensor([  5.3597, -17.2591])\n",
      "Epoch 3501, Loss 2.927829\n",
      "grad:tensor([-0.0014,  0.0078])\n",
      "params:tensor([  5.3597, -17.2592])\n",
      "Epoch 3502, Loss 2.927828\n",
      "grad:tensor([-0.0014,  0.0077])\n",
      "params:tensor([  5.3597, -17.2593])\n",
      "Epoch 3503, Loss 2.927828\n",
      "grad:tensor([-0.0014,  0.0077])\n",
      "params:tensor([  5.3597, -17.2594])\n",
      "Epoch 3504, Loss 2.927827\n",
      "grad:tensor([-0.0014,  0.0077])\n",
      "params:tensor([  5.3597, -17.2594])\n",
      "Epoch 3505, Loss 2.927825\n",
      "grad:tensor([-0.0014,  0.0077])\n",
      "params:tensor([  5.3597, -17.2595])\n",
      "Epoch 3506, Loss 2.927827\n",
      "grad:tensor([-0.0014,  0.0077])\n",
      "params:tensor([  5.3597, -17.2596])\n",
      "Epoch 3507, Loss 2.927825\n",
      "grad:tensor([-0.0014,  0.0077])\n",
      "params:tensor([  5.3597, -17.2597])\n",
      "Epoch 3508, Loss 2.927824\n",
      "grad:tensor([-0.0013,  0.0077])\n",
      "params:tensor([  5.3598, -17.2597])\n",
      "Epoch 3509, Loss 2.927824\n",
      "grad:tensor([-0.0013,  0.0077])\n",
      "params:tensor([  5.3598, -17.2598])\n",
      "Epoch 3510, Loss 2.927824\n",
      "grad:tensor([-0.0013,  0.0076])\n",
      "params:tensor([  5.3598, -17.2599])\n",
      "Epoch 3511, Loss 2.927822\n",
      "grad:tensor([-0.0014,  0.0076])\n",
      "params:tensor([  5.3598, -17.2600])\n",
      "Epoch 3512, Loss 2.927822\n",
      "grad:tensor([-0.0014,  0.0076])\n",
      "params:tensor([  5.3598, -17.2600])\n",
      "Epoch 3513, Loss 2.927821\n",
      "grad:tensor([-0.0013,  0.0076])\n",
      "params:tensor([  5.3598, -17.2601])\n",
      "Epoch 3514, Loss 2.927820\n",
      "grad:tensor([-0.0013,  0.0076])\n",
      "params:tensor([  5.3598, -17.2602])\n",
      "Epoch 3515, Loss 2.927820\n",
      "grad:tensor([-0.0014,  0.0076])\n",
      "params:tensor([  5.3599, -17.2603])\n",
      "Epoch 3516, Loss 2.927821\n",
      "grad:tensor([-0.0014,  0.0076])\n",
      "params:tensor([  5.3599, -17.2604])\n",
      "Epoch 3517, Loss 2.927819\n",
      "grad:tensor([-0.0014,  0.0075])\n",
      "params:tensor([  5.3599, -17.2604])\n",
      "Epoch 3518, Loss 2.927819\n",
      "grad:tensor([-0.0014,  0.0075])\n",
      "params:tensor([  5.3599, -17.2605])\n",
      "Epoch 3519, Loss 2.927819\n",
      "grad:tensor([-0.0013,  0.0075])\n",
      "params:tensor([  5.3599, -17.2606])\n",
      "Epoch 3520, Loss 2.927817\n",
      "grad:tensor([-0.0013,  0.0075])\n",
      "params:tensor([  5.3599, -17.2607])\n",
      "Epoch 3521, Loss 2.927817\n",
      "grad:tensor([-0.0013,  0.0075])\n",
      "params:tensor([  5.3599, -17.2607])\n",
      "Epoch 3522, Loss 2.927816\n",
      "grad:tensor([-0.0013,  0.0075])\n",
      "params:tensor([  5.3599, -17.2608])\n",
      "Epoch 3523, Loss 2.927815\n",
      "grad:tensor([-0.0013,  0.0075])\n",
      "params:tensor([  5.3600, -17.2609])\n",
      "Epoch 3524, Loss 2.927816\n",
      "grad:tensor([-0.0013,  0.0075])\n",
      "params:tensor([  5.3600, -17.2610])\n",
      "Epoch 3525, Loss 2.927815\n",
      "grad:tensor([-0.0013,  0.0074])\n",
      "params:tensor([  5.3600, -17.2610])\n",
      "Epoch 3526, Loss 2.927814\n",
      "grad:tensor([-0.0013,  0.0074])\n",
      "params:tensor([  5.3600, -17.2611])\n",
      "Epoch 3527, Loss 2.927813\n",
      "grad:tensor([-0.0013,  0.0074])\n",
      "params:tensor([  5.3600, -17.2612])\n",
      "Epoch 3528, Loss 2.927812\n",
      "grad:tensor([-0.0013,  0.0074])\n",
      "params:tensor([  5.3600, -17.2612])\n",
      "Epoch 3529, Loss 2.927811\n",
      "grad:tensor([-0.0013,  0.0074])\n",
      "params:tensor([  5.3600, -17.2613])\n",
      "Epoch 3530, Loss 2.927812\n",
      "grad:tensor([-0.0013,  0.0074])\n",
      "params:tensor([  5.3601, -17.2614])\n",
      "Epoch 3531, Loss 2.927812\n",
      "grad:tensor([-0.0013,  0.0074])\n",
      "params:tensor([  5.3601, -17.2615])\n",
      "Epoch 3532, Loss 2.927810\n",
      "grad:tensor([-0.0013,  0.0074])\n",
      "params:tensor([  5.3601, -17.2615])\n",
      "Epoch 3533, Loss 2.927809\n",
      "grad:tensor([-0.0013,  0.0073])\n",
      "params:tensor([  5.3601, -17.2616])\n",
      "Epoch 3534, Loss 2.927810\n",
      "grad:tensor([-0.0013,  0.0073])\n",
      "params:tensor([  5.3601, -17.2617])\n",
      "Epoch 3535, Loss 2.927809\n",
      "grad:tensor([-0.0013,  0.0073])\n",
      "params:tensor([  5.3601, -17.2618])\n",
      "Epoch 3536, Loss 2.927808\n",
      "grad:tensor([-0.0013,  0.0073])\n",
      "params:tensor([  5.3601, -17.2618])\n",
      "Epoch 3537, Loss 2.927808\n",
      "grad:tensor([-0.0013,  0.0073])\n",
      "params:tensor([  5.3601, -17.2619])\n",
      "Epoch 3538, Loss 2.927806\n",
      "grad:tensor([-0.0013,  0.0073])\n",
      "params:tensor([  5.3602, -17.2620])\n",
      "Epoch 3539, Loss 2.927806\n",
      "grad:tensor([-0.0013,  0.0073])\n",
      "params:tensor([  5.3602, -17.2621])\n",
      "Epoch 3540, Loss 2.927805\n",
      "grad:tensor([-0.0013,  0.0073])\n",
      "params:tensor([  5.3602, -17.2621])\n",
      "Epoch 3541, Loss 2.927804\n",
      "grad:tensor([-0.0013,  0.0073])\n",
      "params:tensor([  5.3602, -17.2622])\n",
      "Epoch 3542, Loss 2.927805\n",
      "grad:tensor([-0.0013,  0.0072])\n",
      "params:tensor([  5.3602, -17.2623])\n",
      "Epoch 3543, Loss 2.927804\n",
      "grad:tensor([-0.0013,  0.0072])\n",
      "params:tensor([  5.3602, -17.2623])\n",
      "Epoch 3544, Loss 2.927805\n",
      "grad:tensor([-0.0013,  0.0072])\n",
      "params:tensor([  5.3602, -17.2624])\n",
      "Epoch 3545, Loss 2.927804\n",
      "grad:tensor([-0.0013,  0.0072])\n",
      "params:tensor([  5.3602, -17.2625])\n",
      "Epoch 3546, Loss 2.927804\n",
      "grad:tensor([-0.0013,  0.0072])\n",
      "params:tensor([  5.3603, -17.2626])\n",
      "Epoch 3547, Loss 2.927803\n",
      "grad:tensor([-0.0013,  0.0072])\n",
      "params:tensor([  5.3603, -17.2626])\n",
      "Epoch 3548, Loss 2.927802\n",
      "grad:tensor([-0.0013,  0.0072])\n",
      "params:tensor([  5.3603, -17.2627])\n",
      "Epoch 3549, Loss 2.927801\n",
      "grad:tensor([-0.0013,  0.0071])\n",
      "params:tensor([  5.3603, -17.2628])\n",
      "Epoch 3550, Loss 2.927801\n",
      "grad:tensor([-0.0013,  0.0071])\n",
      "params:tensor([  5.3603, -17.2628])\n",
      "Epoch 3551, Loss 2.927799\n",
      "grad:tensor([-0.0012,  0.0071])\n",
      "params:tensor([  5.3603, -17.2629])\n",
      "Epoch 3552, Loss 2.927801\n",
      "grad:tensor([-0.0012,  0.0071])\n",
      "params:tensor([  5.3603, -17.2630])\n",
      "Epoch 3553, Loss 2.927798\n",
      "grad:tensor([-0.0012,  0.0071])\n",
      "params:tensor([  5.3603, -17.2631])\n",
      "Epoch 3554, Loss 2.927798\n",
      "grad:tensor([-0.0012,  0.0071])\n",
      "params:tensor([  5.3604, -17.2631])\n",
      "Epoch 3555, Loss 2.927798\n",
      "grad:tensor([-0.0013,  0.0071])\n",
      "params:tensor([  5.3604, -17.2632])\n",
      "Epoch 3556, Loss 2.927798\n",
      "grad:tensor([-0.0013,  0.0071])\n",
      "params:tensor([  5.3604, -17.2633])\n",
      "Epoch 3557, Loss 2.927798\n",
      "grad:tensor([-0.0013,  0.0071])\n",
      "params:tensor([  5.3604, -17.2633])\n",
      "Epoch 3558, Loss 2.927796\n",
      "grad:tensor([-0.0013,  0.0070])\n",
      "params:tensor([  5.3604, -17.2634])\n",
      "Epoch 3559, Loss 2.927795\n",
      "grad:tensor([-0.0013,  0.0070])\n",
      "params:tensor([  5.3604, -17.2635])\n",
      "Epoch 3560, Loss 2.927796\n",
      "grad:tensor([-0.0013,  0.0070])\n",
      "params:tensor([  5.3604, -17.2636])\n",
      "Epoch 3561, Loss 2.927794\n",
      "grad:tensor([-0.0013,  0.0070])\n",
      "params:tensor([  5.3604, -17.2636])\n",
      "Epoch 3562, Loss 2.927795\n",
      "grad:tensor([-0.0013,  0.0070])\n",
      "params:tensor([  5.3605, -17.2637])\n",
      "Epoch 3563, Loss 2.927795\n",
      "grad:tensor([-0.0013,  0.0070])\n",
      "params:tensor([  5.3605, -17.2638])\n",
      "Epoch 3564, Loss 2.927793\n",
      "grad:tensor([-0.0013,  0.0070])\n",
      "params:tensor([  5.3605, -17.2638])\n",
      "Epoch 3565, Loss 2.927795\n",
      "grad:tensor([-0.0012,  0.0070])\n",
      "params:tensor([  5.3605, -17.2639])\n",
      "Epoch 3566, Loss 2.927791\n",
      "grad:tensor([-0.0012,  0.0069])\n",
      "params:tensor([  5.3605, -17.2640])\n",
      "Epoch 3567, Loss 2.927791\n",
      "grad:tensor([-0.0012,  0.0069])\n",
      "params:tensor([  5.3605, -17.2640])\n",
      "Epoch 3568, Loss 2.927791\n",
      "grad:tensor([-0.0012,  0.0069])\n",
      "params:tensor([  5.3605, -17.2641])\n",
      "Epoch 3569, Loss 2.927790\n",
      "grad:tensor([-0.0012,  0.0069])\n",
      "params:tensor([  5.3605, -17.2642])\n",
      "Epoch 3570, Loss 2.927790\n",
      "grad:tensor([-0.0012,  0.0069])\n",
      "params:tensor([  5.3606, -17.2642])\n",
      "Epoch 3571, Loss 2.927789\n",
      "grad:tensor([-0.0012,  0.0069])\n",
      "params:tensor([  5.3606, -17.2643])\n",
      "Epoch 3572, Loss 2.927790\n",
      "grad:tensor([-0.0012,  0.0069])\n",
      "params:tensor([  5.3606, -17.2644])\n",
      "Epoch 3573, Loss 2.927789\n",
      "grad:tensor([-0.0012,  0.0069])\n",
      "params:tensor([  5.3606, -17.2645])\n",
      "Epoch 3574, Loss 2.927789\n",
      "grad:tensor([-0.0012,  0.0069])\n",
      "params:tensor([  5.3606, -17.2645])\n",
      "Epoch 3575, Loss 2.927789\n",
      "grad:tensor([-0.0012,  0.0068])\n",
      "params:tensor([  5.3606, -17.2646])\n",
      "Epoch 3576, Loss 2.927787\n",
      "grad:tensor([-0.0012,  0.0068])\n",
      "params:tensor([  5.3606, -17.2647])\n",
      "Epoch 3577, Loss 2.927786\n",
      "grad:tensor([-0.0012,  0.0068])\n",
      "params:tensor([  5.3606, -17.2647])\n",
      "Epoch 3578, Loss 2.927788\n",
      "grad:tensor([-0.0012,  0.0068])\n",
      "params:tensor([  5.3607, -17.2648])\n",
      "Epoch 3579, Loss 2.927785\n",
      "grad:tensor([-0.0012,  0.0068])\n",
      "params:tensor([  5.3607, -17.2649])\n",
      "Epoch 3580, Loss 2.927785\n",
      "grad:tensor([-0.0012,  0.0068])\n",
      "params:tensor([  5.3607, -17.2649])\n",
      "Epoch 3581, Loss 2.927786\n",
      "grad:tensor([-0.0012,  0.0068])\n",
      "params:tensor([  5.3607, -17.2650])\n",
      "Epoch 3582, Loss 2.927785\n",
      "grad:tensor([-0.0012,  0.0068])\n",
      "params:tensor([  5.3607, -17.2651])\n",
      "Epoch 3583, Loss 2.927784\n",
      "grad:tensor([-0.0012,  0.0067])\n",
      "params:tensor([  5.3607, -17.2651])\n",
      "Epoch 3584, Loss 2.927784\n",
      "grad:tensor([-0.0012,  0.0067])\n",
      "params:tensor([  5.3607, -17.2652])\n",
      "Epoch 3585, Loss 2.927783\n",
      "grad:tensor([-0.0012,  0.0067])\n",
      "params:tensor([  5.3607, -17.2653])\n",
      "Epoch 3586, Loss 2.927783\n",
      "grad:tensor([-0.0012,  0.0067])\n",
      "params:tensor([  5.3607, -17.2653])\n",
      "Epoch 3587, Loss 2.927781\n",
      "grad:tensor([-0.0012,  0.0067])\n",
      "params:tensor([  5.3608, -17.2654])\n",
      "Epoch 3588, Loss 2.927782\n",
      "grad:tensor([-0.0012,  0.0067])\n",
      "params:tensor([  5.3608, -17.2655])\n",
      "Epoch 3589, Loss 2.927781\n",
      "grad:tensor([-0.0012,  0.0067])\n",
      "params:tensor([  5.3608, -17.2655])\n",
      "Epoch 3590, Loss 2.927781\n",
      "grad:tensor([-0.0012,  0.0067])\n",
      "params:tensor([  5.3608, -17.2656])\n",
      "Epoch 3591, Loss 2.927781\n",
      "grad:tensor([-0.0012,  0.0067])\n",
      "params:tensor([  5.3608, -17.2657])\n",
      "Epoch 3592, Loss 2.927780\n",
      "grad:tensor([-0.0012,  0.0066])\n",
      "params:tensor([  5.3608, -17.2657])\n",
      "Epoch 3593, Loss 2.927780\n",
      "grad:tensor([-0.0012,  0.0066])\n",
      "params:tensor([  5.3608, -17.2658])\n",
      "Epoch 3594, Loss 2.927778\n",
      "grad:tensor([-0.0012,  0.0066])\n",
      "params:tensor([  5.3608, -17.2659])\n",
      "Epoch 3595, Loss 2.927779\n",
      "grad:tensor([-0.0012,  0.0066])\n",
      "params:tensor([  5.3609, -17.2659])\n",
      "Epoch 3596, Loss 2.927778\n",
      "grad:tensor([-0.0012,  0.0066])\n",
      "params:tensor([  5.3609, -17.2660])\n",
      "Epoch 3597, Loss 2.927778\n",
      "grad:tensor([-0.0012,  0.0066])\n",
      "params:tensor([  5.3609, -17.2661])\n",
      "Epoch 3598, Loss 2.927779\n",
      "grad:tensor([-0.0012,  0.0066])\n",
      "params:tensor([  5.3609, -17.2661])\n",
      "Epoch 3599, Loss 2.927777\n",
      "grad:tensor([-0.0012,  0.0066])\n",
      "params:tensor([  5.3609, -17.2662])\n",
      "Epoch 3600, Loss 2.927776\n",
      "grad:tensor([-0.0012,  0.0066])\n",
      "params:tensor([  5.3609, -17.2663])\n",
      "Epoch 3601, Loss 2.927775\n",
      "grad:tensor([-0.0012,  0.0065])\n",
      "params:tensor([  5.3609, -17.2663])\n",
      "Epoch 3602, Loss 2.927776\n",
      "grad:tensor([-0.0012,  0.0065])\n",
      "params:tensor([  5.3609, -17.2664])\n",
      "Epoch 3603, Loss 2.927773\n",
      "grad:tensor([-0.0012,  0.0065])\n",
      "params:tensor([  5.3609, -17.2665])\n",
      "Epoch 3604, Loss 2.927775\n",
      "grad:tensor([-0.0012,  0.0065])\n",
      "params:tensor([  5.3610, -17.2665])\n",
      "Epoch 3605, Loss 2.927775\n",
      "grad:tensor([-0.0011,  0.0065])\n",
      "params:tensor([  5.3610, -17.2666])\n",
      "Epoch 3606, Loss 2.927775\n",
      "grad:tensor([-0.0011,  0.0065])\n",
      "params:tensor([  5.3610, -17.2667])\n",
      "Epoch 3607, Loss 2.927773\n",
      "grad:tensor([-0.0011,  0.0065])\n",
      "params:tensor([  5.3610, -17.2667])\n",
      "Epoch 3608, Loss 2.927773\n",
      "grad:tensor([-0.0011,  0.0065])\n",
      "params:tensor([  5.3610, -17.2668])\n",
      "Epoch 3609, Loss 2.927773\n",
      "grad:tensor([-0.0011,  0.0065])\n",
      "params:tensor([  5.3610, -17.2668])\n",
      "Epoch 3610, Loss 2.927772\n",
      "grad:tensor([-0.0011,  0.0064])\n",
      "params:tensor([  5.3610, -17.2669])\n",
      "Epoch 3611, Loss 2.927772\n",
      "grad:tensor([-0.0011,  0.0064])\n",
      "params:tensor([  5.3610, -17.2670])\n",
      "Epoch 3612, Loss 2.927770\n",
      "grad:tensor([-0.0011,  0.0064])\n",
      "params:tensor([  5.3611, -17.2670])\n",
      "Epoch 3613, Loss 2.927772\n",
      "grad:tensor([-0.0011,  0.0064])\n",
      "params:tensor([  5.3611, -17.2671])\n",
      "Epoch 3614, Loss 2.927771\n",
      "grad:tensor([-0.0011,  0.0064])\n",
      "params:tensor([  5.3611, -17.2672])\n",
      "Epoch 3615, Loss 2.927770\n",
      "grad:tensor([-0.0011,  0.0064])\n",
      "params:tensor([  5.3611, -17.2672])\n",
      "Epoch 3616, Loss 2.927770\n",
      "grad:tensor([-0.0011,  0.0064])\n",
      "params:tensor([  5.3611, -17.2673])\n",
      "Epoch 3617, Loss 2.927769\n",
      "grad:tensor([-0.0011,  0.0064])\n",
      "params:tensor([  5.3611, -17.2674])\n",
      "Epoch 3618, Loss 2.927768\n",
      "grad:tensor([-0.0011,  0.0064])\n",
      "params:tensor([  5.3611, -17.2674])\n",
      "Epoch 3619, Loss 2.927769\n",
      "grad:tensor([-0.0011,  0.0064])\n",
      "params:tensor([  5.3611, -17.2675])\n",
      "Epoch 3620, Loss 2.927768\n",
      "grad:tensor([-0.0011,  0.0063])\n",
      "params:tensor([  5.3611, -17.2675])\n",
      "Epoch 3621, Loss 2.927767\n",
      "grad:tensor([-0.0011,  0.0063])\n",
      "params:tensor([  5.3612, -17.2676])\n",
      "Epoch 3622, Loss 2.927767\n",
      "grad:tensor([-0.0011,  0.0063])\n",
      "params:tensor([  5.3612, -17.2677])\n",
      "Epoch 3623, Loss 2.927767\n",
      "grad:tensor([-0.0011,  0.0063])\n",
      "params:tensor([  5.3612, -17.2677])\n",
      "Epoch 3624, Loss 2.927765\n",
      "grad:tensor([-0.0011,  0.0063])\n",
      "params:tensor([  5.3612, -17.2678])\n",
      "Epoch 3625, Loss 2.927766\n",
      "grad:tensor([-0.0011,  0.0063])\n",
      "params:tensor([  5.3612, -17.2679])\n",
      "Epoch 3626, Loss 2.927765\n",
      "grad:tensor([-0.0011,  0.0063])\n",
      "params:tensor([  5.3612, -17.2679])\n",
      "Epoch 3627, Loss 2.927765\n",
      "grad:tensor([-0.0011,  0.0063])\n",
      "params:tensor([  5.3612, -17.2680])\n",
      "Epoch 3628, Loss 2.927764\n",
      "grad:tensor([-0.0011,  0.0063])\n",
      "params:tensor([  5.3612, -17.2681])\n",
      "Epoch 3629, Loss 2.927764\n",
      "grad:tensor([-0.0011,  0.0062])\n",
      "params:tensor([  5.3612, -17.2681])\n",
      "Epoch 3630, Loss 2.927764\n",
      "grad:tensor([-0.0011,  0.0062])\n",
      "params:tensor([  5.3613, -17.2682])\n",
      "Epoch 3631, Loss 2.927762\n",
      "grad:tensor([-0.0011,  0.0062])\n",
      "params:tensor([  5.3613, -17.2682])\n",
      "Epoch 3632, Loss 2.927763\n",
      "grad:tensor([-0.0011,  0.0062])\n",
      "params:tensor([  5.3613, -17.2683])\n",
      "Epoch 3633, Loss 2.927763\n",
      "grad:tensor([-0.0011,  0.0062])\n",
      "params:tensor([  5.3613, -17.2684])\n",
      "Epoch 3634, Loss 2.927762\n",
      "grad:tensor([-0.0011,  0.0062])\n",
      "params:tensor([  5.3613, -17.2684])\n",
      "Epoch 3635, Loss 2.927761\n",
      "grad:tensor([-0.0011,  0.0062])\n",
      "params:tensor([  5.3613, -17.2685])\n",
      "Epoch 3636, Loss 2.927762\n",
      "grad:tensor([-0.0011,  0.0062])\n",
      "params:tensor([  5.3613, -17.2685])\n",
      "Epoch 3637, Loss 2.927759\n",
      "grad:tensor([-0.0011,  0.0062])\n",
      "params:tensor([  5.3613, -17.2686])\n",
      "Epoch 3638, Loss 2.927761\n",
      "grad:tensor([-0.0011,  0.0061])\n",
      "params:tensor([  5.3613, -17.2687])\n",
      "Epoch 3639, Loss 2.927761\n",
      "grad:tensor([-0.0011,  0.0061])\n",
      "params:tensor([  5.3614, -17.2687])\n",
      "Epoch 3640, Loss 2.927760\n",
      "grad:tensor([-0.0011,  0.0061])\n",
      "params:tensor([  5.3614, -17.2688])\n",
      "Epoch 3641, Loss 2.927759\n",
      "grad:tensor([-0.0011,  0.0061])\n",
      "params:tensor([  5.3614, -17.2689])\n",
      "Epoch 3642, Loss 2.927758\n",
      "grad:tensor([-0.0011,  0.0061])\n",
      "params:tensor([  5.3614, -17.2689])\n",
      "Epoch 3643, Loss 2.927759\n",
      "grad:tensor([-0.0011,  0.0061])\n",
      "params:tensor([  5.3614, -17.2690])\n",
      "Epoch 3644, Loss 2.927757\n",
      "grad:tensor([-0.0011,  0.0061])\n",
      "params:tensor([  5.3614, -17.2690])\n",
      "Epoch 3645, Loss 2.927758\n",
      "grad:tensor([-0.0011,  0.0061])\n",
      "params:tensor([  5.3614, -17.2691])\n",
      "Epoch 3646, Loss 2.927757\n",
      "grad:tensor([-0.0011,  0.0061])\n",
      "params:tensor([  5.3614, -17.2692])\n",
      "Epoch 3647, Loss 2.927757\n",
      "grad:tensor([-0.0011,  0.0061])\n",
      "params:tensor([  5.3614, -17.2692])\n",
      "Epoch 3648, Loss 2.927757\n",
      "grad:tensor([-0.0011,  0.0060])\n",
      "params:tensor([  5.3614, -17.2693])\n",
      "Epoch 3649, Loss 2.927756\n",
      "grad:tensor([-0.0011,  0.0060])\n",
      "params:tensor([  5.3615, -17.2693])\n",
      "Epoch 3650, Loss 2.927757\n",
      "grad:tensor([-0.0011,  0.0060])\n",
      "params:tensor([  5.3615, -17.2694])\n",
      "Epoch 3651, Loss 2.927756\n",
      "grad:tensor([-0.0011,  0.0060])\n",
      "params:tensor([  5.3615, -17.2695])\n",
      "Epoch 3652, Loss 2.927756\n",
      "grad:tensor([-0.0010,  0.0060])\n",
      "params:tensor([  5.3615, -17.2695])\n",
      "Epoch 3653, Loss 2.927755\n",
      "grad:tensor([-0.0010,  0.0060])\n",
      "params:tensor([  5.3615, -17.2696])\n",
      "Epoch 3654, Loss 2.927755\n",
      "grad:tensor([-0.0010,  0.0060])\n",
      "params:tensor([  5.3615, -17.2696])\n",
      "Epoch 3655, Loss 2.927754\n",
      "grad:tensor([-0.0010,  0.0060])\n",
      "params:tensor([  5.3615, -17.2697])\n",
      "Epoch 3656, Loss 2.927754\n",
      "grad:tensor([-0.0010,  0.0060])\n",
      "params:tensor([  5.3615, -17.2698])\n",
      "Epoch 3657, Loss 2.927755\n",
      "grad:tensor([-0.0010,  0.0060])\n",
      "params:tensor([  5.3615, -17.2698])\n",
      "Epoch 3658, Loss 2.927753\n",
      "grad:tensor([-0.0010,  0.0059])\n",
      "params:tensor([  5.3616, -17.2699])\n",
      "Epoch 3659, Loss 2.927752\n",
      "grad:tensor([-0.0010,  0.0059])\n",
      "params:tensor([  5.3616, -17.2699])\n",
      "Epoch 3660, Loss 2.927754\n",
      "grad:tensor([-0.0010,  0.0059])\n",
      "params:tensor([  5.3616, -17.2700])\n",
      "Epoch 3661, Loss 2.927752\n",
      "grad:tensor([-0.0010,  0.0059])\n",
      "params:tensor([  5.3616, -17.2701])\n",
      "Epoch 3662, Loss 2.927751\n",
      "grad:tensor([-0.0010,  0.0059])\n",
      "params:tensor([  5.3616, -17.2701])\n",
      "Epoch 3663, Loss 2.927752\n",
      "grad:tensor([-0.0010,  0.0059])\n",
      "params:tensor([  5.3616, -17.2702])\n",
      "Epoch 3664, Loss 2.927750\n",
      "grad:tensor([-0.0011,  0.0059])\n",
      "params:tensor([  5.3616, -17.2702])\n",
      "Epoch 3665, Loss 2.927749\n",
      "grad:tensor([-0.0010,  0.0059])\n",
      "params:tensor([  5.3616, -17.2703])\n",
      "Epoch 3666, Loss 2.927751\n",
      "grad:tensor([-0.0010,  0.0059])\n",
      "params:tensor([  5.3616, -17.2703])\n",
      "Epoch 3667, Loss 2.927750\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3616, -17.2704])\n",
      "Epoch 3668, Loss 2.927750\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3617, -17.2705])\n",
      "Epoch 3669, Loss 2.927747\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3617, -17.2705])\n",
      "Epoch 3670, Loss 2.927749\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3617, -17.2706])\n",
      "Epoch 3671, Loss 2.927747\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3617, -17.2706])\n",
      "Epoch 3672, Loss 2.927748\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3617, -17.2707])\n",
      "Epoch 3673, Loss 2.927748\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3617, -17.2708])\n",
      "Epoch 3674, Loss 2.927747\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3617, -17.2708])\n",
      "Epoch 3675, Loss 2.927747\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3617, -17.2709])\n",
      "Epoch 3676, Loss 2.927748\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3617, -17.2709])\n",
      "Epoch 3677, Loss 2.927747\n",
      "grad:tensor([-0.0010,  0.0058])\n",
      "params:tensor([  5.3617, -17.2710])\n",
      "Epoch 3678, Loss 2.927747\n",
      "grad:tensor([-0.0010,  0.0057])\n",
      "params:tensor([  5.3618, -17.2710])\n",
      "Epoch 3679, Loss 2.927745\n",
      "grad:tensor([-0.0010,  0.0057])\n",
      "params:tensor([  5.3618, -17.2711])\n",
      "Epoch 3680, Loss 2.927745\n",
      "grad:tensor([-0.0010,  0.0057])\n",
      "params:tensor([  5.3618, -17.2712])\n",
      "Epoch 3681, Loss 2.927746\n",
      "grad:tensor([-0.0010,  0.0057])\n",
      "params:tensor([  5.3618, -17.2712])\n",
      "Epoch 3682, Loss 2.927744\n",
      "grad:tensor([-0.0010,  0.0057])\n",
      "params:tensor([  5.3618, -17.2713])\n",
      "Epoch 3683, Loss 2.927743\n",
      "grad:tensor([-0.0010,  0.0057])\n",
      "params:tensor([  5.3618, -17.2713])\n",
      "Epoch 3684, Loss 2.927743\n",
      "grad:tensor([-0.0010,  0.0057])\n",
      "params:tensor([  5.3618, -17.2714])\n",
      "Epoch 3685, Loss 2.927743\n",
      "grad:tensor([-0.0010,  0.0057])\n",
      "params:tensor([  5.3618, -17.2714])\n",
      "Epoch 3686, Loss 2.927743\n",
      "grad:tensor([-0.0010,  0.0057])\n",
      "params:tensor([  5.3618, -17.2715])\n",
      "Epoch 3687, Loss 2.927743\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3618, -17.2716])\n",
      "Epoch 3688, Loss 2.927744\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3619, -17.2716])\n",
      "Epoch 3689, Loss 2.927742\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3619, -17.2717])\n",
      "Epoch 3690, Loss 2.927742\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3619, -17.2717])\n",
      "Epoch 3691, Loss 2.927742\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3619, -17.2718])\n",
      "Epoch 3692, Loss 2.927742\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3619, -17.2718])\n",
      "Epoch 3693, Loss 2.927741\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3619, -17.2719])\n",
      "Epoch 3694, Loss 2.927741\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3619, -17.2719])\n",
      "Epoch 3695, Loss 2.927741\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3619, -17.2720])\n",
      "Epoch 3696, Loss 2.927742\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3619, -17.2721])\n",
      "Epoch 3697, Loss 2.927741\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3619, -17.2721])\n",
      "Epoch 3698, Loss 2.927741\n",
      "grad:tensor([-0.0010,  0.0056])\n",
      "params:tensor([  5.3620, -17.2722])\n",
      "Epoch 3699, Loss 2.927740\n",
      "grad:tensor([-0.0010,  0.0055])\n",
      "params:tensor([  5.3620, -17.2722])\n",
      "Epoch 3700, Loss 2.927739\n",
      "grad:tensor([-0.0010,  0.0055])\n",
      "params:tensor([  5.3620, -17.2723])\n",
      "Epoch 3701, Loss 2.927738\n",
      "grad:tensor([-0.0010,  0.0055])\n",
      "params:tensor([  5.3620, -17.2723])\n",
      "Epoch 3702, Loss 2.927738\n",
      "grad:tensor([-0.0010,  0.0055])\n",
      "params:tensor([  5.3620, -17.2724])\n",
      "Epoch 3703, Loss 2.927737\n",
      "grad:tensor([-0.0010,  0.0055])\n",
      "params:tensor([  5.3620, -17.2724])\n",
      "Epoch 3704, Loss 2.927737\n",
      "grad:tensor([-0.0010,  0.0055])\n",
      "params:tensor([  5.3620, -17.2725])\n",
      "Epoch 3705, Loss 2.927738\n",
      "grad:tensor([-0.0010,  0.0055])\n",
      "params:tensor([  5.3620, -17.2726])\n",
      "Epoch 3706, Loss 2.927737\n",
      "grad:tensor([-0.0010,  0.0055])\n",
      "params:tensor([  5.3620, -17.2726])\n",
      "Epoch 3707, Loss 2.927736\n",
      "grad:tensor([-0.0010,  0.0055])\n",
      "params:tensor([  5.3620, -17.2727])\n",
      "Epoch 3708, Loss 2.927737\n",
      "grad:tensor([-0.0010,  0.0055])\n",
      "params:tensor([  5.3621, -17.2727])\n",
      "Epoch 3709, Loss 2.927737\n",
      "grad:tensor([-0.0010,  0.0054])\n",
      "params:tensor([  5.3621, -17.2728])\n",
      "Epoch 3710, Loss 2.927736\n",
      "grad:tensor([-0.0010,  0.0054])\n",
      "params:tensor([  5.3621, -17.2728])\n",
      "Epoch 3711, Loss 2.927734\n",
      "grad:tensor([-0.0010,  0.0054])\n",
      "params:tensor([  5.3621, -17.2729])\n",
      "Epoch 3712, Loss 2.927735\n",
      "grad:tensor([-0.0010,  0.0054])\n",
      "params:tensor([  5.3621, -17.2729])\n",
      "Epoch 3713, Loss 2.927735\n",
      "grad:tensor([-0.0009,  0.0054])\n",
      "params:tensor([  5.3621, -17.2730])\n",
      "Epoch 3714, Loss 2.927734\n",
      "grad:tensor([-0.0009,  0.0054])\n",
      "params:tensor([  5.3621, -17.2730])\n",
      "Epoch 3715, Loss 2.927734\n",
      "grad:tensor([-0.0009,  0.0054])\n",
      "params:tensor([  5.3621, -17.2731])\n",
      "Epoch 3716, Loss 2.927733\n",
      "grad:tensor([-0.0009,  0.0054])\n",
      "params:tensor([  5.3621, -17.2732])\n",
      "Epoch 3717, Loss 2.927734\n",
      "grad:tensor([-0.0009,  0.0054])\n",
      "params:tensor([  5.3621, -17.2732])\n",
      "Epoch 3718, Loss 2.927733\n",
      "grad:tensor([-0.0009,  0.0054])\n",
      "params:tensor([  5.3622, -17.2733])\n",
      "Epoch 3719, Loss 2.927733\n",
      "grad:tensor([-0.0009,  0.0054])\n",
      "params:tensor([  5.3622, -17.2733])\n",
      "Epoch 3720, Loss 2.927733\n",
      "grad:tensor([-0.0010,  0.0053])\n",
      "params:tensor([  5.3622, -17.2734])\n",
      "Epoch 3721, Loss 2.927732\n",
      "grad:tensor([-0.0009,  0.0053])\n",
      "params:tensor([  5.3622, -17.2734])\n",
      "Epoch 3722, Loss 2.927731\n",
      "grad:tensor([-0.0009,  0.0053])\n",
      "params:tensor([  5.3622, -17.2735])\n",
      "Epoch 3723, Loss 2.927731\n",
      "grad:tensor([-0.0009,  0.0053])\n",
      "params:tensor([  5.3622, -17.2735])\n",
      "Epoch 3724, Loss 2.927733\n",
      "grad:tensor([-0.0009,  0.0053])\n",
      "params:tensor([  5.3622, -17.2736])\n",
      "Epoch 3725, Loss 2.927730\n",
      "grad:tensor([-0.0009,  0.0053])\n",
      "params:tensor([  5.3622, -17.2736])\n",
      "Epoch 3726, Loss 2.927730\n",
      "grad:tensor([-0.0009,  0.0053])\n",
      "params:tensor([  5.3622, -17.2737])\n",
      "Epoch 3727, Loss 2.927732\n",
      "grad:tensor([-0.0009,  0.0053])\n",
      "params:tensor([  5.3622, -17.2737])\n",
      "Epoch 3728, Loss 2.927730\n",
      "grad:tensor([-0.0010,  0.0053])\n",
      "params:tensor([  5.3622, -17.2738])\n",
      "Epoch 3729, Loss 2.927732\n",
      "grad:tensor([-0.0009,  0.0053])\n",
      "params:tensor([  5.3623, -17.2738])\n",
      "Epoch 3730, Loss 2.927731\n",
      "grad:tensor([-0.0009,  0.0053])\n",
      "params:tensor([  5.3623, -17.2739])\n",
      "Epoch 3731, Loss 2.927730\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3623, -17.2740])\n",
      "Epoch 3732, Loss 2.927728\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3623, -17.2740])\n",
      "Epoch 3733, Loss 2.927729\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3623, -17.2741])\n",
      "Epoch 3734, Loss 2.927729\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3623, -17.2741])\n",
      "Epoch 3735, Loss 2.927728\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3623, -17.2742])\n",
      "Epoch 3736, Loss 2.927728\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3623, -17.2742])\n",
      "Epoch 3737, Loss 2.927728\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3623, -17.2743])\n",
      "Epoch 3738, Loss 2.927728\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3623, -17.2743])\n",
      "Epoch 3739, Loss 2.927727\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3623, -17.2744])\n",
      "Epoch 3740, Loss 2.927728\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3624, -17.2744])\n",
      "Epoch 3741, Loss 2.927728\n",
      "grad:tensor([-0.0009,  0.0052])\n",
      "params:tensor([  5.3624, -17.2745])\n",
      "Epoch 3742, Loss 2.927727\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3624, -17.2745])\n",
      "Epoch 3743, Loss 2.927727\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3624, -17.2746])\n",
      "Epoch 3744, Loss 2.927726\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3624, -17.2746])\n",
      "Epoch 3745, Loss 2.927726\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3624, -17.2747])\n",
      "Epoch 3746, Loss 2.927725\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3624, -17.2747])\n",
      "Epoch 3747, Loss 2.927725\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3624, -17.2748])\n",
      "Epoch 3748, Loss 2.927725\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3624, -17.2748])\n",
      "Epoch 3749, Loss 2.927723\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3624, -17.2749])\n",
      "Epoch 3750, Loss 2.927724\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3624, -17.2749])\n",
      "Epoch 3751, Loss 2.927724\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3625, -17.2750])\n",
      "Epoch 3752, Loss 2.927725\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3625, -17.2750])\n",
      "Epoch 3753, Loss 2.927724\n",
      "grad:tensor([-0.0009,  0.0051])\n",
      "params:tensor([  5.3625, -17.2751])\n",
      "Epoch 3754, Loss 2.927724\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3625, -17.2751])\n",
      "Epoch 3755, Loss 2.927723\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3625, -17.2752])\n",
      "Epoch 3756, Loss 2.927723\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3625, -17.2752])\n",
      "Epoch 3757, Loss 2.927723\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3625, -17.2753])\n",
      "Epoch 3758, Loss 2.927722\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3625, -17.2753])\n",
      "Epoch 3759, Loss 2.927723\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3625, -17.2754])\n",
      "Epoch 3760, Loss 2.927722\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3625, -17.2754])\n",
      "Epoch 3761, Loss 2.927723\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3625, -17.2755])\n",
      "Epoch 3762, Loss 2.927721\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3626, -17.2755])\n",
      "Epoch 3763, Loss 2.927722\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3626, -17.2756])\n",
      "Epoch 3764, Loss 2.927720\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3626, -17.2756])\n",
      "Epoch 3765, Loss 2.927720\n",
      "grad:tensor([-0.0009,  0.0050])\n",
      "params:tensor([  5.3626, -17.2757])\n",
      "Epoch 3766, Loss 2.927719\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3626, -17.2757])\n",
      "Epoch 3767, Loss 2.927721\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3626, -17.2758])\n",
      "Epoch 3768, Loss 2.927719\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3626, -17.2758])\n",
      "Epoch 3769, Loss 2.927719\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3626, -17.2759])\n",
      "Epoch 3770, Loss 2.927719\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3626, -17.2759])\n",
      "Epoch 3771, Loss 2.927719\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3626, -17.2760])\n",
      "Epoch 3772, Loss 2.927719\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3626, -17.2760])\n",
      "Epoch 3773, Loss 2.927720\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3626, -17.2761])\n",
      "Epoch 3774, Loss 2.927718\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3627, -17.2761])\n",
      "Epoch 3775, Loss 2.927718\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3627, -17.2762])\n",
      "Epoch 3776, Loss 2.927717\n",
      "grad:tensor([-0.0009,  0.0049])\n",
      "params:tensor([  5.3627, -17.2762])\n",
      "Epoch 3777, Loss 2.927718\n",
      "grad:tensor([-0.0008,  0.0049])\n",
      "params:tensor([  5.3627, -17.2763])\n",
      "Epoch 3778, Loss 2.927717\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3627, -17.2763])\n",
      "Epoch 3779, Loss 2.927717\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3627, -17.2764])\n",
      "Epoch 3780, Loss 2.927716\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3627, -17.2764])\n",
      "Epoch 3781, Loss 2.927716\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3627, -17.2765])\n",
      "Epoch 3782, Loss 2.927717\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3627, -17.2765])\n",
      "Epoch 3783, Loss 2.927717\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3627, -17.2766])\n",
      "Epoch 3784, Loss 2.927716\n",
      "grad:tensor([-0.0009,  0.0048])\n",
      "params:tensor([  5.3627, -17.2766])\n",
      "Epoch 3785, Loss 2.927715\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3627, -17.2767])\n",
      "Epoch 3786, Loss 2.927715\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3628, -17.2767])\n",
      "Epoch 3787, Loss 2.927715\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3628, -17.2767])\n",
      "Epoch 3788, Loss 2.927715\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3628, -17.2768])\n",
      "Epoch 3789, Loss 2.927715\n",
      "grad:tensor([-0.0008,  0.0048])\n",
      "params:tensor([  5.3628, -17.2768])\n",
      "Epoch 3790, Loss 2.927715\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3628, -17.2769])\n",
      "Epoch 3791, Loss 2.927714\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3628, -17.2769])\n",
      "Epoch 3792, Loss 2.927714\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3628, -17.2770])\n",
      "Epoch 3793, Loss 2.927714\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3628, -17.2770])\n",
      "Epoch 3794, Loss 2.927714\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3628, -17.2771])\n",
      "Epoch 3795, Loss 2.927713\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3628, -17.2771])\n",
      "Epoch 3796, Loss 2.927714\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3628, -17.2772])\n",
      "Epoch 3797, Loss 2.927713\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3629, -17.2772])\n",
      "Epoch 3798, Loss 2.927712\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3629, -17.2773])\n",
      "Epoch 3799, Loss 2.927712\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3629, -17.2773])\n",
      "Epoch 3800, Loss 2.927713\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3629, -17.2774])\n",
      "Epoch 3801, Loss 2.927711\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3629, -17.2774])\n",
      "Epoch 3802, Loss 2.927712\n",
      "grad:tensor([-0.0008,  0.0047])\n",
      "params:tensor([  5.3629, -17.2775])\n",
      "Epoch 3803, Loss 2.927712\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3629, -17.2775])\n",
      "Epoch 3804, Loss 2.927711\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3629, -17.2775])\n",
      "Epoch 3805, Loss 2.927712\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3629, -17.2776])\n",
      "Epoch 3806, Loss 2.927711\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3629, -17.2776])\n",
      "Epoch 3807, Loss 2.927711\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3629, -17.2777])\n",
      "Epoch 3808, Loss 2.927711\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3629, -17.2777])\n",
      "Epoch 3809, Loss 2.927709\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3629, -17.2778])\n",
      "Epoch 3810, Loss 2.927710\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3630, -17.2778])\n",
      "Epoch 3811, Loss 2.927710\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3630, -17.2779])\n",
      "Epoch 3812, Loss 2.927708\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3630, -17.2779])\n",
      "Epoch 3813, Loss 2.927708\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3630, -17.2780])\n",
      "Epoch 3814, Loss 2.927709\n",
      "grad:tensor([-0.0008,  0.0046])\n",
      "params:tensor([  5.3630, -17.2780])\n",
      "Epoch 3815, Loss 2.927709\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3630, -17.2781])\n",
      "Epoch 3816, Loss 2.927710\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3630, -17.2781])\n",
      "Epoch 3817, Loss 2.927708\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3630, -17.2781])\n",
      "Epoch 3818, Loss 2.927708\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3630, -17.2782])\n",
      "Epoch 3819, Loss 2.927706\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3630, -17.2782])\n",
      "Epoch 3820, Loss 2.927707\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3630, -17.2783])\n",
      "Epoch 3821, Loss 2.927708\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3630, -17.2783])\n",
      "Epoch 3822, Loss 2.927707\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3631, -17.2784])\n",
      "Epoch 3823, Loss 2.927707\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3631, -17.2784])\n",
      "Epoch 3824, Loss 2.927707\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3631, -17.2785])\n",
      "Epoch 3825, Loss 2.927708\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3631, -17.2785])\n",
      "Epoch 3826, Loss 2.927707\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3631, -17.2786])\n",
      "Epoch 3827, Loss 2.927706\n",
      "grad:tensor([-0.0008,  0.0045])\n",
      "params:tensor([  5.3631, -17.2786])\n",
      "Epoch 3828, Loss 2.927707\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3631, -17.2786])\n",
      "Epoch 3829, Loss 2.927705\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3631, -17.2787])\n",
      "Epoch 3830, Loss 2.927706\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3631, -17.2787])\n",
      "Epoch 3831, Loss 2.927706\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3631, -17.2788])\n",
      "Epoch 3832, Loss 2.927705\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3631, -17.2788])\n",
      "Epoch 3833, Loss 2.927705\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3631, -17.2789])\n",
      "Epoch 3834, Loss 2.927705\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3631, -17.2789])\n",
      "Epoch 3835, Loss 2.927705\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3632, -17.2789])\n",
      "Epoch 3836, Loss 2.927705\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3632, -17.2790])\n",
      "Epoch 3837, Loss 2.927705\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3632, -17.2790])\n",
      "Epoch 3838, Loss 2.927704\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3632, -17.2791])\n",
      "Epoch 3839, Loss 2.927704\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3632, -17.2791])\n",
      "Epoch 3840, Loss 2.927704\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3632, -17.2792])\n",
      "Epoch 3841, Loss 2.927703\n",
      "grad:tensor([-0.0008,  0.0044])\n",
      "params:tensor([  5.3632, -17.2792])\n",
      "Epoch 3842, Loss 2.927702\n",
      "grad:tensor([-0.0008,  0.0043])\n",
      "params:tensor([  5.3632, -17.2793])\n",
      "Epoch 3843, Loss 2.927703\n",
      "grad:tensor([-0.0008,  0.0043])\n",
      "params:tensor([  5.3632, -17.2793])\n",
      "Epoch 3844, Loss 2.927703\n",
      "grad:tensor([-0.0008,  0.0043])\n",
      "params:tensor([  5.3632, -17.2793])\n",
      "Epoch 3845, Loss 2.927704\n",
      "grad:tensor([-0.0008,  0.0043])\n",
      "params:tensor([  5.3632, -17.2794])\n",
      "Epoch 3846, Loss 2.927702\n",
      "grad:tensor([-0.0008,  0.0043])\n",
      "params:tensor([  5.3632, -17.2794])\n",
      "Epoch 3847, Loss 2.927701\n",
      "grad:tensor([-0.0008,  0.0043])\n",
      "params:tensor([  5.3632, -17.2795])\n",
      "Epoch 3848, Loss 2.927703\n",
      "grad:tensor([-0.0008,  0.0043])\n",
      "params:tensor([  5.3633, -17.2795])\n",
      "Epoch 3849, Loss 2.927702\n",
      "grad:tensor([-0.0008,  0.0043])\n",
      "params:tensor([  5.3633, -17.2796])\n",
      "Epoch 3850, Loss 2.927701\n",
      "grad:tensor([-0.0007,  0.0043])\n",
      "params:tensor([  5.3633, -17.2796])\n",
      "Epoch 3851, Loss 2.927701\n",
      "grad:tensor([-0.0007,  0.0043])\n",
      "params:tensor([  5.3633, -17.2796])\n",
      "Epoch 3852, Loss 2.927703\n",
      "grad:tensor([-0.0007,  0.0043])\n",
      "params:tensor([  5.3633, -17.2797])\n",
      "Epoch 3853, Loss 2.927700\n",
      "grad:tensor([-0.0007,  0.0043])\n",
      "params:tensor([  5.3633, -17.2797])\n",
      "Epoch 3854, Loss 2.927701\n",
      "grad:tensor([-0.0007,  0.0043])\n",
      "params:tensor([  5.3633, -17.2798])\n",
      "Epoch 3855, Loss 2.927701\n",
      "grad:tensor([-0.0007,  0.0043])\n",
      "params:tensor([  5.3633, -17.2798])\n",
      "Epoch 3856, Loss 2.927700\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3633, -17.2799])\n",
      "Epoch 3857, Loss 2.927700\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3633, -17.2799])\n",
      "Epoch 3858, Loss 2.927700\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3633, -17.2799])\n",
      "Epoch 3859, Loss 2.927701\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3633, -17.2800])\n",
      "Epoch 3860, Loss 2.927699\n",
      "grad:tensor([-0.0008,  0.0042])\n",
      "params:tensor([  5.3633, -17.2800])\n",
      "Epoch 3861, Loss 2.927699\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3634, -17.2801])\n",
      "Epoch 3862, Loss 2.927700\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3634, -17.2801])\n",
      "Epoch 3863, Loss 2.927699\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3634, -17.2801])\n",
      "Epoch 3864, Loss 2.927698\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3634, -17.2802])\n",
      "Epoch 3865, Loss 2.927699\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3634, -17.2802])\n",
      "Epoch 3866, Loss 2.927697\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3634, -17.2803])\n",
      "Epoch 3867, Loss 2.927700\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3634, -17.2803])\n",
      "Epoch 3868, Loss 2.927699\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3634, -17.2804])\n",
      "Epoch 3869, Loss 2.927698\n",
      "grad:tensor([-0.0007,  0.0042])\n",
      "params:tensor([  5.3634, -17.2804])\n",
      "Epoch 3870, Loss 2.927697\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3634, -17.2804])\n",
      "Epoch 3871, Loss 2.927698\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3634, -17.2805])\n",
      "Epoch 3872, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3634, -17.2805])\n",
      "Epoch 3873, Loss 2.927699\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3634, -17.2806])\n",
      "Epoch 3874, Loss 2.927698\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3634, -17.2806])\n",
      "Epoch 3875, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3635, -17.2806])\n",
      "Epoch 3876, Loss 2.927698\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3635, -17.2807])\n",
      "Epoch 3877, Loss 2.927697\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3635, -17.2807])\n",
      "Epoch 3878, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3635, -17.2808])\n",
      "Epoch 3879, Loss 2.927697\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3635, -17.2808])\n",
      "Epoch 3880, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3635, -17.2808])\n",
      "Epoch 3881, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3635, -17.2809])\n",
      "Epoch 3882, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3635, -17.2809])\n",
      "Epoch 3883, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0041])\n",
      "params:tensor([  5.3635, -17.2810])\n",
      "Epoch 3884, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3635, -17.2810])\n",
      "Epoch 3885, Loss 2.927695\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3635, -17.2810])\n",
      "Epoch 3886, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3635, -17.2811])\n",
      "Epoch 3887, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3635, -17.2811])\n",
      "Epoch 3888, Loss 2.927695\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3635, -17.2812])\n",
      "Epoch 3889, Loss 2.927695\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3636, -17.2812])\n",
      "Epoch 3890, Loss 2.927694\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3636, -17.2812])\n",
      "Epoch 3891, Loss 2.927693\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3636, -17.2813])\n",
      "Epoch 3892, Loss 2.927693\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3636, -17.2813])\n",
      "Epoch 3893, Loss 2.927695\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3636, -17.2814])\n",
      "Epoch 3894, Loss 2.927695\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3636, -17.2814])\n",
      "Epoch 3895, Loss 2.927694\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3636, -17.2815])\n",
      "Epoch 3896, Loss 2.927696\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3636, -17.2815])\n",
      "Epoch 3897, Loss 2.927693\n",
      "grad:tensor([-0.0007,  0.0040])\n",
      "params:tensor([  5.3636, -17.2815])\n",
      "Epoch 3898, Loss 2.927693\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3636, -17.2816])\n",
      "Epoch 3899, Loss 2.927694\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3636, -17.2816])\n",
      "Epoch 3900, Loss 2.927693\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3636, -17.2817])\n",
      "Epoch 3901, Loss 2.927692\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3636, -17.2817])\n",
      "Epoch 3902, Loss 2.927694\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3636, -17.2817])\n",
      "Epoch 3903, Loss 2.927692\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2818])\n",
      "Epoch 3904, Loss 2.927693\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2818])\n",
      "Epoch 3905, Loss 2.927691\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2818])\n",
      "Epoch 3906, Loss 2.927692\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2819])\n",
      "Epoch 3907, Loss 2.927692\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2819])\n",
      "Epoch 3908, Loss 2.927692\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2820])\n",
      "Epoch 3909, Loss 2.927691\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2820])\n",
      "Epoch 3910, Loss 2.927692\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2820])\n",
      "Epoch 3911, Loss 2.927690\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2821])\n",
      "Epoch 3912, Loss 2.927691\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2821])\n",
      "Epoch 3913, Loss 2.927691\n",
      "grad:tensor([-0.0007,  0.0039])\n",
      "params:tensor([  5.3637, -17.2822])\n",
      "Epoch 3914, Loss 2.927691\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3637, -17.2822])\n",
      "Epoch 3915, Loss 2.927690\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3637, -17.2822])\n",
      "Epoch 3916, Loss 2.927691\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3637, -17.2823])\n",
      "Epoch 3917, Loss 2.927691\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3637, -17.2823])\n",
      "Epoch 3918, Loss 2.927689\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2823])\n",
      "Epoch 3919, Loss 2.927690\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2824])\n",
      "Epoch 3920, Loss 2.927690\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2824])\n",
      "Epoch 3921, Loss 2.927690\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2825])\n",
      "Epoch 3922, Loss 2.927690\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2825])\n",
      "Epoch 3923, Loss 2.927689\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2825])\n",
      "Epoch 3924, Loss 2.927689\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2826])\n",
      "Epoch 3925, Loss 2.927689\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2826])\n",
      "Epoch 3926, Loss 2.927688\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2826])\n",
      "Epoch 3927, Loss 2.927689\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2827])\n",
      "Epoch 3928, Loss 2.927689\n",
      "grad:tensor([-0.0007,  0.0038])\n",
      "params:tensor([  5.3638, -17.2827])\n",
      "Epoch 3929, Loss 2.927688\n",
      "grad:tensor([-0.0007,  0.0037])\n",
      "params:tensor([  5.3638, -17.2828])\n",
      "Epoch 3930, Loss 2.927688\n",
      "grad:tensor([-0.0007,  0.0037])\n",
      "params:tensor([  5.3638, -17.2828])\n",
      "Epoch 3931, Loss 2.927688\n",
      "grad:tensor([-0.0007,  0.0037])\n",
      "params:tensor([  5.3638, -17.2828])\n",
      "Epoch 3932, Loss 2.927688\n",
      "grad:tensor([-0.0007,  0.0037])\n",
      "params:tensor([  5.3638, -17.2829])\n",
      "Epoch 3933, Loss 2.927687\n",
      "grad:tensor([-0.0007,  0.0037])\n",
      "params:tensor([  5.3639, -17.2829])\n",
      "Epoch 3934, Loss 2.927689\n",
      "grad:tensor([-0.0006,  0.0037])\n",
      "params:tensor([  5.3639, -17.2829])\n",
      "Epoch 3935, Loss 2.927688\n",
      "grad:tensor([-0.0006,  0.0037])\n",
      "params:tensor([  5.3639, -17.2830])\n",
      "Epoch 3936, Loss 2.927687\n",
      "grad:tensor([-0.0006,  0.0037])\n",
      "params:tensor([  5.3639, -17.2830])\n",
      "Epoch 3937, Loss 2.927687\n",
      "grad:tensor([-0.0006,  0.0037])\n",
      "params:tensor([  5.3639, -17.2831])\n",
      "Epoch 3938, Loss 2.927687\n",
      "grad:tensor([-0.0006,  0.0037])\n",
      "params:tensor([  5.3639, -17.2831])\n",
      "Epoch 3939, Loss 2.927686\n",
      "grad:tensor([-0.0006,  0.0037])\n",
      "params:tensor([  5.3639, -17.2831])\n",
      "Epoch 3940, Loss 2.927686\n",
      "grad:tensor([-0.0007,  0.0037])\n",
      "params:tensor([  5.3639, -17.2832])\n",
      "Epoch 3941, Loss 2.927687\n",
      "grad:tensor([-0.0006,  0.0037])\n",
      "params:tensor([  5.3639, -17.2832])\n",
      "Epoch 3942, Loss 2.927686\n",
      "grad:tensor([-0.0006,  0.0037])\n",
      "params:tensor([  5.3639, -17.2832])\n",
      "Epoch 3943, Loss 2.927686\n",
      "grad:tensor([-0.0006,  0.0037])\n",
      "params:tensor([  5.3639, -17.2833])\n",
      "Epoch 3944, Loss 2.927686\n",
      "grad:tensor([-0.0006,  0.0037])\n",
      "params:tensor([  5.3639, -17.2833])\n",
      "Epoch 3945, Loss 2.927686\n",
      "grad:tensor([-0.0007,  0.0036])\n",
      "params:tensor([  5.3639, -17.2833])\n",
      "Epoch 3946, Loss 2.927685\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3639, -17.2834])\n",
      "Epoch 3947, Loss 2.927685\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3639, -17.2834])\n",
      "Epoch 3948, Loss 2.927686\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3640, -17.2835])\n",
      "Epoch 3949, Loss 2.927685\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3640, -17.2835])\n",
      "Epoch 3950, Loss 2.927686\n",
      "grad:tensor([-0.0007,  0.0036])\n",
      "params:tensor([  5.3640, -17.2835])\n",
      "Epoch 3951, Loss 2.927686\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3640, -17.2836])\n",
      "Epoch 3952, Loss 2.927687\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3640, -17.2836])\n",
      "Epoch 3953, Loss 2.927685\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3640, -17.2836])\n",
      "Epoch 3954, Loss 2.927686\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3640, -17.2837])\n",
      "Epoch 3955, Loss 2.927686\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3640, -17.2837])\n",
      "Epoch 3956, Loss 2.927685\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3640, -17.2837])\n",
      "Epoch 3957, Loss 2.927683\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3640, -17.2838])\n",
      "Epoch 3958, Loss 2.927684\n",
      "grad:tensor([-0.0007,  0.0036])\n",
      "params:tensor([  5.3640, -17.2838])\n",
      "Epoch 3959, Loss 2.927685\n",
      "grad:tensor([-0.0006,  0.0036])\n",
      "params:tensor([  5.3640, -17.2839])\n",
      "Epoch 3960, Loss 2.927684\n",
      "grad:tensor([-0.0007,  0.0036])\n",
      "params:tensor([  5.3640, -17.2839])\n",
      "Epoch 3961, Loss 2.927684\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3640, -17.2839])\n",
      "Epoch 3962, Loss 2.927684\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3640, -17.2840])\n",
      "Epoch 3963, Loss 2.927685\n",
      "grad:tensor([-0.0007,  0.0035])\n",
      "params:tensor([  5.3640, -17.2840])\n",
      "Epoch 3964, Loss 2.927683\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2840])\n",
      "Epoch 3965, Loss 2.927685\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2841])\n",
      "Epoch 3966, Loss 2.927685\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2841])\n",
      "Epoch 3967, Loss 2.927684\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2841])\n",
      "Epoch 3968, Loss 2.927683\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2842])\n",
      "Epoch 3969, Loss 2.927683\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2842])\n",
      "Epoch 3970, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2842])\n",
      "Epoch 3971, Loss 2.927683\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2843])\n",
      "Epoch 3972, Loss 2.927684\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2843])\n",
      "Epoch 3973, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2843])\n",
      "Epoch 3974, Loss 2.927683\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2844])\n",
      "Epoch 3975, Loss 2.927683\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2844])\n",
      "Epoch 3976, Loss 2.927683\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2844])\n",
      "Epoch 3977, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2845])\n",
      "Epoch 3978, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0035])\n",
      "params:tensor([  5.3641, -17.2845])\n",
      "Epoch 3979, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3641, -17.2845])\n",
      "Epoch 3980, Loss 2.927681\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2846])\n",
      "Epoch 3981, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2846])\n",
      "Epoch 3982, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2847])\n",
      "Epoch 3983, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2847])\n",
      "Epoch 3984, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2847])\n",
      "Epoch 3985, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2848])\n",
      "Epoch 3986, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2848])\n",
      "Epoch 3987, Loss 2.927680\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2848])\n",
      "Epoch 3988, Loss 2.927682\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2849])\n",
      "Epoch 3989, Loss 2.927681\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2849])\n",
      "Epoch 3990, Loss 2.927681\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2849])\n",
      "Epoch 3991, Loss 2.927680\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2850])\n",
      "Epoch 3992, Loss 2.927681\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2850])\n",
      "Epoch 3993, Loss 2.927680\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2850])\n",
      "Epoch 3994, Loss 2.927680\n",
      "grad:tensor([-0.0006,  0.0034])\n",
      "params:tensor([  5.3642, -17.2851])\n",
      "Epoch 3995, Loss 2.927681\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3642, -17.2851])\n",
      "Epoch 3996, Loss 2.927681\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3642, -17.2851])\n",
      "Epoch 3997, Loss 2.927679\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2852])\n",
      "Epoch 3998, Loss 2.927680\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2852])\n",
      "Epoch 3999, Loss 2.927679\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2852])\n",
      "Epoch 4000, Loss 2.927680\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2853])\n",
      "Epoch 4001, Loss 2.927680\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2853])\n",
      "Epoch 4002, Loss 2.927681\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2853])\n",
      "Epoch 4003, Loss 2.927679\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2854])\n",
      "Epoch 4004, Loss 2.927679\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2854])\n",
      "Epoch 4005, Loss 2.927680\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2854])\n",
      "Epoch 4006, Loss 2.927680\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2855])\n",
      "Epoch 4007, Loss 2.927677\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2855])\n",
      "Epoch 4008, Loss 2.927678\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2855])\n",
      "Epoch 4009, Loss 2.927679\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2856])\n",
      "Epoch 4010, Loss 2.927678\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2856])\n",
      "Epoch 4011, Loss 2.927679\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2856])\n",
      "Epoch 4012, Loss 2.927679\n",
      "grad:tensor([-0.0006,  0.0033])\n",
      "params:tensor([  5.3643, -17.2857])\n",
      "Epoch 4013, Loss 2.927679\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3643, -17.2857])\n",
      "Epoch 4014, Loss 2.927677\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2857])\n",
      "Epoch 4015, Loss 2.927677\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2857])\n",
      "Epoch 4016, Loss 2.927677\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2858])\n",
      "Epoch 4017, Loss 2.927679\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2858])\n",
      "Epoch 4018, Loss 2.927677\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2858])\n",
      "Epoch 4019, Loss 2.927678\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2859])\n",
      "Epoch 4020, Loss 2.927678\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2859])\n",
      "Epoch 4021, Loss 2.927677\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2859])\n",
      "Epoch 4022, Loss 2.927677\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2860])\n",
      "Epoch 4023, Loss 2.927678\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2860])\n",
      "Epoch 4024, Loss 2.927677\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2860])\n",
      "Epoch 4025, Loss 2.927677\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2861])\n",
      "Epoch 4026, Loss 2.927676\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2861])\n",
      "Epoch 4027, Loss 2.927676\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2861])\n",
      "Epoch 4028, Loss 2.927675\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2862])\n",
      "Epoch 4029, Loss 2.927677\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2862])\n",
      "Epoch 4030, Loss 2.927674\n",
      "grad:tensor([-0.0006,  0.0032])\n",
      "params:tensor([  5.3644, -17.2862])\n",
      "Epoch 4031, Loss 2.927676\n",
      "grad:tensor([-0.0006,  0.0031])\n",
      "params:tensor([  5.3644, -17.2863])\n",
      "Epoch 4032, Loss 2.927675\n",
      "grad:tensor([-0.0006,  0.0031])\n",
      "params:tensor([  5.3645, -17.2863])\n",
      "Epoch 4033, Loss 2.927675\n",
      "grad:tensor([-0.0006,  0.0031])\n",
      "params:tensor([  5.3645, -17.2863])\n",
      "Epoch 4034, Loss 2.927675\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2864])\n",
      "Epoch 4035, Loss 2.927675\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2864])\n",
      "Epoch 4036, Loss 2.927674\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2864])\n",
      "Epoch 4037, Loss 2.927674\n",
      "grad:tensor([-0.0006,  0.0031])\n",
      "params:tensor([  5.3645, -17.2865])\n",
      "Epoch 4038, Loss 2.927677\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2865])\n",
      "Epoch 4039, Loss 2.927674\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2865])\n",
      "Epoch 4040, Loss 2.927676\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2865])\n",
      "Epoch 4041, Loss 2.927675\n",
      "grad:tensor([-0.0006,  0.0031])\n",
      "params:tensor([  5.3645, -17.2866])\n",
      "Epoch 4042, Loss 2.927675\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2866])\n",
      "Epoch 4043, Loss 2.927675\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2866])\n",
      "Epoch 4044, Loss 2.927674\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2867])\n",
      "Epoch 4045, Loss 2.927674\n",
      "grad:tensor([-0.0006,  0.0031])\n",
      "params:tensor([  5.3645, -17.2867])\n",
      "Epoch 4046, Loss 2.927675\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2867])\n",
      "Epoch 4047, Loss 2.927674\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2868])\n",
      "Epoch 4048, Loss 2.927674\n",
      "grad:tensor([-0.0006,  0.0031])\n",
      "params:tensor([  5.3645, -17.2868])\n",
      "Epoch 4049, Loss 2.927675\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3645, -17.2868])\n",
      "Epoch 4050, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0031])\n",
      "params:tensor([  5.3646, -17.2868])\n",
      "Epoch 4051, Loss 2.927675\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2869])\n",
      "Epoch 4052, Loss 2.927675\n",
      "grad:tensor([-0.0006,  0.0030])\n",
      "params:tensor([  5.3646, -17.2869])\n",
      "Epoch 4053, Loss 2.927673\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2869])\n",
      "Epoch 4054, Loss 2.927673\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2870])\n",
      "Epoch 4055, Loss 2.927674\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2870])\n",
      "Epoch 4056, Loss 2.927673\n",
      "grad:tensor([-0.0006,  0.0030])\n",
      "params:tensor([  5.3646, -17.2870])\n",
      "Epoch 4057, Loss 2.927674\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2871])\n",
      "Epoch 4058, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2871])\n",
      "Epoch 4059, Loss 2.927674\n",
      "grad:tensor([-0.0006,  0.0030])\n",
      "params:tensor([  5.3646, -17.2871])\n",
      "Epoch 4060, Loss 2.927675\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2872])\n",
      "Epoch 4061, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2872])\n",
      "Epoch 4062, Loss 2.927673\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2872])\n",
      "Epoch 4063, Loss 2.927675\n",
      "grad:tensor([-0.0006,  0.0030])\n",
      "params:tensor([  5.3646, -17.2872])\n",
      "Epoch 4064, Loss 2.927673\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2873])\n",
      "Epoch 4065, Loss 2.927674\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2873])\n",
      "Epoch 4066, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2873])\n",
      "Epoch 4067, Loss 2.927673\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2874])\n",
      "Epoch 4068, Loss 2.927673\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3646, -17.2874])\n",
      "Epoch 4069, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3647, -17.2874])\n",
      "Epoch 4070, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0030])\n",
      "params:tensor([  5.3647, -17.2875])\n",
      "Epoch 4071, Loss 2.927673\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2875])\n",
      "Epoch 4072, Loss 2.927673\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2875])\n",
      "Epoch 4073, Loss 2.927671\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2875])\n",
      "Epoch 4074, Loss 2.927673\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2876])\n",
      "Epoch 4075, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2876])\n",
      "Epoch 4076, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2876])\n",
      "Epoch 4077, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2877])\n",
      "Epoch 4078, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2877])\n",
      "Epoch 4079, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2877])\n",
      "Epoch 4080, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2877])\n",
      "Epoch 4081, Loss 2.927671\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2878])\n",
      "Epoch 4082, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2878])\n",
      "Epoch 4083, Loss 2.927673\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2878])\n",
      "Epoch 4084, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2879])\n",
      "Epoch 4085, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2879])\n",
      "Epoch 4086, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2879])\n",
      "Epoch 4087, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2879])\n",
      "Epoch 4088, Loss 2.927672\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3647, -17.2880])\n",
      "Epoch 4089, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3648, -17.2880])\n",
      "Epoch 4090, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0029])\n",
      "params:tensor([  5.3648, -17.2880])\n",
      "Epoch 4091, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2881])\n",
      "Epoch 4092, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2881])\n",
      "Epoch 4093, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2881])\n",
      "Epoch 4094, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2881])\n",
      "Epoch 4095, Loss 2.927669\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2882])\n",
      "Epoch 4096, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2882])\n",
      "Epoch 4097, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2882])\n",
      "Epoch 4098, Loss 2.927671\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2883])\n",
      "Epoch 4099, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2883])\n",
      "Epoch 4100, Loss 2.927671\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2883])\n",
      "Epoch 4101, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2883])\n",
      "Epoch 4102, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2884])\n",
      "Epoch 4103, Loss 2.927671\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2884])\n",
      "Epoch 4104, Loss 2.927669\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2884])\n",
      "Epoch 4105, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2885])\n",
      "Epoch 4106, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2885])\n",
      "Epoch 4107, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2885])\n",
      "Epoch 4108, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3648, -17.2885])\n",
      "Epoch 4109, Loss 2.927668\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3649, -17.2886])\n",
      "Epoch 4110, Loss 2.927669\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3649, -17.2886])\n",
      "Epoch 4111, Loss 2.927669\n",
      "grad:tensor([-0.0005,  0.0028])\n",
      "params:tensor([  5.3649, -17.2886])\n",
      "Epoch 4112, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2886])\n",
      "Epoch 4113, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2887])\n",
      "Epoch 4114, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2887])\n",
      "Epoch 4115, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2887])\n",
      "Epoch 4116, Loss 2.927669\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2887])\n",
      "Epoch 4117, Loss 2.927669\n",
      "grad:tensor([-0.0004,  0.0027])\n",
      "params:tensor([  5.3649, -17.2888])\n",
      "Epoch 4118, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2888])\n",
      "Epoch 4119, Loss 2.927669\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2888])\n",
      "Epoch 4120, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2889])\n",
      "Epoch 4121, Loss 2.927668\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2889])\n",
      "Epoch 4122, Loss 2.927668\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2889])\n",
      "Epoch 4123, Loss 2.927669\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2889])\n",
      "Epoch 4124, Loss 2.927668\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2890])\n",
      "Epoch 4125, Loss 2.927670\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2890])\n",
      "Epoch 4126, Loss 2.927666\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2890])\n",
      "Epoch 4127, Loss 2.927669\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2890])\n",
      "Epoch 4128, Loss 2.927668\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2891])\n",
      "Epoch 4129, Loss 2.927669\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3649, -17.2891])\n",
      "Epoch 4130, Loss 2.927667\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3650, -17.2891])\n",
      "Epoch 4131, Loss 2.927667\n",
      "grad:tensor([-0.0004,  0.0027])\n",
      "params:tensor([  5.3650, -17.2892])\n",
      "Epoch 4132, Loss 2.927668\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3650, -17.2892])\n",
      "Epoch 4133, Loss 2.927667\n",
      "grad:tensor([-0.0005,  0.0027])\n",
      "params:tensor([  5.3650, -17.2892])\n",
      "Epoch 4134, Loss 2.927667\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2892])\n",
      "Epoch 4135, Loss 2.927666\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2893])\n",
      "Epoch 4136, Loss 2.927666\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2893])\n",
      "Epoch 4137, Loss 2.927669\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2893])\n",
      "Epoch 4138, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0026])\n",
      "params:tensor([  5.3650, -17.2893])\n",
      "Epoch 4139, Loss 2.927668\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2894])\n",
      "Epoch 4140, Loss 2.927666\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2894])\n",
      "Epoch 4141, Loss 2.927667\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2894])\n",
      "Epoch 4142, Loss 2.927667\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2894])\n",
      "Epoch 4143, Loss 2.927666\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2895])\n",
      "Epoch 4144, Loss 2.927667\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2895])\n",
      "Epoch 4145, Loss 2.927666\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2895])\n",
      "Epoch 4146, Loss 2.927667\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2896])\n",
      "Epoch 4147, Loss 2.927667\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2896])\n",
      "Epoch 4148, Loss 2.927667\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2896])\n",
      "Epoch 4149, Loss 2.927667\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2896])\n",
      "Epoch 4150, Loss 2.927665\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3650, -17.2897])\n",
      "Epoch 4151, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0026])\n",
      "params:tensor([  5.3651, -17.2897])\n",
      "Epoch 4152, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0026])\n",
      "params:tensor([  5.3651, -17.2897])\n",
      "Epoch 4153, Loss 2.927666\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3651, -17.2897])\n",
      "Epoch 4154, Loss 2.927666\n",
      "grad:tensor([-0.0005,  0.0026])\n",
      "params:tensor([  5.3651, -17.2898])\n",
      "Epoch 4155, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0026])\n",
      "params:tensor([  5.3651, -17.2898])\n",
      "Epoch 4156, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0026])\n",
      "params:tensor([  5.3651, -17.2898])\n",
      "Epoch 4157, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2898])\n",
      "Epoch 4158, Loss 2.927665\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2899])\n",
      "Epoch 4159, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2899])\n",
      "Epoch 4160, Loss 2.927665\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2899])\n",
      "Epoch 4161, Loss 2.927664\n",
      "grad:tensor([-0.0005,  0.0025])\n",
      "params:tensor([  5.3651, -17.2899])\n",
      "Epoch 4162, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2900])\n",
      "Epoch 4163, Loss 2.927665\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2900])\n",
      "Epoch 4164, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2900])\n",
      "Epoch 4165, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2900])\n",
      "Epoch 4166, Loss 2.927665\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2901])\n",
      "Epoch 4167, Loss 2.927665\n",
      "grad:tensor([-0.0005,  0.0025])\n",
      "params:tensor([  5.3651, -17.2901])\n",
      "Epoch 4168, Loss 2.927665\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2901])\n",
      "Epoch 4169, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2901])\n",
      "Epoch 4170, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2902])\n",
      "Epoch 4171, Loss 2.927665\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2902])\n",
      "Epoch 4172, Loss 2.927666\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3651, -17.2902])\n",
      "Epoch 4173, Loss 2.927663\n",
      "grad:tensor([-0.0005,  0.0025])\n",
      "params:tensor([  5.3651, -17.2902])\n",
      "Epoch 4174, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3652, -17.2903])\n",
      "Epoch 4175, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3652, -17.2903])\n",
      "Epoch 4176, Loss 2.927665\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3652, -17.2903])\n",
      "Epoch 4177, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0025])\n",
      "params:tensor([  5.3652, -17.2903])\n",
      "Epoch 4178, Loss 2.927664\n",
      "grad:tensor([-0.0005,  0.0025])\n",
      "params:tensor([  5.3652, -17.2903])\n",
      "Epoch 4179, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2904])\n",
      "Epoch 4180, Loss 2.927663\n",
      "grad:tensor([-0.0005,  0.0024])\n",
      "params:tensor([  5.3652, -17.2904])\n",
      "Epoch 4181, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2904])\n",
      "Epoch 4182, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2904])\n",
      "Epoch 4183, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2905])\n",
      "Epoch 4184, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2905])\n",
      "Epoch 4185, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2905])\n",
      "Epoch 4186, Loss 2.927662\n",
      "grad:tensor([-0.0005,  0.0024])\n",
      "params:tensor([  5.3652, -17.2905])\n",
      "Epoch 4187, Loss 2.927665\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2906])\n",
      "Epoch 4188, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2906])\n",
      "Epoch 4189, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2906])\n",
      "Epoch 4190, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2906])\n",
      "Epoch 4191, Loss 2.927664\n",
      "grad:tensor([-0.0005,  0.0024])\n",
      "params:tensor([  5.3652, -17.2907])\n",
      "Epoch 4192, Loss 2.927664\n",
      "grad:tensor([-0.0005,  0.0024])\n",
      "params:tensor([  5.3652, -17.2907])\n",
      "Epoch 4193, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2907])\n",
      "Epoch 4194, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2907])\n",
      "Epoch 4195, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2908])\n",
      "Epoch 4196, Loss 2.927665\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3652, -17.2908])\n",
      "Epoch 4197, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3653, -17.2908])\n",
      "Epoch 4198, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3653, -17.2908])\n",
      "Epoch 4199, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3653, -17.2909])\n",
      "Epoch 4200, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3653, -17.2909])\n",
      "Epoch 4201, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3653, -17.2909])\n",
      "Epoch 4202, Loss 2.927661\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3653, -17.2909])\n",
      "Epoch 4203, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3653, -17.2910])\n",
      "Epoch 4204, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0024])\n",
      "params:tensor([  5.3653, -17.2910])\n",
      "Epoch 4205, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2910])\n",
      "Epoch 4206, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2910])\n",
      "Epoch 4207, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2910])\n",
      "Epoch 4208, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2911])\n",
      "Epoch 4209, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2911])\n",
      "Epoch 4210, Loss 2.927664\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2911])\n",
      "Epoch 4211, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2911])\n",
      "Epoch 4212, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2912])\n",
      "Epoch 4213, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2912])\n",
      "Epoch 4214, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2912])\n",
      "Epoch 4215, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2912])\n",
      "Epoch 4216, Loss 2.927661\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2913])\n",
      "Epoch 4217, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2913])\n",
      "Epoch 4218, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2913])\n",
      "Epoch 4219, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2913])\n",
      "Epoch 4220, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2913])\n",
      "Epoch 4221, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3653, -17.2914])\n",
      "Epoch 4222, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3654, -17.2914])\n",
      "Epoch 4223, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3654, -17.2914])\n",
      "Epoch 4224, Loss 2.927663\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3654, -17.2914])\n",
      "Epoch 4225, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3654, -17.2915])\n",
      "Epoch 4226, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3654, -17.2915])\n",
      "Epoch 4227, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3654, -17.2915])\n",
      "Epoch 4228, Loss 2.927661\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3654, -17.2915])\n",
      "Epoch 4229, Loss 2.927661\n",
      "grad:tensor([-0.0004,  0.0023])\n",
      "params:tensor([  5.3654, -17.2915])\n",
      "Epoch 4230, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2916])\n",
      "Epoch 4231, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2916])\n",
      "Epoch 4232, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2916])\n",
      "Epoch 4233, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2916])\n",
      "Epoch 4234, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2917])\n",
      "Epoch 4235, Loss 2.927661\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2917])\n",
      "Epoch 4236, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2917])\n",
      "Epoch 4237, Loss 2.927661\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2917])\n",
      "Epoch 4238, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2918])\n",
      "Epoch 4239, Loss 2.927661\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2918])\n",
      "Epoch 4240, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2918])\n",
      "Epoch 4241, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2918])\n",
      "Epoch 4242, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2918])\n",
      "Epoch 4243, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2919])\n",
      "Epoch 4244, Loss 2.927661\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2919])\n",
      "Epoch 4245, Loss 2.927661\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2919])\n",
      "Epoch 4246, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3654, -17.2919])\n",
      "Epoch 4247, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2920])\n",
      "Epoch 4248, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2920])\n",
      "Epoch 4249, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2920])\n",
      "Epoch 4250, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2920])\n",
      "Epoch 4251, Loss 2.927662\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2920])\n",
      "Epoch 4252, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2921])\n",
      "Epoch 4253, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2921])\n",
      "Epoch 4254, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2921])\n",
      "Epoch 4255, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2921])\n",
      "Epoch 4256, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2921])\n",
      "Epoch 4257, Loss 2.927661\n",
      "grad:tensor([-0.0004,  0.0022])\n",
      "params:tensor([  5.3655, -17.2922])\n",
      "Epoch 4258, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2922])\n",
      "Epoch 4259, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2922])\n",
      "Epoch 4260, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2922])\n",
      "Epoch 4261, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2922])\n",
      "Epoch 4262, Loss 2.927662\n",
      "grad:tensor([-0.0003,  0.0021])\n",
      "params:tensor([  5.3655, -17.2923])\n",
      "Epoch 4263, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2923])\n",
      "Epoch 4264, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2923])\n",
      "Epoch 4265, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2923])\n",
      "Epoch 4266, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2924])\n",
      "Epoch 4267, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2924])\n",
      "Epoch 4268, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2924])\n",
      "Epoch 4269, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2924])\n",
      "Epoch 4270, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2924])\n",
      "Epoch 4271, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2925])\n",
      "Epoch 4272, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2925])\n",
      "Epoch 4273, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3655, -17.2925])\n",
      "Epoch 4274, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2925])\n",
      "Epoch 4275, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2925])\n",
      "Epoch 4276, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2926])\n",
      "Epoch 4277, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2926])\n",
      "Epoch 4278, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2926])\n",
      "Epoch 4279, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2926])\n",
      "Epoch 4280, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2926])\n",
      "Epoch 4281, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2927])\n",
      "Epoch 4282, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2927])\n",
      "Epoch 4283, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2927])\n",
      "Epoch 4284, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0021])\n",
      "params:tensor([  5.3656, -17.2927])\n",
      "Epoch 4285, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2927])\n",
      "Epoch 4286, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2928])\n",
      "Epoch 4287, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2928])\n",
      "Epoch 4288, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2928])\n",
      "Epoch 4289, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2928])\n",
      "Epoch 4290, Loss 2.927657\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2929])\n",
      "Epoch 4291, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2929])\n",
      "Epoch 4292, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2929])\n",
      "Epoch 4293, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2929])\n",
      "Epoch 4294, Loss 2.927659\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3656, -17.2929])\n",
      "Epoch 4295, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2930])\n",
      "Epoch 4296, Loss 2.927657\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2930])\n",
      "Epoch 4297, Loss 2.927657\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2930])\n",
      "Epoch 4298, Loss 2.927657\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2930])\n",
      "Epoch 4299, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2930])\n",
      "Epoch 4300, Loss 2.927659\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3656, -17.2931])\n",
      "Epoch 4301, Loss 2.927660\n",
      "grad:tensor([-0.0004,  0.0020])\n",
      "params:tensor([  5.3657, -17.2931])\n",
      "Epoch 4302, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2931])\n",
      "Epoch 4303, Loss 2.927658\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2931])\n",
      "Epoch 4304, Loss 2.927658\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2931])\n",
      "Epoch 4305, Loss 2.927658\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2932])\n",
      "Epoch 4306, Loss 2.927658\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2932])\n",
      "Epoch 4307, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2932])\n",
      "Epoch 4308, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2932])\n",
      "Epoch 4309, Loss 2.927658\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2932])\n",
      "Epoch 4310, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2932])\n",
      "Epoch 4311, Loss 2.927660\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2933])\n",
      "Epoch 4312, Loss 2.927659\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2933])\n",
      "Epoch 4313, Loss 2.927658\n",
      "grad:tensor([-0.0003,  0.0020])\n",
      "params:tensor([  5.3657, -17.2933])\n",
      "Epoch 4314, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2933])\n",
      "Epoch 4315, Loss 2.927658\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2933])\n",
      "Epoch 4316, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2934])\n",
      "Epoch 4317, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2934])\n",
      "Epoch 4318, Loss 2.927658\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2934])\n",
      "Epoch 4319, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0019])\n",
      "params:tensor([  5.3657, -17.2934])\n",
      "Epoch 4320, Loss 2.927657\n",
      "grad:tensor([-0.0004,  0.0019])\n",
      "params:tensor([  5.3657, -17.2934])\n",
      "Epoch 4321, Loss 2.927656\n",
      "grad:tensor([-0.0004,  0.0019])\n",
      "params:tensor([  5.3657, -17.2935])\n",
      "Epoch 4322, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2935])\n",
      "Epoch 4323, Loss 2.927657\n",
      "grad:tensor([-0.0004,  0.0019])\n",
      "params:tensor([  5.3657, -17.2935])\n",
      "Epoch 4324, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0019])\n",
      "params:tensor([  5.3657, -17.2935])\n",
      "Epoch 4325, Loss 2.927658\n",
      "grad:tensor([-0.0004,  0.0019])\n",
      "params:tensor([  5.3657, -17.2935])\n",
      "Epoch 4326, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2936])\n",
      "Epoch 4327, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2936])\n",
      "Epoch 4328, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2936])\n",
      "Epoch 4329, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2936])\n",
      "Epoch 4330, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3657, -17.2936])\n",
      "Epoch 4331, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2936])\n",
      "Epoch 4332, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2937])\n",
      "Epoch 4333, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2937])\n",
      "Epoch 4334, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2937])\n",
      "Epoch 4335, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2937])\n",
      "Epoch 4336, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2937])\n",
      "Epoch 4337, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2938])\n",
      "Epoch 4338, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2938])\n",
      "Epoch 4339, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2938])\n",
      "Epoch 4340, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2938])\n",
      "Epoch 4341, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0019])\n",
      "params:tensor([  5.3658, -17.2938])\n",
      "Epoch 4342, Loss 2.927657\n",
      "grad:tensor([-0.0004,  0.0019])\n",
      "params:tensor([  5.3658, -17.2939])\n",
      "Epoch 4343, Loss 2.927656\n",
      "grad:tensor([-0.0004,  0.0019])\n",
      "params:tensor([  5.3658, -17.2939])\n",
      "Epoch 4344, Loss 2.927656\n",
      "grad:tensor([-0.0004,  0.0018])\n",
      "params:tensor([  5.3658, -17.2939])\n",
      "Epoch 4345, Loss 2.927657\n",
      "grad:tensor([-0.0004,  0.0018])\n",
      "params:tensor([  5.3658, -17.2939])\n",
      "Epoch 4346, Loss 2.927656\n",
      "grad:tensor([-0.0004,  0.0018])\n",
      "params:tensor([  5.3658, -17.2939])\n",
      "Epoch 4347, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2940])\n",
      "Epoch 4348, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2940])\n",
      "Epoch 4349, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2940])\n",
      "Epoch 4350, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2940])\n",
      "Epoch 4351, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2940])\n",
      "Epoch 4352, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2941])\n",
      "Epoch 4353, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2941])\n",
      "Epoch 4354, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2941])\n",
      "Epoch 4355, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2941])\n",
      "Epoch 4356, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2941])\n",
      "Epoch 4357, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2941])\n",
      "Epoch 4358, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2942])\n",
      "Epoch 4359, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2942])\n",
      "Epoch 4360, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3658, -17.2942])\n",
      "Epoch 4361, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2942])\n",
      "Epoch 4362, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2942])\n",
      "Epoch 4363, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2942])\n",
      "Epoch 4364, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2943])\n",
      "Epoch 4365, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2943])\n",
      "Epoch 4366, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2943])\n",
      "Epoch 4367, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2943])\n",
      "Epoch 4368, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2943])\n",
      "Epoch 4369, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2943])\n",
      "Epoch 4370, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2944])\n",
      "Epoch 4371, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2944])\n",
      "Epoch 4372, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2944])\n",
      "Epoch 4373, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2944])\n",
      "Epoch 4374, Loss 2.927657\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2944])\n",
      "Epoch 4375, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2945])\n",
      "Epoch 4376, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2945])\n",
      "Epoch 4377, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2945])\n",
      "Epoch 4378, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0018])\n",
      "params:tensor([  5.3659, -17.2945])\n",
      "Epoch 4379, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2945])\n",
      "Epoch 4380, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2945])\n",
      "Epoch 4381, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2946])\n",
      "Epoch 4382, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2946])\n",
      "Epoch 4383, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2946])\n",
      "Epoch 4384, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2946])\n",
      "Epoch 4385, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2946])\n",
      "Epoch 4386, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2946])\n",
      "Epoch 4387, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2947])\n",
      "Epoch 4388, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2947])\n",
      "Epoch 4389, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2947])\n",
      "Epoch 4390, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2947])\n",
      "Epoch 4391, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2947])\n",
      "Epoch 4392, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2947])\n",
      "Epoch 4393, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3659, -17.2948])\n",
      "Epoch 4394, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2948])\n",
      "Epoch 4395, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2948])\n",
      "Epoch 4396, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2948])\n",
      "Epoch 4397, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2948])\n",
      "Epoch 4398, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2948])\n",
      "Epoch 4399, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2949])\n",
      "Epoch 4400, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2949])\n",
      "Epoch 4401, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2949])\n",
      "Epoch 4402, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2949])\n",
      "Epoch 4403, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2949])\n",
      "Epoch 4404, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2949])\n",
      "Epoch 4405, Loss 2.927656\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2950])\n",
      "Epoch 4406, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2950])\n",
      "Epoch 4407, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2950])\n",
      "Epoch 4408, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2950])\n",
      "Epoch 4409, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2950])\n",
      "Epoch 4410, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2951])\n",
      "Epoch 4411, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0017])\n",
      "params:tensor([  5.3660, -17.2951])\n",
      "Epoch 4412, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2951])\n",
      "Epoch 4413, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2951])\n",
      "Epoch 4414, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2951])\n",
      "Epoch 4415, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2951])\n",
      "Epoch 4416, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2952])\n",
      "Epoch 4417, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2952])\n",
      "Epoch 4418, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2952])\n",
      "Epoch 4419, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2952])\n",
      "Epoch 4420, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2952])\n",
      "Epoch 4421, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2952])\n",
      "Epoch 4422, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2953])\n",
      "Epoch 4423, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2953])\n",
      "Epoch 4424, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2953])\n",
      "Epoch 4425, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2953])\n",
      "Epoch 4426, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2953])\n",
      "Epoch 4427, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3660, -17.2953])\n",
      "Epoch 4428, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2953])\n",
      "Epoch 4429, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2954])\n",
      "Epoch 4430, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2954])\n",
      "Epoch 4431, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2954])\n",
      "Epoch 4432, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2954])\n",
      "Epoch 4433, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2954])\n",
      "Epoch 4434, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2954])\n",
      "Epoch 4435, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2955])\n",
      "Epoch 4436, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2955])\n",
      "Epoch 4437, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2955])\n",
      "Epoch 4438, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2955])\n",
      "Epoch 4439, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2955])\n",
      "Epoch 4440, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2955])\n",
      "Epoch 4441, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2955])\n",
      "Epoch 4442, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2956])\n",
      "Epoch 4443, Loss 2.927653\n",
      "grad:tensor([-0.0002,  0.0016])\n",
      "params:tensor([  5.3661, -17.2956])\n",
      "Epoch 4444, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2956])\n",
      "Epoch 4445, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2956])\n",
      "Epoch 4446, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2956])\n",
      "Epoch 4447, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2956])\n",
      "Epoch 4448, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2957])\n",
      "Epoch 4449, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0016])\n",
      "params:tensor([  5.3661, -17.2957])\n",
      "Epoch 4450, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2957])\n",
      "Epoch 4451, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2957])\n",
      "Epoch 4452, Loss 2.927651\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2957])\n",
      "Epoch 4453, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2957])\n",
      "Epoch 4454, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2957])\n",
      "Epoch 4455, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2958])\n",
      "Epoch 4456, Loss 2.927655\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2958])\n",
      "Epoch 4457, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2958])\n",
      "Epoch 4458, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2958])\n",
      "Epoch 4459, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2958])\n",
      "Epoch 4460, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2958])\n",
      "Epoch 4461, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2959])\n",
      "Epoch 4462, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2959])\n",
      "Epoch 4463, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2959])\n",
      "Epoch 4464, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3661, -17.2959])\n",
      "Epoch 4465, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2959])\n",
      "Epoch 4466, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2959])\n",
      "Epoch 4467, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2959])\n",
      "Epoch 4468, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2960])\n",
      "Epoch 4469, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2960])\n",
      "Epoch 4470, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2960])\n",
      "Epoch 4471, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2960])\n",
      "Epoch 4472, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2960])\n",
      "Epoch 4473, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2960])\n",
      "Epoch 4474, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2960])\n",
      "Epoch 4475, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2961])\n",
      "Epoch 4476, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2961])\n",
      "Epoch 4477, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2961])\n",
      "Epoch 4478, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2961])\n",
      "Epoch 4479, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2961])\n",
      "Epoch 4480, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2961])\n",
      "Epoch 4481, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2962])\n",
      "Epoch 4482, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2962])\n",
      "Epoch 4483, Loss 2.927654\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2962])\n",
      "Epoch 4484, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2962])\n",
      "Epoch 4485, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2962])\n",
      "Epoch 4486, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2962])\n",
      "Epoch 4487, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0015])\n",
      "params:tensor([  5.3662, -17.2962])\n",
      "Epoch 4488, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0014])\n",
      "params:tensor([  5.3662, -17.2963])\n",
      "Epoch 4489, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0014])\n",
      "params:tensor([  5.3662, -17.2963])\n",
      "Epoch 4490, Loss 2.927652\n",
      "grad:tensor([-0.0003,  0.0014])\n",
      "params:tensor([  5.3662, -17.2963])\n",
      "Epoch 4491, Loss 2.927651\n",
      "grad:tensor([-0.0003,  0.0014])\n",
      "params:tensor([  5.3662, -17.2963])\n",
      "Epoch 4492, Loss 2.927651\n",
      "grad:tensor([-0.0003,  0.0014])\n",
      "params:tensor([  5.3662, -17.2963])\n",
      "Epoch 4493, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0014])\n",
      "params:tensor([  5.3662, -17.2963])\n",
      "Epoch 4494, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0014])\n",
      "params:tensor([  5.3662, -17.2964])\n",
      "Epoch 4495, Loss 2.927653\n",
      "grad:tensor([-0.0003,  0.0014])\n",
      "params:tensor([  5.3662, -17.2964])\n",
      "Epoch 4496, Loss 2.927651\n",
      "grad:tensor([-0.0003,  0.0014])\n",
      "params:tensor([  5.3662, -17.2964])\n",
      "Epoch 4497, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3662, -17.2964])\n",
      "Epoch 4498, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3662, -17.2964])\n",
      "Epoch 4499, Loss 2.927653\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3662, -17.2964])\n",
      "Epoch 4500, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3662, -17.2964])\n",
      "Epoch 4501, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3662, -17.2964])\n",
      "Epoch 4502, Loss 2.927653\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3662, -17.2965])\n",
      "Epoch 4503, Loss 2.927653\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2965])\n",
      "Epoch 4504, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2965])\n",
      "Epoch 4505, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2965])\n",
      "Epoch 4506, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2965])\n",
      "Epoch 4507, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2965])\n",
      "Epoch 4508, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2965])\n",
      "Epoch 4509, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2966])\n",
      "Epoch 4510, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2966])\n",
      "Epoch 4511, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2966])\n",
      "Epoch 4512, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2966])\n",
      "Epoch 4513, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2966])\n",
      "Epoch 4514, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2966])\n",
      "Epoch 4515, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2966])\n",
      "Epoch 4516, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2966])\n",
      "Epoch 4517, Loss 2.927653\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2967])\n",
      "Epoch 4518, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2967])\n",
      "Epoch 4519, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2967])\n",
      "Epoch 4520, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2967])\n",
      "Epoch 4521, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2967])\n",
      "Epoch 4522, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2967])\n",
      "Epoch 4523, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2967])\n",
      "Epoch 4524, Loss 2.927653\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2968])\n",
      "Epoch 4525, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2968])\n",
      "Epoch 4526, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2968])\n",
      "Epoch 4527, Loss 2.927653\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2968])\n",
      "Epoch 4528, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2968])\n",
      "Epoch 4529, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2968])\n",
      "Epoch 4530, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2968])\n",
      "Epoch 4531, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0014])\n",
      "params:tensor([  5.3663, -17.2969])\n",
      "Epoch 4532, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2969])\n",
      "Epoch 4533, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2969])\n",
      "Epoch 4534, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2969])\n",
      "Epoch 4535, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2969])\n",
      "Epoch 4536, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2969])\n",
      "Epoch 4537, Loss 2.927653\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2969])\n",
      "Epoch 4538, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2969])\n",
      "Epoch 4539, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2970])\n",
      "Epoch 4540, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2970])\n",
      "Epoch 4541, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2970])\n",
      "Epoch 4542, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2970])\n",
      "Epoch 4543, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2970])\n",
      "Epoch 4544, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3663, -17.2970])\n",
      "Epoch 4545, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2970])\n",
      "Epoch 4546, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2971])\n",
      "Epoch 4547, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2971])\n",
      "Epoch 4548, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2971])\n",
      "Epoch 4549, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2971])\n",
      "Epoch 4550, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2971])\n",
      "Epoch 4551, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2971])\n",
      "Epoch 4552, Loss 2.927653\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2971])\n",
      "Epoch 4553, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2971])\n",
      "Epoch 4554, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2972])\n",
      "Epoch 4555, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2972])\n",
      "Epoch 4556, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2972])\n",
      "Epoch 4557, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2972])\n",
      "Epoch 4558, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2972])\n",
      "Epoch 4559, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2972])\n",
      "Epoch 4560, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2972])\n",
      "Epoch 4561, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2973])\n",
      "Epoch 4562, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2973])\n",
      "Epoch 4563, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2973])\n",
      "Epoch 4564, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2973])\n",
      "Epoch 4565, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2973])\n",
      "Epoch 4566, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2973])\n",
      "Epoch 4567, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2973])\n",
      "Epoch 4568, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2973])\n",
      "Epoch 4569, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2974])\n",
      "Epoch 4570, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2974])\n",
      "Epoch 4571, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2974])\n",
      "Epoch 4572, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2974])\n",
      "Epoch 4573, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0013])\n",
      "params:tensor([  5.3664, -17.2974])\n",
      "Epoch 4574, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2974])\n",
      "Epoch 4575, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2974])\n",
      "Epoch 4576, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2975])\n",
      "Epoch 4577, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2975])\n",
      "Epoch 4578, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2975])\n",
      "Epoch 4579, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2975])\n",
      "Epoch 4580, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2975])\n",
      "Epoch 4581, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2975])\n",
      "Epoch 4582, Loss 2.927653\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2975])\n",
      "Epoch 4583, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2975])\n",
      "Epoch 4584, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2976])\n",
      "Epoch 4585, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2976])\n",
      "Epoch 4586, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2976])\n",
      "Epoch 4587, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2976])\n",
      "Epoch 4588, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3664, -17.2976])\n",
      "Epoch 4589, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2976])\n",
      "Epoch 4590, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2976])\n",
      "Epoch 4591, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2976])\n",
      "Epoch 4592, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2976])\n",
      "Epoch 4593, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2977])\n",
      "Epoch 4594, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2977])\n",
      "Epoch 4595, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2977])\n",
      "Epoch 4596, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2977])\n",
      "Epoch 4597, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2977])\n",
      "Epoch 4598, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2977])\n",
      "Epoch 4599, Loss 2.927652\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2977])\n",
      "Epoch 4600, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2977])\n",
      "Epoch 4601, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2977])\n",
      "Epoch 4602, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2978])\n",
      "Epoch 4603, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2978])\n",
      "Epoch 4604, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2978])\n",
      "Epoch 4605, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2978])\n",
      "Epoch 4606, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2978])\n",
      "Epoch 4607, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2978])\n",
      "Epoch 4608, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2978])\n",
      "Epoch 4609, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2978])\n",
      "Epoch 4610, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2978])\n",
      "Epoch 4611, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2979])\n",
      "Epoch 4612, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2979])\n",
      "Epoch 4613, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2979])\n",
      "Epoch 4614, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2979])\n",
      "Epoch 4615, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2979])\n",
      "Epoch 4616, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2979])\n",
      "Epoch 4617, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2979])\n",
      "Epoch 4618, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2979])\n",
      "Epoch 4619, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2980])\n",
      "Epoch 4620, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2980])\n",
      "Epoch 4621, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2980])\n",
      "Epoch 4622, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2980])\n",
      "Epoch 4623, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2980])\n",
      "Epoch 4624, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2980])\n",
      "Epoch 4625, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2980])\n",
      "Epoch 4626, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0012])\n",
      "params:tensor([  5.3665, -17.2980])\n",
      "Epoch 4627, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2980])\n",
      "Epoch 4628, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2981])\n",
      "Epoch 4629, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2981])\n",
      "Epoch 4630, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2981])\n",
      "Epoch 4631, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2981])\n",
      "Epoch 4632, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2981])\n",
      "Epoch 4633, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2981])\n",
      "Epoch 4634, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2981])\n",
      "Epoch 4635, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2981])\n",
      "Epoch 4636, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2981])\n",
      "Epoch 4637, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2982])\n",
      "Epoch 4638, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3665, -17.2982])\n",
      "Epoch 4639, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2982])\n",
      "Epoch 4640, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2982])\n",
      "Epoch 4641, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2982])\n",
      "Epoch 4642, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2982])\n",
      "Epoch 4643, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2982])\n",
      "Epoch 4644, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2982])\n",
      "Epoch 4645, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2982])\n",
      "Epoch 4646, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2983])\n",
      "Epoch 4647, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2983])\n",
      "Epoch 4648, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2983])\n",
      "Epoch 4649, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2983])\n",
      "Epoch 4650, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2983])\n",
      "Epoch 4651, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2983])\n",
      "Epoch 4652, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2983])\n",
      "Epoch 4653, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2983])\n",
      "Epoch 4654, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2984])\n",
      "Epoch 4655, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2984])\n",
      "Epoch 4656, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2984])\n",
      "Epoch 4657, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2984])\n",
      "Epoch 4658, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2984])\n",
      "Epoch 4659, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2984])\n",
      "Epoch 4660, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2984])\n",
      "Epoch 4661, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2984])\n",
      "Epoch 4662, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2984])\n",
      "Epoch 4663, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2985])\n",
      "Epoch 4664, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2985])\n",
      "Epoch 4665, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2985])\n",
      "Epoch 4666, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2985])\n",
      "Epoch 4667, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2985])\n",
      "Epoch 4668, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2985])\n",
      "Epoch 4669, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2985])\n",
      "Epoch 4670, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2985])\n",
      "Epoch 4671, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2985])\n",
      "Epoch 4672, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2986])\n",
      "Epoch 4673, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2986])\n",
      "Epoch 4674, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2986])\n",
      "Epoch 4675, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0011])\n",
      "params:tensor([  5.3666, -17.2986])\n",
      "Epoch 4676, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2986])\n",
      "Epoch 4677, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2986])\n",
      "Epoch 4678, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2986])\n",
      "Epoch 4679, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2986])\n",
      "Epoch 4680, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2986])\n",
      "Epoch 4681, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2987])\n",
      "Epoch 4682, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2987])\n",
      "Epoch 4683, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2987])\n",
      "Epoch 4684, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2987])\n",
      "Epoch 4685, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2987])\n",
      "Epoch 4686, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2987])\n",
      "Epoch 4687, Loss 2.927651\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2987])\n",
      "Epoch 4688, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2987])\n",
      "Epoch 4689, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3666, -17.2987])\n",
      "Epoch 4690, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2987])\n",
      "Epoch 4691, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2987])\n",
      "Epoch 4692, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2988])\n",
      "Epoch 4693, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2988])\n",
      "Epoch 4694, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2988])\n",
      "Epoch 4695, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2988])\n",
      "Epoch 4696, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2988])\n",
      "Epoch 4697, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2988])\n",
      "Epoch 4698, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2988])\n",
      "Epoch 4699, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2988])\n",
      "Epoch 4700, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2988])\n",
      "Epoch 4701, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2988])\n",
      "Epoch 4702, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4703, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4704, Loss 2.927647\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4705, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4706, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4707, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4708, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4709, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4710, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4711, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4712, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2989])\n",
      "Epoch 4713, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2990])\n",
      "Epoch 4714, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2990])\n",
      "Epoch 4715, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2990])\n",
      "Epoch 4716, Loss 2.927647\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2990])\n",
      "Epoch 4717, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2990])\n",
      "Epoch 4718, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2990])\n",
      "Epoch 4719, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2990])\n",
      "Epoch 4720, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2990])\n",
      "Epoch 4721, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2990])\n",
      "Epoch 4722, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2990])\n",
      "Epoch 4723, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4724, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4725, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4726, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4727, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4728, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4729, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4730, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4731, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4732, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4733, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2991])\n",
      "Epoch 4734, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2992])\n",
      "Epoch 4735, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2992])\n",
      "Epoch 4736, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2992])\n",
      "Epoch 4737, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2992])\n",
      "Epoch 4738, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0010])\n",
      "params:tensor([  5.3667, -17.2992])\n",
      "Epoch 4739, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2992])\n",
      "Epoch 4740, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2992])\n",
      "Epoch 4741, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2992])\n",
      "Epoch 4742, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2992])\n",
      "Epoch 4743, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2992])\n",
      "Epoch 4744, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2993])\n",
      "Epoch 4745, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2993])\n",
      "Epoch 4746, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2993])\n",
      "Epoch 4747, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2993])\n",
      "Epoch 4748, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2993])\n",
      "Epoch 4749, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3667, -17.2993])\n",
      "Epoch 4750, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2993])\n",
      "Epoch 4751, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2993])\n",
      "Epoch 4752, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2993])\n",
      "Epoch 4753, Loss 2.927647\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2993])\n",
      "Epoch 4754, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2993])\n",
      "Epoch 4755, Loss 2.927647\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2994])\n",
      "Epoch 4756, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2994])\n",
      "Epoch 4757, Loss 2.927647\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2994])\n",
      "Epoch 4758, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2994])\n",
      "Epoch 4759, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2994])\n",
      "Epoch 4760, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2994])\n",
      "Epoch 4761, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2994])\n",
      "Epoch 4762, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2994])\n",
      "Epoch 4763, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2994])\n",
      "Epoch 4764, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0009])\n",
      "params:tensor([  5.3668, -17.2994])\n",
      "Epoch 4765, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4766, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4767, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4768, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4769, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4770, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4771, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4772, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4773, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4774, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4775, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2995])\n",
      "Epoch 4776, Loss 2.927647\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2996])\n",
      "Epoch 4777, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2996])\n",
      "Epoch 4778, Loss 2.927647\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2996])\n",
      "Epoch 4779, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2996])\n",
      "Epoch 4780, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2996])\n",
      "Epoch 4781, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2996])\n",
      "Epoch 4782, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2996])\n",
      "Epoch 4783, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2996])\n",
      "Epoch 4784, Loss 2.927649\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2996])\n",
      "Epoch 4785, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2996])\n",
      "Epoch 4786, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4787, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4788, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4789, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4790, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4791, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4792, Loss 2.927647\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4793, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4794, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4795, Loss 2.927650\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4796, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4797, Loss 2.927648\n",
      "grad:tensor([-0.0002,  0.0009])\n",
      "params:tensor([  5.3668, -17.2997])\n",
      "Epoch 4798, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0009])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4799, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0009])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4800, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0009])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4801, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0009])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4802, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0009])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4803, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0009])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4804, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4805, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4806, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4807, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4808, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4809, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4810, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3668, -17.2998])\n",
      "Epoch 4811, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3668, -17.2999])\n",
      "Epoch 4812, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3668, -17.2999])\n",
      "Epoch 4813, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4814, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4815, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4816, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4817, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4818, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4819, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4820, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4821, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4822, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4823, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.2999])\n",
      "Epoch 4824, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4825, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4826, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4827, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4828, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4829, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4830, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4831, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4832, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4833, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4834, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4835, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4836, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3000])\n",
      "Epoch 4837, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4838, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4839, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4840, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4841, Loss 2.927650\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4842, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4843, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4844, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4845, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4846, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4847, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4848, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4849, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3001])\n",
      "Epoch 4850, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4851, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4852, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4853, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4854, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4855, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4856, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4857, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4858, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4859, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4860, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4861, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4862, Loss 2.927645\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3002])\n",
      "Epoch 4863, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4864, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4865, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4866, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4867, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4868, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4869, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4870, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4871, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4872, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4873, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4874, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4875, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3003])\n",
      "Epoch 4876, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4877, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4878, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4879, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4880, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4881, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0008])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4882, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4883, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4884, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4885, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4886, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3669, -17.3004])\n",
      "Epoch 4887, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3004])\n",
      "Epoch 4888, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3004])\n",
      "Epoch 4889, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4890, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4891, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4892, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4893, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4894, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4895, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4896, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4897, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4898, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4899, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4900, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4901, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3005])\n",
      "Epoch 4902, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4903, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4904, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4905, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4906, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4907, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4908, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4909, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4910, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4911, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4912, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4913, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4914, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4915, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3006])\n",
      "Epoch 4916, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4917, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4918, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4919, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4920, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4921, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4922, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4923, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4924, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4925, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4926, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4927, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4928, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3007])\n",
      "Epoch 4929, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4930, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4931, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4932, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4933, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4934, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4935, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4936, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4937, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4938, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4939, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4940, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4941, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3008])\n",
      "Epoch 4942, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4943, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4944, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4945, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4946, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4947, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4948, Loss 2.927649\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4949, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4950, Loss 2.927648\n",
      "grad:tensor([-9.9361e-05,  6.6355e-04])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4951, Loss 2.927646\n",
      "grad:tensor([-9.5367e-05,  6.6292e-04])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4952, Loss 2.927646\n",
      "grad:tensor([-9.6858e-05,  6.6188e-04])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4953, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4954, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4955, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4956, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4957, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4958, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3009])\n",
      "Epoch 4959, Loss 2.927648\n",
      "grad:tensor([-9.8228e-05,  6.5479e-04])\n",
      "params:tensor([  5.3670, -17.3010])\n",
      "Epoch 4960, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3010])\n",
      "Epoch 4961, Loss 2.927648\n",
      "grad:tensor([-9.8288e-05,  6.5270e-04])\n",
      "params:tensor([  5.3670, -17.3010])\n",
      "Epoch 4962, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0007])\n",
      "params:tensor([  5.3670, -17.3010])\n",
      "Epoch 4963, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3670, -17.3010])\n",
      "Epoch 4964, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3670, -17.3010])\n",
      "Epoch 4965, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3670, -17.3010])\n",
      "Epoch 4966, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3670, -17.3010])\n",
      "Epoch 4967, Loss 2.927646\n",
      "grad:tensor([-9.7990e-05,  6.4683e-04])\n",
      "params:tensor([  5.3671, -17.3010])\n",
      "Epoch 4968, Loss 2.927647\n",
      "grad:tensor([-9.7573e-05,  6.4588e-04])\n",
      "params:tensor([  5.3671, -17.3010])\n",
      "Epoch 4969, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3010])\n",
      "Epoch 4970, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3010])\n",
      "Epoch 4971, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3010])\n",
      "Epoch 4972, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3010])\n",
      "Epoch 4973, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3010])\n",
      "Epoch 4974, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3010])\n",
      "Epoch 4975, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3010])\n",
      "Epoch 4976, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4977, Loss 2.927647\n",
      "grad:tensor([-9.5606e-05,  6.3694e-04])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4978, Loss 2.927648\n",
      "grad:tensor([-9.8169e-05,  6.3545e-04])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4979, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4980, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4981, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4982, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4983, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4984, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4985, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4986, Loss 2.927648\n",
      "grad:tensor([-9.6440e-05,  6.2808e-04])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4987, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4988, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4989, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4990, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4991, Loss 2.927646\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4992, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4993, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3011])\n",
      "Epoch 4994, Loss 2.927646\n",
      "grad:tensor([-9.2626e-05,  6.2042e-04])\n",
      "params:tensor([  5.3671, -17.3012])\n",
      "Epoch 4995, Loss 2.927647\n",
      "grad:tensor([-9.8884e-05,  6.1837e-04])\n",
      "params:tensor([  5.3671, -17.3012])\n",
      "Epoch 4996, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3012])\n",
      "Epoch 4997, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3012])\n",
      "Epoch 4998, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3012])\n",
      "Epoch 4999, Loss 2.927647\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3012])\n",
      "Epoch 5000, Loss 2.927648\n",
      "grad:tensor([-0.0001,  0.0006])\n",
      "params:tensor([  5.3671, -17.3012])\n"
     ]
    }
   ],
   "source": [
    "params = training_loop(\n",
    "    n_epochs = 5000 ,\n",
    "    learning_rate = 1e-2,\n",
    "    params = torch.tensor([1.0, 0.0]),\n",
    "    t_u = t_un,\n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10296134-fa74-4ee4-8f92-749ab572cc44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6176cb-ae7b-4c59-a0ad-23c3a3a89c6c",
   "metadata": {},
   "source": [
    "5000回ループしてもしっかり損失が減少していっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9e6f978-ed9e-4d51-b036-1662df4d0663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.8593, 12.7008, 13.9352, 26.6552, 12.9155,  8.9439,  0.8932, -5.6009,\n",
       "         8.6755, 15.1160, 19.4097])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_un, *params)\n",
    "t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a496fa9-52ef-45bf-aae6-536437d1f512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9bc7bf8b20>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.xlabel(\"Temperature (°Fahrenheit)\")\n",
    "plt.ylabel(\"Temperature (°Celsius)\")\n",
    "plt.plot(t_u.numpy(),  t_p.detach().numpy())#予測\n",
    "plt.plot(t_u.numpy(),  t_c.numpy(), 'o')#答え\n",
    "#plt.savefig(\"temp_unknown_plot.png\", format=\"png\")  # 保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea95b6-ced5-4d7a-bb26-7c1ff1419217",
   "metadata": {},
   "source": [
    "微分の連鎖律を通して導関数を後方に伝播させることで、\n",
    "モデルと損失関数から構成される合成関数の内部パラメータに関する勾配wを計算した"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f017f87e-e07a-410a-9dbb-f59fdbcfd3bf",
   "metadata": {},
   "source": [
    "## 勾配の自動計算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49060783-fed5-4498-8b3c-9d174704b6aa",
   "metadata": {},
   "source": [
    "pytorchのautogradは導関数の連鎖を自動的に計算する\\n\n",
    "手計算でモデルの導関数を求める必要がない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82b87745-4676-4ad2-9971-d108b5eec344",
   "metadata": {},
   "outputs": [],
   "source": [
    "#再度定義\n",
    "def model(t_u, w, b):\n",
    "    return w * t_u + b\n",
    "\n",
    "def loss_fn(t_p, t_c):\n",
    "    squared_diffs = (t_p - t_c)**2\n",
    "    return squared_diffs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8e0752a-bea5-4f0b-aa58-e72fb95d9348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#求めたいパラメータのテンソル変数を初期化\n",
    "\n",
    "#requires_gradはテンソルのツリーすべてを追跡して微分可能にする\n",
    "#この＝Trueにしたテンソルを起点にモデルを呼び出し、損失を計算して最後にbackward（逆伝播）を呼び出す\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4fcdd098-ff4f-4c6a-853b-83a75a3aeff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2659534b-648d-4d76-b133-767199904662",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c9f3c31-fcfd-4995-978f-14fae548864f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4517.2969,   82.6000])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.grad\n",
    "#損失の導関数が計算される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "78da05e7-c05f-444b-96f5-83d5bb779a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramsの導関数の計算結果は保持されており、backwardを実行すると累積（合計）されてしまう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19b1acd8-38ee-4d29-9686-4425a8aea1b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9034.5938,  165.2000])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#二回目\n",
    "loss = loss_fn(model(t_u, *params), t_c)\n",
    "loss.backward()\n",
    "\n",
    "params.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a9b4459-40d0-42be-bf5f-c2e658ddf071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#明示的に勾配を０にする必要がある下記が必要\n",
    "\n",
    "if params.grad is not None:\n",
    "    params.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4f1969e1-b131-40a2-af1b-e49e5c3644cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        if params.grad is not None:  # <1>\n",
    "            params.grad.zero_()\n",
    "        \n",
    "        t_p = model(t_u, *params) \n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():  # <2>\n",
    "            params -= learning_rate * params.grad\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c37c9e2-e1ed-41fe-8b67-cb0488a4d26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860115\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loop(\n",
    "    n_epochs = 5000, \n",
    "    learning_rate = 1e-2, \n",
    "    params = torch.tensor([1.0, 0.0], requires_grad=True), # <1> \n",
    "    t_u = t_un, # <2> \n",
    "    t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b499633-35f4-4f56-9683-a34b22ef6b8e",
   "metadata": {},
   "source": [
    "様々な最適化関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "78624737-a3f7-4240-baf3-c055045ce641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASGD',\n",
       " 'Adadelta',\n",
       " 'Adagrad',\n",
       " 'Adam',\n",
       " 'AdamW',\n",
       " 'Adamax',\n",
       " 'LBFGS',\n",
       " 'NAdam',\n",
       " 'Optimizer',\n",
       " 'RAdam',\n",
       " 'RMSprop',\n",
       " 'Rprop',\n",
       " 'SGD',\n",
       " 'SparseAdam',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_functional',\n",
       " '_multi_tensor',\n",
       " 'lr_scheduler',\n",
       " 'swa_utils']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "dir(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7bc328-ab74-432d-9669-f7f2fb6f34ac",
   "metadata": {},
   "source": [
    "全て　required_grad=Trueのパラメータのリストを受け取る\n",
    "\n",
    "\n",
    "オプティマイザのオブジェクトは内部にパラメータを保持し、パラメータのgradにアクセスできる\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19101214-00cc-48d3-9229-6ce14da32f1b",
   "metadata": {},
   "source": [
    "各オプティマイザクラスはzero_gradとstepのメソッドを公開しており、\n",
    "\n",
    "zero_gradで構築時に渡しているパラメータのgradを0にする\n",
    "\n",
    "stepは各パラメータの更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "645f00b6-752a-446d-afe7-fafc506a2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad = True)\n",
    "learning_rate =1e-5\n",
    "optimizer = optim.SGD([params],lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8612d744-cf04-456d-b468-93488b773343",
   "metadata": {},
   "source": [
    "step時にgradを見て、gradの更新、paramsの値に学習率をかけたgradの値を引き算する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64e90061-af67-4f35-b329-5419bf6a3a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0008e+00, 1.0640e-04], requires_grad=True)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_p = model(t_u, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "t_p = model(t_un, *params)\n",
    "loss = loss_fn(t_p, t_c)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "#backward時に勾配が累積されるため、直前にzero_grad\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b01e157e-128a-4b13-9c9a-8a0972abef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#手動の最適化をモジュールで\n",
    "def training_loop(n_epochs, optimizer, params, t_u, t_c):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        t_p = model(t_u, *params)\n",
    "        loss = loss_fn(t_p, t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 500 ==0:\n",
    "            print('Epoch %d, Loss %f'%(epoch, float(loss)))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c94214f2-9f3a-48e9-aa49-2bbf46a76e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.860120\n",
      "Epoch 1000, Loss 3.828538\n",
      "Epoch 1500, Loss 3.092191\n",
      "Epoch 2000, Loss 2.957698\n",
      "Epoch 2500, Loss 2.933134\n",
      "Epoch 3000, Loss 2.928648\n",
      "Epoch 3500, Loss 2.927830\n",
      "Epoch 4000, Loss 2.927679\n",
      "Epoch 4500, Loss 2.927652\n",
      "Epoch 5000, Loss 2.927647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3671, -17.3012], requires_grad=True)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "training_loop(n_epochs=5000,\n",
    "             optimizer = optimizer,\n",
    "             params = params,\n",
    "             t_u = t_un,\n",
    "             t_c = t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "029aeb48-b02b-4797-b622-901eee6e3bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500, Loss 7.612900\n",
      "Epoch 1000, Loss 3.086700\n",
      "Epoch 1500, Loss 2.928579\n",
      "Epoch 2000, Loss 2.927644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  0.5367, -17.3021], requires_grad=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#他のオプティマイザを使用\n",
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-1\n",
    "optimizer = optim.Adam([params], lr=learning_rate)\n",
    "training_loop(n_epochs=2000,\n",
    "             optimizer = optimizer,\n",
    "             params = params,\n",
    "             t_u = t_u,\n",
    "             t_c = t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6174ecaa-5cea-40bd-af0d-e0604f0553ca",
   "metadata": {},
   "source": [
    "線形変換で解決できる問題だったため、モデルの変更は今回無意味だが、\n",
    "オプティマイザ以外にもモデル関数の変更も行うことを考慮する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27669a-c0f1-4dac-9d3f-a5f58acd4f20",
   "metadata": {},
   "source": [
    "##　検証データの汎化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabda460-6bbe-492d-b634-d5ae382b70fa",
   "metadata": {},
   "source": [
    "過学習を防ぐためには過学習が起こるかもしれないことを認識すること\n",
    "\n",
    "そのためにはデータを検証セットと訓練セットに分け、訓練セットにモデルをfitする\n",
    "損失は訓練セットと検証セットを一回ずつ評価できる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d717cfef-4a89-499d-a2d6-495796edd537",
   "metadata": {},
   "source": [
    "訓練\n",
    "\n",
    "訓練データではモデルが訓練データ内の情報を処理できるか確認する。\n",
    "パラメータが少なければ、ネットワークで近似できる関数の形状は単純になる\n",
    "\n",
    "訓練データの損失が減少しない場合はモデルがデータに対して単純すぎる可能性がある\n",
    "他の可能性としては訓練データに出力を説明できるような意味のある情報が含まれていないケースがある"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e63eeb-3528-4cfd-9a47-f95c2708eb22",
   "metadata": {},
   "source": [
    "検証\n",
    "\n",
    "検証データで評価した損失値が、訓練データでの結果とともに減少しない場合は、\n",
    "モデルは訓練データへの適合性は改善出来ているが、訓練に使用したデータ以外には汎化していないことを意味する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b418542-7b70-4035-84aa-f5fb46b27841",
   "metadata": {},
   "source": [
    "訓練データでの損失と検証データでの損失が乖離している場合は過学習を意味する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632b57a-32e8-4b11-85c1-c2baf1799972",
   "metadata": {},
   "source": [
    "ペナルティ項\n",
    "\n",
    "十分なデータがある場合にモデルにフィットさせる時は可能な限り定常的であることを確認する。\n",
    "そのためにペナルティ項を設けてモデルの変化を鈍くする。\n",
    "\n",
    "他には入力データへノイズを加える、モデルをシンプルなものにするといった方法がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fda147db-2dba-4ad8-ade5-6df75ff2b26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#データセットの分割\n",
    "\n",
    "n_samples = t_u.shape[0]\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73028fa5-a30a-46d0-a0ed-392e77a1f698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val = int(0.2 * n_samples)\n",
    "n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "637bb537-fa33-4111-8a2c-a17abfb36a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2,  6,  4, 10,  8,  9,  7,  0,  3,  1,  5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#並び替え\n",
    "suffled_indices = torch.randperm(n_samples)\n",
    "suffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2af404d7-ac8c-4787-940f-23a1a9378a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  6,  4, 10,  8,  9,  7,  0,  3]) tensor([1, 5])\n"
     ]
    }
   ],
   "source": [
    "#データ分けのインデックス取得\n",
    "train_indices = suffled_indices[:-n_val]\n",
    "val_indices = suffled_indices[-n_val:]\n",
    "print(train_indices, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f040b894-5ebe-45d6-9618-75410e577255",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_u = t_u[train_indices]\n",
    "train_t_c = t_c[train_indices]\n",
    "\n",
    "val_t_u = t_u[val_indices]\n",
    "val_t_c = t_c[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b12d7ef4-2181-40eb-87a1-cc8128323dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t_un = 0.1* train_t_u\n",
    "val_t_un = 0.1* val_t_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "770240d9-10e2-4937-b9a4-72c0c1a4d5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.7008,  8.9437], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_t_p = model(val_t_u, *params)\n",
    "val_t_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1bb00120-de2b-4ba1-868b-5fae1382f35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.,  8.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_t_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "79876ac6-99bc-4844-b7d0-c9f46b532102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2893, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(val_t_p, val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "78892eeb-8dd2-4ab4-bbec-9d2ae7e1a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tarining_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs +1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        val_t_p = model(val_t_u, *params)\n",
    "        val_loss = loss_fn(val_t_p, val_t_c)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()#trainの時のみbackward\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 ==0:\n",
    "            print(f\"Epoch {epoch}, Train loss{train_loss.item():.4f},\"\n",
    "                  f\"Valdation loss{val_loss.item():.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "48715027-bd86-4f5a-bff0-3361363a24ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss89.2897,Valdation loss40.2001\n",
      "Epoch 2, Train loss42.9957,Valdation loss7.5859\n",
      "Epoch 3, Train loss35.9978,Valdation loss5.1593\n",
      "Epoch 500, Train loss7.4615,Valdation loss2.4141\n",
      "Epoch 1000, Train loss3.8338,Valdation loss1.6143\n",
      "Epoch 1500, Train loss3.3591,Valdation loss1.4178\n",
      "Epoch 2000, Train loss3.2970,Valdation loss1.3589\n",
      "Epoch 2500, Train loss3.2889,Valdation loss1.3391\n",
      "Epoch 3000, Train loss3.2878,Valdation loss1.3322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3320, -17.1560], requires_grad=True)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "tarining_loop(n_epochs=3000,\n",
    "              optimizer = optimizer,\n",
    "              params = params,\n",
    "              train_t_u = train_t_un,\n",
    "              val_t_u = val_t_un,\n",
    "              train_t_c = train_t_c,\n",
    "              val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d9e2942-7981-49b7-90e2-e838d2306c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 桁違いではないが検証データより訓練データの損失の方が小さい\n",
    "# 理想はどちらも同じように下がっていくことだが、検証より訓練の方が大きく下がっていくのは許容範囲"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b3744-f4e1-4cff-a272-e78c5b4036dc",
   "metadata": {},
   "source": [
    "自動微分とテンソルの関係\n",
    "\n",
    "訓練データではtrain_t_u -> train_t_p -> train_loss の計算グラフが作成されており、\n",
    "テストデータでも同様に別のグラフが作成されている。\n",
    "\n",
    "baskward()されたら指定のlossの導関数がoptmaozeに累積される。\n",
    "\n",
    "間違ってval_lossでもbackward（）したらその導関数も累積され、全てのデータで学習するのと同じになる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "33e41458-ec82-4a21-8628-70b3500e6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.no_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "00c5fc5d-49c1-40f9-82f7-c2d3fd19d287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tarining_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs +1):\n",
    "        train_t_p = model(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False#強制的な確認、異なっていたらエラー\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()#trainの時のみbackward\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 ==0:\n",
    "            print(f\"Epoch {epoch}, Train loss{train_loss.item():.4f},\"\n",
    "                  f\"Valdation loss{val_loss.item():.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "be92e547-bcec-4878-bf68-503e20f60262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss89.2897,Valdation loss40.2001\n",
      "Epoch 2, Train loss42.9957,Valdation loss7.5859\n",
      "Epoch 3, Train loss35.9978,Valdation loss5.1593\n",
      "Epoch 500, Train loss7.4615,Valdation loss2.4141\n",
      "Epoch 1000, Train loss3.8338,Valdation loss1.6143\n",
      "Epoch 1500, Train loss3.3591,Valdation loss1.4178\n",
      "Epoch 2000, Train loss3.2970,Valdation loss1.3589\n",
      "Epoch 2500, Train loss3.2889,Valdation loss1.3391\n",
      "Epoch 3000, Train loss3.2878,Valdation loss1.3322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  5.3320, -17.1560], requires_grad=True)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = torch.tensor([1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-2\n",
    "optimizer = optim.SGD([params], lr=learning_rate)\n",
    "\n",
    "tarining_loop(n_epochs=3000,\n",
    "              optimizer = optimizer,\n",
    "              params = params,\n",
    "              train_t_u = train_t_un,\n",
    "              val_t_u = val_t_un,\n",
    "              train_t_c = train_t_c,\n",
    "              val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea82ed1-b782-492a-b25b-4c6311845c70",
   "metadata": {},
   "source": [
    "##　演習問題"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dc5662e3-2293-4d17-a657-77a9b2714ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルと引数の変更\n",
    "#パラメータが増え損失は軽減できる\n",
    "def model2(t_u, w1, w2, b):\n",
    "    return w2 * t_u ** 2 + w1 * t_u +b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "09d827b0-7565-4edf-b8d2-039f1302b524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tarining_loop(n_epochs, optimizer, params, train_t_u, val_t_u, train_t_c, val_t_c):\n",
    "    for epoch in range(1, n_epochs +1):\n",
    "        train_t_p = model2(train_t_u, *params)\n",
    "        train_loss = loss_fn(train_t_p, train_t_c)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_t_p = model2(val_t_u, *params)\n",
    "            val_loss = loss_fn(val_t_p, val_t_c)\n",
    "            assert val_loss.requires_grad == False#強制的な確認、異なっていたらエラー\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()#trainの時のみbackward\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch <= 3 or epoch % 500 ==0:\n",
    "            print(f\"Epoch {epoch}, Train loss{train_loss.item():.4f},\"\n",
    "                  f\"Valdation loss{val_loss.item():.4f}\")\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a53f1f8f-1cd0-4b0f-a025-38b61c61ceff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss719.9368,Valdation loss477.1533\n",
      "Epoch 2, Train loss410.7798,Valdation loss273.7194\n",
      "Epoch 3, Train loss237.3644,Valdation loss158.4794\n",
      "Epoch 500, Train loss12.0079,Valdation loss2.7905\n",
      "Epoch 1000, Train loss9.3329,Valdation loss2.4306\n",
      "Epoch 1500, Train loss7.4799,Valdation loss2.3789\n",
      "Epoch 2000, Train loss6.1959,Valdation loss2.5070\n",
      "Epoch 2500, Train loss5.3059,Valdation loss2.7318\n",
      "Epoch 3000, Train loss4.6885,Valdation loss3.0003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.6785,  0.5154, -0.7475], requires_grad=True)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params2 = torch.tensor([1.0, 1.0, 0.0], requires_grad=True)\n",
    "learning_rate = 1e-4\n",
    "optimizer = optim.SGD([params2], lr=learning_rate)\n",
    "\n",
    "tarining_loop(n_epochs=3000,\n",
    "              optimizer = optimizer,\n",
    "              params = params2,\n",
    "              train_t_u = train_t_un,\n",
    "              val_t_u = val_t_un,\n",
    "              train_t_c = train_t_c,\n",
    "              val_t_c = val_t_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65512db3-9126-4cbd-b9ce-bfb94460a337",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_p1 = model(t_un, *params)\n",
    "t_p2 = model2(t_un, *params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7da7e592-2829-45ce-b74c-0af3b93df6cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f9bc73aa640>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "plt.xlabel(\"Temperature (°Fahrenheit)\")\n",
    "plt.ylabel(\"Temperature (°Celsius)\")\n",
    "plt.plot(t_u.numpy(),  t_p1.detach().numpy())\n",
    "\n",
    "plt.plot(t_u.numpy(),  t_p2.detach().numpy())\n",
    "plt.plot(t_u.numpy(),  t_c.numpy(), 'o')#答え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ba4f8896-cc14-491b-8d6d-b8fc9a4666a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#良い結果になってしまったが、本来真のモデルではないため過学習になりval loss が増大する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744e861-ccbf-4cf7-a8b4-84d7f82af4af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
